{
  "hash": "0972abef1284d4f41cc6953e402d1bc2",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Iris ML classification\"\nauthor: \"Ajay A\"\ndate: \"10-21-2024\"\nformat: \n  html: default\n  docx: default\ncategories: [Machine Learning, Random Forest, KNN, EDA]\n---\n\n\n\n\n\n\n\n\n\n\n# Predicting the Species of Iris with Machine Learning\n\n## Introduction\n\nThis is the \"Iris\" dataset. Originally published at UCI Machine Learning Repository: Iris Data Set, this small dataset from 1936 is often used for testing out machine learning algorithms and visualizations (for example, Scatter Plot). Each row of the table represents an iris flower, including its species and dimensions of its botanical parts, sepal and petal, in centimeters.\n\nIris data set is used widely as an example in field of data sciences and widely available to both `python` and `R` users.\n\n## Importing Libraries\n\nLet's load the required packages\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(patchwork)\n```\n:::\n\n\n\n\n\n\n\n\n\n\n## Importing data\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\niris_data <- read_csv(\"F:/Data_Sci/Internship Projects/Iris_ML/Iris.csv\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 150 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Species\ndbl (4): SepalLengthCm, SepalWidthCm, PetalLengthCm, PetalWidthCm\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\niris_data <-  iris_data %>% janitor::clean_names() %>%\n  mutate(species = str_replace_all(species,\"Iris-\", \"\")) %>% \n  mutate(species = as.factor(species))\n\nglimpse(iris_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 150\nColumns: 5\n$ sepal_length_cm <dbl> 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4,…\n$ sepal_width_cm  <dbl> 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7,…\n$ petal_length_cm <dbl> 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5,…\n$ petal_width_cm  <dbl> 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2,…\n$ species         <fct> setosa, setosa, setosa, setosa, setosa, setosa, setosa…\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n\nWe have features like `sepal_length_cm`, `sepal_width_cm`, `petal_length_cm` and `petal_width_cm` and `species` necessary for the classification of the species.\n\nFirst, check for `NA` in the data.\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\niris_data %>% map(~sum(is.na(.)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$sepal_length_cm\n[1] 0\n\n$sepal_width_cm\n[1] 0\n\n$petal_length_cm\n[1] 0\n\n$petal_width_cm\n[1] 0\n\n$species\n[1] 0\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n\nThere no NA which is really good for the data.\n\n## Analysisng data\n\nLet's visualize the data with above parameters\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_iris <- function(param){\n  iris_data %>% ggplot(aes(species, {{param}})) + \n    geom_boxplot(aes(color = species)) +\n    theme(legend.position = \"none\")\n}\n\n(plot_iris(sepal_length_cm) +  plot_iris(petal_length_cm))/\n(plot_iris(sepal_width_cm) + plot_iris(petal_width_cm))\n```\n\n::: {.cell-output-display}\n![Difference in observed parameters between species](iris_ml_classification_files/figure-docx/fig-iris_parameters-1.png){#fig-iris_parameters}\n:::\n:::\n\n\n\n\n\n\n\n\n\n\nThere is quite a difference between the species in all parameters \"setosa\" \\< \"versicolor\" \\< \"virginica\" except for `sepal_width_cm` where \"versicolor\" \\< \"virginica\" \\< \"setosa\"\n\n## Building a model\n\nLet’s start by loading the `tidymodels` package and splitting our data into training and testing sets.\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\nset.seed(2024)\n\niris_split <- initial_split(iris_data, prop = 0.8)\n\niris_train <- training(iris_split)\niris_test <- testing(iris_split)\n```\n:::\n\n\n\n\n\n\n\n\n\n\nData is not large enough to build a model so creating resamples of the data to evaluate the model\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2025)\n\niris_boot <- bootstraps(iris_train, times = 5)\n\niris_boot\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# Bootstrap sampling \n# A tibble: 5 × 2\n  splits           id        \n  <list>           <chr>     \n1 <split [120/45]> Bootstrap1\n2 <split [120/40]> Bootstrap2\n3 <split [120/38]> Bootstrap3\n4 <split [120/39]> Bootstrap4\n5 <split [120/46]> Bootstrap5\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n\nLet’s build 2 models and check which is better for the data.\n\n### Random Forest Model\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# random forest model\n\nrf_spec <- rand_forest() %>% \n  set_mode(\"classification\") %>% \n  set_engine(\"ranger\")\n\nrf_spec\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRandom Forest Model Specification (classification)\n\nComputational engine: ranger \n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n### **K - nearest neighbors** model\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nknn_spec <- nearest_neighbor(neighbors = 5) %>% \n  set_mode(\"classification\") %>% \n  set_engine(\"kknn\")\n\nknn_spec\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nK-Nearest Neighbor Model Specification (classification)\n\nMain Arguments:\n  neighbors = 5\n\nComputational engine: kknn \n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n### Setting `workflow()`\n\nNext let’s start putting together a tidymodels `workflow()`, a helper object to help manage modeling pipelines with pieces that fit together like Lego blocks. Notice that there is no model yet: `Model: None`\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\niris_wf <- workflow() %>% \n  add_formula(species ~ .)\n\niris_wf\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Formula\nModel: None\n\n── Preprocessor ────────────────────────────────────────────────────────────────\nspecies ~ .\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n### Fitting the model\n\nNow we can add a model and fit the model to each of the resamples. First, we can fit the `randomforest` model\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrf_rs <- iris_wf %>% \n  add_model(rf_spec) %>% \n  fit_resamples(\n    resamples = iris_boot,\n    control = control_resamples(save_pred = TRUE)\n  )\n\nrf_rs\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# Resampling results\n# Bootstrap sampling \n# A tibble: 5 × 5\n  splits           id         .metrics         .notes           .predictions\n  <list>           <chr>      <list>           <list>           <list>      \n1 <split [120/45]> Bootstrap1 <tibble [3 × 4]> <tibble [0 × 3]> <tibble>    \n2 <split [120/40]> Bootstrap2 <tibble [3 × 4]> <tibble [0 × 3]> <tibble>    \n3 <split [120/38]> Bootstrap3 <tibble [3 × 4]> <tibble [0 × 3]> <tibble>    \n4 <split [120/39]> Bootstrap4 <tibble [3 × 4]> <tibble [0 × 3]> <tibble>    \n5 <split [120/46]> Bootstrap5 <tibble [3 × 4]> <tibble [0 × 3]> <tibble>    \n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n\nNow we can add a model and fit the model to each of the resamples. First, we can fit the `knn` model\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nknn_rs <- iris_wf %>% \n  add_model(knn_spec) %>% \n  fit_resamples(\n    resamples = iris_boot,\n    control = control_resamples(save_pred = TRUE)\n  )\n\nknn_rs\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# Resampling results\n# Bootstrap sampling \n# A tibble: 5 × 5\n  splits           id         .metrics         .notes           .predictions\n  <list>           <chr>      <list>           <list>           <list>      \n1 <split [120/45]> Bootstrap1 <tibble [3 × 4]> <tibble [0 × 3]> <tibble>    \n2 <split [120/40]> Bootstrap2 <tibble [3 × 4]> <tibble [0 × 3]> <tibble>    \n3 <split [120/38]> Bootstrap3 <tibble [3 × 4]> <tibble [0 × 3]> <tibble>    \n4 <split [120/39]> Bootstrap4 <tibble [3 × 4]> <tibble [0 × 3]> <tibble>    \n5 <split [120/46]> Bootstrap5 <tibble [3 × 4]> <tibble [0 × 3]> <tibble>    \n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n## Evaluating the model\n\n### Evaluating Random Forest model\n\n`collect_metrics` function collect the necessary parameters for evaluation\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncollect_metrics(rf_rs)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 6\n  .metric     .estimator   mean     n  std_err .config             \n  <chr>       <chr>       <dbl> <int>    <dbl> <chr>               \n1 accuracy    multiclass 0.947      5 0.0144   Preprocessor1_Model1\n2 brier_class multiclass 0.0332     5 0.0106   Preprocessor1_Model1\n3 roc_auc     hand_till  0.998      5 0.000846 Preprocessor1_Model1\n```\n\n\n:::\n\n```{.r .cell-code}\ncollect_predictions(rf_rs) %>% glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 208\nColumns: 8\n$ .pred_class      <fct> versicolor, virginica, virginica, setosa, versicolor,…\n$ .pred_setosa     <dbl> 0.0020, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.000…\n$ .pred_versicolor <dbl> 0.93323254, 0.00000000, 0.33701825, 0.00000000, 0.998…\n$ .pred_virginica  <dbl> 0.0647674603, 1.0000000000, 0.6629817460, 0.000000000…\n$ id               <chr> \"Bootstrap1\", \"Bootstrap1\", \"Bootstrap1\", \"Bootstrap1…\n$ .row             <int> 1, 6, 7, 9, 18, 19, 21, 22, 28, 30, 34, 35, 39, 40, 4…\n$ species          <fct> versicolor, virginica, virginica, setosa, versicolor,…\n$ .config          <chr> \"Preprocessor1_Model1\", \"Preprocessor1_Model1\", \"Prep…\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n### Evaluating K-nearest neighbor model\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncollect_metrics(knn_rs)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 6\n  .metric     .estimator   mean     n std_err .config             \n  <chr>       <chr>       <dbl> <int>   <dbl> <chr>               \n1 accuracy    multiclass 0.922      5 0.00981 Preprocessor1_Model1\n2 brier_class multiclass 0.0660     5 0.00783 Preprocessor1_Model1\n3 roc_auc     hand_till  0.979      5 0.00687 Preprocessor1_Model1\n```\n\n\n:::\n\n```{.r .cell-code}\ncollect_predictions(rf_rs) %>% glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 208\nColumns: 8\n$ .pred_class      <fct> versicolor, virginica, virginica, setosa, versicolor,…\n$ .pred_setosa     <dbl> 0.0020, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.000…\n$ .pred_versicolor <dbl> 0.93323254, 0.00000000, 0.33701825, 0.00000000, 0.998…\n$ .pred_virginica  <dbl> 0.0647674603, 1.0000000000, 0.6629817460, 0.000000000…\n$ id               <chr> \"Bootstrap1\", \"Bootstrap1\", \"Bootstrap1\", \"Bootstrap1…\n$ .row             <int> 1, 6, 7, 9, 18, 19, 21, 22, 28, 30, 34, 35, 39, 40, 4…\n$ species          <fct> versicolor, virginica, virginica, setosa, versicolor,…\n$ .config          <chr> \"Preprocessor1_Model1\", \"Preprocessor1_Model1\", \"Prep…\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n\nAs we can see that `random_forest` model has higher accuracy than `knn` model\n\nConfusion matrix lets us know how accurate the model is predicting the values\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrf_rs %>% conf_mat_resampled()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 9 × 3\n  Prediction Truth       Freq\n  <fct>      <fct>      <dbl>\n1 setosa     setosa      13.4\n2 setosa     versicolor   0  \n3 setosa     virginica    0  \n4 versicolor setosa       0  \n5 versicolor versicolor  13.2\n6 versicolor virginica    1.8\n7 virginica  setosa       0  \n8 virginica  versicolor   0.4\n9 virginica  virginica   12.8\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n\nNow for the `roc` curve which shows us how accurate a model is for different species in the data.\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrf_rs %>%\n  collect_predictions() %>%\n  roc_curve(truth = species, .pred_setosa, .pred_versicolor,\n                    .pred_virginica) %>%\n  ggplot(aes(1 - specificity, sensitivity, color = .level)) +\n  geom_abline(lty = 2, color = \"gray80\", linewidth = 1.5) +\n  geom_path(show.legend = TRUE, alpha = 0.6, linewidth = 1.2) +\n  coord_equal() + \n  labs(title = \"ROC Curve for Random Forest Classification\",\n       color = \"Class\")\n```\n\n::: {.cell-output-display}\n![ROC curve for Random Forest Model](iris_ml_classification_files/figure-docx/roc_plot_random_forest-1.png)\n:::\n:::\n\n\n\n\n\n\n\n\n\n\nWhen we compare the same to the \"KNN\" model we can see the difference.\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nknn_rs %>%\n  collect_predictions() %>%\n  roc_curve(truth = species, .pred_setosa, .pred_versicolor,\n                    .pred_virginica) %>%\n  ggplot(aes(1 - specificity, sensitivity, color = .level)) +\n  geom_abline(lty = 2, color = \"gray80\", linewidth = 1.5) +\n  geom_path(show.legend = TRUE, alpha = 0.6, linewidth = 1.2) +\n  coord_equal() + \n  labs(title = \"ROC Curve for KNN Classification\",\n       color = \"Class\")\n```\n\n::: {.cell-output-display}\n![ROC curve for KNN Model](iris_ml_classification_files/figure-docx/roc_plot_knn-1.png)\n:::\n:::\n\n\n\n\n\n\n\n\n\n\nThe \"1 - Specificity\" drops for the \"KNN\" model when compared to the \"Random Forest\" model, so we will use the \"Random Forest\" model to do predictions.\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\niris_final <- iris_wf %>%\n  add_model(rf_spec) %>%\n  last_fit(iris_split)\n\niris_final %>% collect_metrics()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 4\n  .metric     .estimator .estimate .config             \n  <chr>       <chr>          <dbl> <chr>               \n1 accuracy    multiclass    0.933  Preprocessor1_Model1\n2 roc_auc     hand_till     1      Preprocessor1_Model1\n3 brier_class multiclass    0.0534 Preprocessor1_Model1\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n### Predicting Outcomes\n\nBased on the \"iris_final\" model we can predict the `species` based on the other parameters.\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a new data frame for the measurements\nnew_data <- tibble(\n  sepal_length_cm = 4.6,\n  sepal_width_cm = 3.8,\n  petal_length_cm = 1.4,\n  petal_width_cm = 0.2\n)\n\n# Extract the workflow from the last_fit result\nworkflow_fit <- iris_final %>% extract_workflow()\n\n# Make predictions using the new data\npredictions <- predict(workflow_fit, new_data)\n\npredictions\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 1\n  .pred_class\n  <fct>      \n1 setosa     \n```\n\n\n:::\n:::\n",
    "supporting": [
      "iris_ml_classification_files\\figure-docx"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}