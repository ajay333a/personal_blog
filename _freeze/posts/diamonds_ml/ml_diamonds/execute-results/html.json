{
  "hash": "e86ab3c0a2ae6119f95d2885fd95ad92",
  "result": {
    "markdown": "---\ntitle: \"Predict Price of Diamonds\"\nauthor: \"Ajay Shankar A\"\ndate: \"2023-12-10\"\nformat: html\ncode-fold: false\ncategories: [Analysis, Code, EDA, Modeling, Machine Learning]\ntoc: true\n---\n\n\n## Predicting Diamonds Price\n\n### Introduction\n\nBuilding a model to **predict the price of the diamonds** using `tidymodels`.\n\nDiamonds data set is readily available to use through the `ggplot2` library in the `tidyverse` and we will be using this data set predict the prices of the other diamonds.\n\nIn the data set various parameters of diamonds are given and each of these parameters may or may not effect the **price** of the diamonds.\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n\ndata(\"diamonds\")\ndiamonds\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 53,940 × 10\n   carat cut       color clarity depth table price     x     y     z\n   <dbl> <ord>     <ord> <ord>   <dbl> <dbl> <int> <dbl> <dbl> <dbl>\n 1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n 2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n 3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n 4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n 5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n 6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n 7  0.24 Very Good I     VVS1     62.3    57   336  3.95  3.98  2.47\n 8  0.26 Very Good H     SI1      61.9    55   337  4.07  4.11  2.53\n 9  0.22 Fair      E     VS2      65.1    61   337  3.87  3.78  2.49\n10  0.23 Very Good H     VS1      59.4    61   338  4     4.05  2.39\n# ℹ 53,930 more rows\n```\n:::\n:::\n\n\nData has over 50,000 observations which is good for modeling.\n\n### Exploring the data\n\nThe diamonds data set is available to explore in `ggplot2` library as mentioned above.\n\nLet's check for NA's before exploring the data\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndiamonds %>% map( ~sum(is.na(.))) %>% unlist()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  carat     cut   color clarity   depth   table   price       x       y       z \n      0       0       0       0       0       0       0       0       0       0 \n```\n:::\n:::\n\n\nIt's really good that there are no `NA`'s but we have to be careful of the `0` in the numeric columns.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndiamonds %>% select(carat, x, y, z) %>% arrange(x, y, z)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 53,940 × 4\n   carat     x     y     z\n   <dbl> <dbl> <dbl> <dbl>\n 1  1     0     0     0   \n 2  1.14  0     0     0   \n 3  1.56  0     0     0   \n 4  1.2   0     0     0   \n 5  2.25  0     0     0   \n 6  0.71  0     0     0   \n 7  0.71  0     0     0   \n 8  1.07  0     6.62  0   \n 9  0.2   3.73  3.68  2.31\n10  0.2   3.73  3.71  2.33\n# ℹ 53,930 more rows\n```\n:::\n:::\n\n\nDiamonds cannot have a `x`(length), `y`(width), `z`(depth) of 0 and have weight. So let's replace these values with `NA` or we can remove them out completely too.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndiamonds %>% mutate(x = if_else(x == \"0\", NA, x),\n                    y = if_else(y == \"0\", NA, y),\n                    z = if_else(y == \"0\", NA, z))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 53,940 × 10\n   carat cut       color clarity depth table price     x     y     z\n   <dbl> <ord>     <ord> <ord>   <dbl> <dbl> <int> <dbl> <dbl> <dbl>\n 1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n 2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n 3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n 4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n 5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n 6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n 7  0.24 Very Good I     VVS1     62.3    57   336  3.95  3.98  2.47\n 8  0.26 Very Good H     SI1      61.9    55   337  4.07  4.11  2.53\n 9  0.22 Fair      E     VS2      65.1    61   337  3.87  3.78  2.49\n10  0.23 Very Good H     VS1      59.4    61   338  4     4.05  2.39\n# ℹ 53,930 more rows\n```\n:::\n:::\n\n\nNow lets visualize the distribution of the diamonds.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndiamonds %>% ggplot(aes(carat)) + geom_freqpoly(binwidth = 0.05)\n```\n\n::: {.cell-output-display}\n![Frequency polygon plot](ml_diamonds_files/figure-html/fig-price_hist-1.png){#fig-price_hist width=1200}\n:::\n:::\n\n\nFrom the @fig-price_hist we can observe that - Most of the diamonds are between 0.2 to 1.5 carats. - There are peaks which means higher number of diamonds at whole and common fractions.\n\nMy general knowledge is that the weight i.e, carat of the diamond influences the price most. Let's visualize that.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndiamonds %>% ggplot(aes(carat, price)) + geom_hex(bins = 50)\n```\n\n::: {.cell-output-display}\n![](ml_diamonds_files/figure-html/price_vs_carat-1.png){width=1200}\n:::\n:::\n\n\nThe price tends to follow exponential curve the `log2()` curve, we can confirm this by another graph.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndiamonds %>% filter(carat < 2.5) %>% \n  mutate(log_price = log10(price),\n                    log_carat = log10(carat)) %>% \n  ggplot(aes(log_carat, log_price)) + geom_hex(bins = 50)\n```\n\n::: {.cell-output-display}\n![Log of carat vs Log of Price at base 2](ml_diamonds_files/figure-html/fig-log_prc_vs_carat-1.png){#fig-log_prc_vs_carat width=1200}\n:::\n:::\n\n\nThe above @fig-log_prc_vs_carat shows that once we apply `log2()` to both price and carat the relationship mostly looks to be linear.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndiamonds %>% filter(carat <= 2.5) %>% ggplot(aes(carat, price)) +\n  geom_point(alpha = 0.1, color = \"purple\")\n```\n\n::: {.cell-output-display}\n![](ml_diamonds_files/figure-html/distribution-1.png){width=1200}\n:::\n:::\n\n\nWe can see that `price` jumps when the weight is exactly or greater than to the whole and common fractions such as 0.5, 1.0, 1.5 and 2.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(patchwork)\n\nplot_parameter <- function(param){\n  ggplot(diamonds, aes(fct_reorder({{param}}, price), price)) +\n    geom_boxplot() + stat_summary(fun.y = mean, geom = \"point\") +\n    labs(x = as_label(substitute(param)))\n}\n\n(plot_parameter(cut) + plot_parameter(color)) /\n  (plot_parameter(clarity))\n```\n\n::: {.cell-output-display}\n![](ml_diamonds_files/figure-html/exploring the data-1.png){width=1200}\n:::\n:::\n\n\nLow quality diamonds with Fair cut and low quality color seems to have very high price. So now lets use `tidymodels` to model the data using `rand_forest`\n\n### Building a model\n\nAs every parameter in the data is important for the price prediction we are going to keep all the columns intact.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\nset.seed(2023)\n\ndiamonds_2 <- diamonds %>% \n  mutate(price = log2(price), carat = log2(carat))\n\ndiamonds_split <- initial_split(diamonds_2, strata = carat, prop = 0.8)\ndiamonds_split\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<Training/Testing/Total>\n<43150/10790/53940>\n```\n:::\n\n```{.r .cell-code}\ndiamonds_train <- training(diamonds_split)\ndiamonds_test <- testing(diamonds_split)\n```\n:::\n\n\nI am using `strata` with `carat` as most of the diamonds are not properly distributed yet all diamonds of different weight should be well represented.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndiamonds_folds <- vfold_cv(diamonds_train, v = 10)\ndiamonds_folds\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#  10-fold cross-validation \n# A tibble: 10 × 2\n   splits               id    \n   <list>               <chr> \n 1 <split [38835/4315]> Fold01\n 2 <split [38835/4315]> Fold02\n 3 <split [38835/4315]> Fold03\n 4 <split [38835/4315]> Fold04\n 5 <split [38835/4315]> Fold05\n 6 <split [38835/4315]> Fold06\n 7 <split [38835/4315]> Fold07\n 8 <split [38835/4315]> Fold08\n 9 <split [38835/4315]> Fold09\n10 <split [38835/4315]> Fold10\n```\n:::\n:::\n\n\nI think `rand_forest` will work better on this data set but lets compare both **Linear Regression** models and **Random Forest** models.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_spec <- linear_reg() %>% set_engine(\"glm\")\nlm_spec\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear Regression Model Specification (regression)\n\nComputational engine: glm \n```\n:::\n\n```{.r .cell-code}\nrf_spec <- rand_forest(mtry = 3, trees = 500, min_n = 5 ) %>% \n  set_mode(\"regression\") %>% \n  set_engine(\"ranger\")\nrf_spec\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  mtry = 3\n  trees = 500\n  min_n = 5\n\nComputational engine: ranger \n```\n:::\n:::\n\n\nWe still need to maniplulate some parts of the data like price and carat so that they are optimised which can be done using `recipe` library.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndiamonds_recp <- \n  recipe(price ~ ., data = diamonds_train)\n\ndiamonds_recp\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n── Recipe ──────────────────────────────────────────────────────────────────────\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n── Inputs \n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nNumber of variables by role\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\noutcome:   1\npredictor: 9\n```\n:::\n:::\n\n\nNext let's start putting together a tidymodels `workflow()`, a helper object to help manage modeling pipelines with pieces that fit together like Lego blocks.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndiamonds_wf <- workflow() %>%\n  add_recipe(diamonds_recp)\n\ndiamonds_wf\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: None\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n```\n:::\n:::\n\n\nLet's fit the two models we prepared for the data. First code block contains linear regression model and the second contains the random_forest model.\n\n1.  Linear Regression model\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Linear regression\nglm_rs <- diamonds_wf %>% \n  add_model(lm_spec) %>% \n  fit_resamples(\n    resamples = diamonds_folds,\n    control = control_resamples(save_pred = TRUE)\n  )\n\nglm_rs\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# Resampling results\n# 10-fold cross-validation \n# A tibble: 10 × 5\n   splits               id     .metrics         .notes           .predictions\n   <list>               <chr>  <list>           <list>           <list>      \n 1 <split [38835/4315]> Fold01 <tibble [2 × 4]> <tibble [0 × 3]> <tibble>    \n 2 <split [38835/4315]> Fold02 <tibble [2 × 4]> <tibble [0 × 3]> <tibble>    \n 3 <split [38835/4315]> Fold03 <tibble [2 × 4]> <tibble [0 × 3]> <tibble>    \n 4 <split [38835/4315]> Fold04 <tibble [2 × 4]> <tibble [0 × 3]> <tibble>    \n 5 <split [38835/4315]> Fold05 <tibble [2 × 4]> <tibble [0 × 3]> <tibble>    \n 6 <split [38835/4315]> Fold06 <tibble [2 × 4]> <tibble [0 × 3]> <tibble>    \n 7 <split [38835/4315]> Fold07 <tibble [2 × 4]> <tibble [0 × 3]> <tibble>    \n 8 <split [38835/4315]> Fold08 <tibble [2 × 4]> <tibble [0 × 3]> <tibble>    \n 9 <split [38835/4315]> Fold09 <tibble [2 × 4]> <tibble [0 × 3]> <tibble>    \n10 <split [38835/4315]> Fold10 <tibble [2 × 4]> <tibble [0 × 3]> <tibble>    \n```\n:::\n:::\n\n\n2.  Random Forest model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrf_rs <- diamonds_wf %>% \n  add_model(rf_spec) %>% \n  fit_resamples(\n    resamples = diamonds_folds,\n    control = control_resamples(save_pred = TRUE)\n  )\n\nrf_rs\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# Resampling results\n# 10-fold cross-validation \n# A tibble: 10 × 5\n   splits               id     .metrics         .notes           .predictions\n   <list>               <chr>  <list>           <list>           <list>      \n 1 <split [38835/4315]> Fold01 <tibble [2 × 4]> <tibble [0 × 3]> <tibble>    \n 2 <split [38835/4315]> Fold02 <tibble [2 × 4]> <tibble [0 × 3]> <tibble>    \n 3 <split [38835/4315]> Fold03 <tibble [2 × 4]> <tibble [0 × 3]> <tibble>    \n 4 <split [38835/4315]> Fold04 <tibble [2 × 4]> <tibble [0 × 3]> <tibble>    \n 5 <split [38835/4315]> Fold05 <tibble [2 × 4]> <tibble [0 × 3]> <tibble>    \n 6 <split [38835/4315]> Fold06 <tibble [2 × 4]> <tibble [0 × 3]> <tibble>    \n 7 <split [38835/4315]> Fold07 <tibble [2 × 4]> <tibble [0 × 3]> <tibble>    \n 8 <split [38835/4315]> Fold08 <tibble [2 × 4]> <tibble [0 × 3]> <tibble>    \n 9 <split [38835/4315]> Fold09 <tibble [2 × 4]> <tibble [0 × 3]> <tibble>    \n10 <split [38835/4315]> Fold10 <tibble [2 × 4]> <tibble [0 × 3]> <tibble>    \n```\n:::\n:::\n\n\nWe have fit out training data to the models.\n\n## Evaluating models\n\nNow let's check the results and how well they performed.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Linear regression model\ncollect_metrics(glm_rs)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 6\n  .metric .estimator  mean     n  std_err .config             \n  <chr>   <chr>      <dbl> <int>    <dbl> <chr>               \n1 rmse    standard   0.194    10 0.00102  Preprocessor1_Model1\n2 rsq     standard   0.983    10 0.000198 Preprocessor1_Model1\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Random Forest model\ncollect_metrics(rf_rs)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 6\n  .metric .estimator  mean     n  std_err .config             \n  <chr>   <chr>      <dbl> <int>    <dbl> <chr>               \n1 rmse    standard   0.133    10 0.000906 Preprocessor1_Model1\n2 rsq     standard   0.992    10 0.000114 Preprocessor1_Model1\n```\n:::\n:::\n\n\nBoth of the models have high accuracy but I prefer **Random Forest Model** as it is preferred when there are more nominal and ordinal data points.\n",
    "supporting": [
      "ml_diamonds_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}