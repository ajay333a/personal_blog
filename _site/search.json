[
  {
    "objectID": "posts/why_ds/why_data_science.html",
    "href": "posts/why_ds/why_data_science.html",
    "title": "Why Data Analytics?",
    "section": "",
    "text": "0.1 Why Data Analytics for a Forestry Graduate?\nAs a Forestry graduate, I often encounter questions about my choice of pursuing a career in Data Analytics and how it aligns with my background and a Master’s degree. Here’s why Data Analytics is a compelling path for me, and for anyone with a Master’s degree seeking a versatile career:\n\n\n0.2 Is a Degree Necessary?\n\nFlexibility: Data Analytics roles often do not mandate a specific degree. Certifications from renowned companies like Google, Microsoft, and IBM offer comprehensive projects, providing hands-on experience crucial for real-world application.\nFoundational Knowledge: Proficiency in Algebra, Arithmetic, and Statistics serves as a solid foundation. Aspirations for roles like Data Scientist may necessitate mastering advanced statistical concepts such as linear regression and A/B testing.\nThe Degree Advantage: While some large firms may prefer degrees in fields like Computer Science or Statistics, showcasing strong project portfolios can outweigh specific educational backgrounds during recruitment.\n\nThis blend of practical experience, foundational knowledge, and adaptability makes Data Analytics an attractive career choice for individuals from diverse academic backgrounds, including Forestry.\n\n\n0.3 Can a Master’s in Forestry Enhance Data Analytics Skills?\n\nAbsolutely\nAs a Forestry Graduate, my college experience has been instrumental in honing my abilities for real-world application and problem-solving, fostering a comprehensive understanding of how to tackle practical challenges. The process of data analytics, akin to crafting a thesis, involves numerous overlapping steps. In my journey, I’ve found my degree to be profoundly beneficial, particularly in mastering data analysis methods and tools.\n\n\n\n\n\n\n\n\nSteps in writing a thesis\n\n\n\n\n\n\n\n\n\nSteps in Data Analytics project\n\n\n\n\n\n\nSimilarities between thesis writing and Data analytics projects\n\n\n\n\nFigure 1: Hacker Rank Certifications\n\n\n\n\n\n\n0.4 1. Data Analysis Methods\nMy education equipped me with a solid foundation in statistical principles, ranging from fundamental concepts like measures of central tendency (mean, median, mode) and Standard Deviation(SD), which I delved into during my undergraduate Fundamentals of Statistics course. Building upon this, my master’s program delved into advanced statistical methodologies essential for applied sciences during Statistical methods for applied Sciences, covering topics such as probability, correlation, linear regression, and ANOVA. These advanced concepts proved invaluable in my thesis work, where I utilized techniques like ANOVA to assess statistical significance. Apart from statistical concepts I got to know how to handle data for cleansing, manipulating and archiving after its life cycle.\n\n\n0.5 2. Data Analysis Tools\nWhile traditional tools like Microsoft Excel remain ubiquitous for data analysis tasks such as storage, cleaning, analysis, and visualization, my exposure to such tools during my academic pursuits was essential. Excel, in particular, proved indispensable for visualizing data and conducting operations like ANOVA.\nFor those interested, I’ve also delved into projects employing the R language, which can be explored further on my blog.\n\nIs a Master’s Degree Sufficient for Data Analytics?\nWhile a master’s degree undoubtedly provides a solid foundation in data analytics, it may not always suffice for roles requiring proficiency in specific tools like SQL, programming languages, or BI tools. Many organizations prioritize candidates familiar with these technologies. Hence, it’s advisable to augment academic learning with practical experience and familiarity with a diverse array of tools, thereby enhancing employability prospects."
  },
  {
    "objectID": "posts/meds_analysis/Medicines_usage_sideeffects.html",
    "href": "posts/meds_analysis/Medicines_usage_sideeffects.html",
    "title": "Medicines Side-effects and their Substitutes",
    "section": "",
    "text": "This dataset contains comprehensive information on over 248,000 medical drugs from all manufacturers available worldwide. The data includes details such as drug names, active ingredients, therapeutic uses, dosage, side effects, and substitutes. The dataset aims to provide a useful resource for medical researchers, healthcare professionals, and drug manufacturers."
  },
  {
    "objectID": "posts/meds_analysis/Medicines_usage_sideeffects.html#importing-libraries",
    "href": "posts/meds_analysis/Medicines_usage_sideeffects.html#importing-libraries",
    "title": "Medicines Side-effects and their Substitutes",
    "section": "1 Importing Libraries",
    "text": "1 Importing Libraries\nFor data manipulation and tidying up data tidyverse package in R has always been best. tidyverse is a collection of packages of R such as\n\ndplyr and tidyr for manipulating data\nggplot2 for visualizing and rendering plots\nlubridate for dealing with dates and time series\nforcats for factoring of data\nreadr for importing, reading, writing different file formats\n\nLets import the libraries\n\n\nCode\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors"
  },
  {
    "objectID": "posts/meds_analysis/Medicines_usage_sideeffects.html#importing-data",
    "href": "posts/meds_analysis/Medicines_usage_sideeffects.html#importing-data",
    "title": "Medicines Side-effects and their Substitutes",
    "section": "2 Importing data",
    "text": "2 Importing data\n\n\nCode\nmed_data &lt;- read_csv(\"F:/r_language/quarto/blog/Data/250k Medicines Usage, Side Effects and Substitutes.csv\",\n  guess_max = 30000) #guess_max function is used to correctly guess the type of the columns\n\n\nWe imported the .csv file and we can see there are a total of 248218 rows and 58 columns in which 1 column is integer(dbl) 57 columns are classifies as character(chr) . The function guess_max makes sure that column type is identified correctly in the data.\n\n2.1 Glancing data\nNow lets take a sneak peek into the data.\n\n\nCode\nmed_data %&gt;% glimpse()\n\n\nRows: 248,218\nColumns: 58\n$ id                  &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,…\n$ name                &lt;chr&gt; \"augmentin 625 duo tablet\", \"azithral 500 tablet\",…\n$ substitute0         &lt;chr&gt; \"Penciclav 500 mg/125 mg Tablet\", \"Zithrocare 500m…\n$ substitute1         &lt;chr&gt; \"Moxikind-CV 625 Tablet\", \"Azax 500 Tablet\", \"Ambr…\n$ substitute2         &lt;chr&gt; \"Moxiforce-CV 625 Tablet\", \"Zady 500 Tablet\", \"Zer…\n$ substitute3         &lt;chr&gt; \"Fightox 625 Tablet\", \"Cazithro 500mg Tablet\", \"Ca…\n$ substitute4         &lt;chr&gt; \"Novamox CV 625mg Tablet\", \"Trulimax 500mg Tablet\"…\n$ sideEffect0         &lt;chr&gt; \"Vomiting\", \"Vomiting\", \"Nausea\", \"Headache\", \"Sle…\n$ sideEffect1         &lt;chr&gt; \"Nausea\", \"Nausea\", \"Vomiting\", \"Drowsiness\", \"Dry…\n$ sideEffect2         &lt;chr&gt; \"Diarrhea\", \"Abdominal pain\", \"Diarrhea\", \"Dizzine…\n$ sideEffect3         &lt;chr&gt; NA, \"Diarrhea\", \"Upset stomach\", \"Nausea\", NA, \"Sk…\n$ sideEffect4         &lt;chr&gt; NA, NA, \"Stomach pain\", NA, NA, \"Flu-like symptoms…\n$ sideEffect5         &lt;chr&gt; NA, NA, \"Allergic reaction\", NA, NA, \"Headache\", N…\n$ sideEffect6         &lt;chr&gt; NA, NA, \"Dizziness\", NA, NA, \"Drowsiness\", NA, NA,…\n$ sideEffect7         &lt;chr&gt; NA, NA, \"Headache\", NA, NA, \"Dizziness\", NA, NA, N…\n$ sideEffect8         &lt;chr&gt; NA, NA, \"Rash\", NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ sideEffect9         &lt;chr&gt; NA, NA, \"Hives\", NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ sideEffect10        &lt;chr&gt; NA, NA, \"Tremors\", NA, NA, NA, NA, NA, NA, NA, NA,…\n$ sideEffect11        &lt;chr&gt; NA, NA, \"Palpitations\", NA, NA, NA, NA, NA, NA, NA…\n$ sideEffect12        &lt;chr&gt; NA, NA, \"Muscle cramp\", NA, NA, NA, NA, NA, NA, NA…\n$ sideEffect13        &lt;chr&gt; NA, NA, \"Increased heart rate\", NA, NA, NA, NA, NA…\n$ sideEffect14        &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ sideEffect15        &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ sideEffect16        &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ sideEffect17        &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ sideEffect18        &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ sideEffect19        &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ sideEffect20        &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ sideEffect21        &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ sideEffect22        &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ sideEffect23        &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ sideEffect24        &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ sideEffect25        &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ sideEffect26        &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ sideEffect27        &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ sideEffect28        &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ sideEffect29        &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ sideEffect30        &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ sideEffect31        &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ sideEffect32        &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ sideEffect33        &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ sideEffect34        &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ sideEffect35        &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ sideEffect36        &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ sideEffect37        &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ sideEffect38        &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ sideEffect39        &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ sideEffect40        &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ sideEffect41        &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ use0                &lt;chr&gt; \"Treatment of Bacterial infections\", \"Treatment of…\n$ use1                &lt;chr&gt; NA, NA, NA, \"Treatment of Allergic conditions\", NA…\n$ use2                &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ use3                &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ use4                &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ `Chemical Class`    &lt;chr&gt; NA, \"Macrolides\", NA, \"Diphenylmethane Derivative\"…\n$ `Habit Forming`     &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"N…\n$ `Therapeutic Class` &lt;chr&gt; \"ANTI INFECTIVES\", \"ANTI INFECTIVES\", \"RESPIRATORY…\n$ `Action Class`      &lt;chr&gt; NA, \"Macrolides\", NA, \"H1 Antihistaminics (second …\n\n\nCode\n# finding rows and columns of data\nmed_data %&gt;% dim()\n\n\n[1] 248218     58\n\n\nA total of {r} nrow(med_data) rows are present with {r} ncol(med_data) columns in which the\n\nid is a number that can act as a primary key\nname is the name of the drugs. - substitute0 to substitue4 are alternate drugs to the drug in 2nd column that has the same use. - sideEffect0 to sideEffect41 are side-effects caused by the drug - use0 to use4 are what drug can be used to cure\nChemical Class is the chemical group of the medicine\nHabit Forming is if a drug is addictive or not\nTherapeutic Class is about how a drug is intended to work\nAction Class is categorization a drug works in the system"
  },
  {
    "objectID": "posts/meds_analysis/Medicines_usage_sideeffects.html#cleaning-the-data",
    "href": "posts/meds_analysis/Medicines_usage_sideeffects.html#cleaning-the-data",
    "title": "Medicines Side-effects and their Substitutes",
    "section": "3 Cleaning the data",
    "text": "3 Cleaning the data\nWe can’t be sure that all the data in the columns is without any excess spaces and wrongly indented commas or brackets.\n\n\nCode\n# converting all the columns of character to lower case letters\nmed_data &lt;- med_data %&gt;%\n  rename_with(~gsub(\" \", \"_\", tolower(.x))) %&gt;% \n  mutate(across(where(is_character), ~tolower(.))) %&gt;% \n  mutate(across(where(is_character), ~trimws(.)))\n\n# replacing all the '{' with '(' and '}' with ')'\n\nmed_data &lt;- med_data %&gt;% \n  mutate(chemical_class = str_replace_all(chemical_class, \"\\\\{\", \"\\\\(\"),\n         chemical_class = str_replace_all(chemical_class, \"\\\\}\", \"\\\\)\"))\n\n\n\n3.1 Finding NAs and Dulicates\nLets look at NAs in the data and the duplicates\n\n\nCode\n# finding NA's in each columns\nmed_data %&gt;% map(~sum(is.na(.))) %&gt;% unlist()\n\n\n               id              name       substitute0       substitute1 \n                0                 0              9597             14351 \n      substitute2       substitute3       substitute4       sideeffect0 \n            17985             21362             24256                 0 \n      sideeffect1       sideeffect2       sideeffect3       sideeffect4 \n             9802             18718             40580             84658 \n      sideeffect5       sideeffect6       sideeffect7       sideeffect8 \n           116960            156361            180468            199712 \n      sideeffect9      sideeffect10      sideeffect11      sideeffect12 \n           210510            220944            227887            231936 \n     sideeffect13      sideeffect14      sideeffect15      sideeffect16 \n           233491            237799            240537            242209 \n     sideeffect17      sideeffect18      sideeffect19      sideeffect20 \n           242836            243703            244272            244995 \n     sideeffect21      sideeffect22      sideeffect23      sideeffect24 \n           245093            245170            245313            245495 \n     sideeffect25      sideeffect26      sideeffect27      sideeffect28 \n           246715            246715            246724            246724 \n     sideeffect29      sideeffect30      sideeffect31      sideeffect32 \n           246780            246889            246889            246890 \n     sideeffect33      sideeffect34      sideeffect35      sideeffect36 \n           247049            247052            248216            248216 \n     sideeffect37      sideeffect38      sideeffect39      sideeffect40 \n           248216            248216            248216            248216 \n     sideeffect41              use0              use1              use2 \n           248216                 0            174853            219911 \n             use3              use4    chemical_class     habit_forming \n           240839            243247            110427                 0 \ntherapeutic_class      action_class \n               69            110182 \n\n\nCode\n# finding duplicates\nduplicated(med_data) %&gt;% sum()\n\n\n[1] 0\n\n\nThere are no duplicated values but there are so many NAs which is not helpful. Only 5 columns i.e., id, name, sideEffect0, use0, Habit Forming does not have any empty values in the column.\n\n\n3.2 Finding unique values\nEven though there no “NA”s in id and name of the drug lets make sure there are no duplicates\n\n\nCode\n# counting unique values in each column\nmed_data %&gt;% map(n_distinct) %&gt;% unlist()\n\n\n               id              name       substitute0       substitute1 \n           248218            222825             19374             16309 \n      substitute2       substitute3       substitute4       sideeffect0 \n            14289             12774             11689               326 \n      sideeffect1       sideeffect2       sideeffect3       sideeffect4 \n              335               352               363               359 \n      sideeffect5       sideeffect6       sideeffect7       sideeffect8 \n              325               299               275               254 \n      sideeffect9      sideeffect10      sideeffect11      sideeffect12 \n              232               212               182               174 \n     sideeffect13      sideeffect14      sideeffect15      sideeffect16 \n              145               121                95                78 \n     sideeffect17      sideeffect18      sideeffect19      sideeffect20 \n               66                52                42                36 \n     sideeffect21      sideeffect22      sideeffect23      sideeffect24 \n               30                25                18                14 \n     sideeffect25      sideeffect26      sideeffect27      sideeffect28 \n               11                11                10                10 \n     sideeffect29      sideeffect30      sideeffect31      sideeffect32 \n                9                 6                 6                 5 \n     sideeffect33      sideeffect34      sideeffect35      sideeffect36 \n                4                 3                 2                 2 \n     sideeffect37      sideeffect38      sideeffect39      sideeffect40 \n                2                 2                 2                 2 \n     sideeffect41              use0              use1              use2 \n                2               655               335               139 \n             use3              use4    chemical_class     habit_forming \n               74                34               833                 2 \ntherapeutic_class      action_class \n               23               432 \n\n\nThere over 2,48,218 ids but 222825 drug names are present at least 24000 names are repeated. Lets check which are repeated.\n\n\nCode\n# \nduplicated_values &lt;- med_data %&gt;% select(-id) %&gt;% duplicated()\n\nduplicated_values %&gt;% sum()\n\n\n[1] 24204\n\n\nLets remove duplicates from the datafame and create a dataset with unique values.\n\n\nCode\n# using filter function to remove duplicates\nmed_data_unique &lt;- med_data %&gt;% filter(!duplicated(select(., -id)))\n\ndim(med_data_unique)\n\n\n[1] 224014     58"
  },
  {
    "objectID": "posts/meds_analysis/Medicines_usage_sideeffects.html#data-manipulation",
    "href": "posts/meds_analysis/Medicines_usage_sideeffects.html#data-manipulation",
    "title": "Medicines Side-effects and their Substitutes",
    "section": "4 Data Manipulation",
    "text": "4 Data Manipulation\n\n4.1 Pivoting Data\nFor machines longer format data is much more readable and workable than wider format and we can drop NAs in the columns much more easily, without loosing data but it also comes at a cost while longer format data is easy for machines to read but very difficult for humans to comprehend and the number of rows can increase to very high numbers to a point that it’s not worth it.\nWe can pivot data and make the wide data format into narrow data format and make it more accessible to manipulate.\n\n\nCode\n#|label: pivoting_substitute_drug\n\n# pivoting data\nmed_data_sub &lt;- med_data_unique %&gt;% select(id:substitute4, use0:use4) %&gt;% \n  pivot_longer(cols = starts_with(\"substitute\"),\n               names_to = \"sub_num\",\n               values_to = \"substitute_drug\")\n# counting NA's\nmed_data_sub %&gt;% map(~sum(is.na(.))) %&gt;% unlist()\n\n\n             id            name            use0            use1            use2 \n              0               0               0          786770          991525 \n           use3            use4         sub_num substitute_drug \n        1086255         1097150               0           80639 \n\n\nCode\nglimpse(med_data_sub)\n\n\nRows: 1,120,070\nColumns: 9\n$ id              &lt;dbl&gt; 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, …\n$ name            &lt;chr&gt; \"augmentin 625 duo tablet\", \"augmentin 625 duo tablet\"…\n$ use0            &lt;chr&gt; \"treatment of bacterial infections\", \"treatment of bac…\n$ use1            &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ use2            &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ use3            &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ use4            &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ sub_num         &lt;chr&gt; \"substitute0\", \"substitute1\", \"substitute2\", \"substitu…\n$ substitute_drug &lt;chr&gt; \"penciclav 500 mg/125 mg tablet\", \"moxikind-cv 625 tab…\n\n\nAs you can see the data only has 9 columns and 1241090 rows.\nNow lets pivot the use of the drugs so that we can make the data more tidy which helps with removing of the duplicates and the NA values easily.\n\n\nCode\n# pivoting data \nmedi_use_pivot &lt;- \n  med_data_sub %&gt;% select(-sub_num) %&gt;% \n  pivot_longer(cols = starts_with(\"use\"),\n               names_to = \"use_num\",\n               values_to = \"use\") %&gt;% \n  select(-use_num) %&gt;% filter(!is.na(use))\n\n# checking for NA values\nmedi_use_pivot %&gt;% map(~sum(is.na(.))) %&gt;% unlist()\n\n\n             id            name substitute_drug             use \n              0               0          111133               0 \n\n\nCode\n# checking for duplicates\nmedi_use_pivot %&gt;% duplicated() %&gt;% sum()\n\n\n[1] 88192\n\n\nCode\n# removing duplicated data\nmed_use &lt;- medi_use_pivot %&gt;% filter(!duplicated(.))\n\n# glimpse of data\nglimpse(med_use)\n\n\nRows: 1,550,458\nColumns: 4\n$ id              &lt;dbl&gt; 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, …\n$ name            &lt;chr&gt; \"augmentin 625 duo tablet\", \"augmentin 625 duo tablet\"…\n$ substitute_drug &lt;chr&gt; \"penciclav 500 mg/125 mg tablet\", \"moxikind-cv 625 tab…\n$ use             &lt;chr&gt; \"treatment of bacterial infections\", \"treatment of bac…\n\n\nWe can use pivot method for sideEffects to convert the data into a longer format. I am doing this case by case and not all in a single table because that would cause very long tables and a lot of NAs which would be hard to filter and we can join different tables using *_join functions with id column as it can act as a primary key.\n\n\nCode\n# pivoting data with side-effect columns\n\nside_effect_med &lt;- \n  med_data_unique %&gt;% select(id, name, sideeffect0:sideeffect41) %&gt;%\n  pivot_longer(cols = starts_with(\"sideeffect\"),\n               names_to = \"sideeffect_num\",\n               values_to = \"side_effects\") %&gt;% \n  select(-sideeffect_num)\n\n# counting NA and duplicates\nside_effect_med %&gt;% map(~sum(is.na(.))) %&gt;% unlist\n\n\n          id         name side_effects \n           0            0      7950288 \n\n\nCode\n# dropping NA's and duplicates\nside_effect_med &lt;- side_effect_med %&gt;% drop_na() %&gt;% \n  filter(!duplicated(.))\n\n# finding duplicates\nduplicated(side_effect_med) %&gt;% sum()\n\n\n[1] 0\n\n\nCode\nglimpse(side_effect_med)\n\n\nRows: 1,458,295\nColumns: 3\n$ id           &lt;dbl&gt; 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, …\n$ name         &lt;chr&gt; \"augmentin 625 duo tablet\", \"augmentin 625 duo tablet\", \"…\n$ side_effects &lt;chr&gt; \"vomiting\", \"nausea\", \"diarrhea\", \"vomiting\", \"nausea\", \"…\n\n\nNow lets use pivoted data to plot graphs"
  },
  {
    "objectID": "posts/meds_analysis/Medicines_usage_sideeffects.html#visualising-with-ggplot2",
    "href": "posts/meds_analysis/Medicines_usage_sideeffects.html#visualising-with-ggplot2",
    "title": "Medicines Side-effects and their Substitutes",
    "section": "5 Visualising with ggplot2",
    "text": "5 Visualising with ggplot2\nggplot2 is one of the most versatile packages I have come across for the purpose of visualizing using Grammar of Graphics\n\n5.1 Bar plots\nLet’s find out and plot to which class most of the drugs in data belong to.\n\n\nCode\nchem_cl_top_10 &lt;-  \n  med_data_unique %&gt;% select(name, chemical_class) %&gt;% \n  count(chemical_class) %&gt;% rename(\"number_of_meds\" = n) %&gt;% \n  slice_max(number_of_meds, n=10) %&gt;%\n  filter(chemical_class != \"NA\") %&gt;% \n  mutate(chemical_class = str_to_sentence(chemical_class))\n\nchem_cl_top_10 %&gt;% \n  ggplot(aes(x = fct_reorder(chemical_class, number_of_meds),\n             y = number_of_meds)) +\n  geom_col(aes(fill = chemical_class)) +\n  theme_minimal() +\n  theme(legend.position = \"none\",\n        axis.text.y = element_text(size = 10)) +\n  labs(x = \"Chemical Class\", y= \"Number of Medicines\",\n       title = \"Most Common Chemical Class\"\n       ) +\n  theme(plot.title = element_text(size = 20)) +\n  scale_fill_brewer(palette = \"Set1\") + coord_flip()\n\n\n\n\n\nMost common drug classes\n\n\n\n\nBy the graph we know that most of the drugs in data belong to the chemical class {r} chem_cl_top_10[1,1] with {r} chem_cl_top_10[1,2] drugs belong to that class, followed by {r} chem_cl_top_10[2,1] with {r} chem_cl_top_10[2,2] followed by {r} chem_cl_top_10[3,1], {r} chem_cl_top_10[4, 1] and {r} chem_cl_top_10[5, 1].\nNow, that we have some basic idea of the data lets answer some questions"
  },
  {
    "objectID": "posts/meds_analysis/Medicines_usage_sideeffects.html#finding-answers-to-specific-questions",
    "href": "posts/meds_analysis/Medicines_usage_sideeffects.html#finding-answers-to-specific-questions",
    "title": "Medicines Side-effects and their Substitutes",
    "section": "6 Finding Answers to Specific Questions",
    "text": "6 Finding Answers to Specific Questions\nNow, that we have some basic idea of the data lets answer some questions\nLets begin with simple ones\n\n6.1 Addictive drugs\nLets find the most addictive drugs in the data set and to which chemical class they belong to.\n\n\nCode\nhabit_forming_classes &lt;- \n  med_data_unique %&gt;% filter(habit_forming == \"yes\") %&gt;% \n  select(name, chemical_class) %&gt;%\n  count(chemical_class, sort = TRUE)\n\nhabit_forming_classes\n\n\n# A tibble: 17 × 2\n   chemical_class                                   n\n   &lt;chr&gt;                                        &lt;int&gt;\n 1 &lt;NA&gt;                                          2532\n 2 benzodiazepines derivative                    2025\n 3 anisole derivative                             365\n 4 imidazopyridine derivative                     194\n 5 benzodiazepine derivative                       83\n 6 barbituric acid derivative                      55\n 7 diphenylmethane derivative                      49\n 8 cyclopyrrolone derivative                       28\n 9 phenanthrenes derivatives                       26\n10 phenanthrenes derivative                        22\n11 aralkylamine derivative                         21\n12 benzomorphan derivatives                        20\n13 phenylpiperidine derivatives                    12\n14 pyrazolopyrimidine derivative                   11\n15 ultrashort-acting barbituric acid derivative    11\n16 amphetamines derivatives                         6\n17 phenylheptylamines derivative                    1\n\n\nWe can see in the table that {r} habit_forming_classes[1,2] drugs which are addictive does not have their class mentioned, while {r} habit_forming_classes[2,1] have {r} habit_forming_classes[2,2] drugs which are habit forming.\n\n\n6.2 No Substitute Drugs\nFind the drugs with no substitute drugs, that have less side-effects, is not habit forming, and has many uses\n\n\nCode\nmed_data_unique %&gt;%\n  # finding drugs with no sbustitute\n  filter(if_all(substitute0:substitute4, is.na) &\n         # medicine with no one side-effect\n         if_all(sideeffect1:sideeffect41, is.na) &\n           sideeffect0 == \"no common side effects seen\" &\n         # Medicine with most uses\n         if_all(use0:use2, ~!is.na(.)) &\n         # not habit forming\n         habit_forming == \"no\" &\n         # Chemical Class, therapeutic class,action class is known\n         !is.na(chemical_class) &\n         !is.na(therapeutic_class) &\n         !is.na(action_class)) %&gt;% \n  head()\n\n\n# A tibble: 1 × 58\n     id name         substitute0 substitute1 substitute2 substitute3 substitute4\n  &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;       &lt;chr&gt;       &lt;chr&gt;       &lt;chr&gt;       &lt;chr&gt;      \n1 33940 bentoform d… &lt;NA&gt;        &lt;NA&gt;        &lt;NA&gt;        &lt;NA&gt;        &lt;NA&gt;       \n# ℹ 51 more variables: sideeffect0 &lt;chr&gt;, sideeffect1 &lt;chr&gt;, sideeffect2 &lt;chr&gt;,\n#   sideeffect3 &lt;chr&gt;, sideeffect4 &lt;chr&gt;, sideeffect5 &lt;chr&gt;, sideeffect6 &lt;chr&gt;,\n#   sideeffect7 &lt;chr&gt;, sideeffect8 &lt;chr&gt;, sideeffect9 &lt;chr&gt;,\n#   sideeffect10 &lt;chr&gt;, sideeffect11 &lt;chr&gt;, sideeffect12 &lt;chr&gt;,\n#   sideeffect13 &lt;chr&gt;, sideeffect14 &lt;chr&gt;, sideeffect15 &lt;chr&gt;,\n#   sideeffect16 &lt;chr&gt;, sideeffect17 &lt;chr&gt;, sideeffect18 &lt;chr&gt;,\n#   sideeffect19 &lt;chr&gt;, sideeffect20 &lt;chr&gt;, sideeffect21 &lt;chr&gt;, …\n\n\nAmong the {r} nrow(med_data_unique) drugs only Betoform Dental Gel is the drug with No known side-effects, no alternate drugs, is not habit forming and has a known Chemical Class.\n\n\n6.3 Most Popular Drug form\nLets find the most common type of form i.e, Tablet, Tonic, etc. in the data set.\n\n\nCode\nmed_data_unique[2,2]\n\n\n# A tibble: 1 × 1\n  name               \n  &lt;chr&gt;              \n1 azithral 500 tablet\n\n\nThe medicine name in the end contains its form but it might not be true for all so lets do a string search so that the for is detected correctly in which it is sold or consumed if we extract it into a separate column we can know the most popular type.\nMaking a new dataframe by detecting strings of the column name\n\n\nCode\nmed_form_df &lt;- \n  med_data_unique %&gt;%\n  select(name) %&gt;%\n  mutate(med_type = case_when(\n    # searching for specific type of medicine and making it a column\n    str_detect(name, \"tablet\") ~ \"tablet\",\n    str_detect(name, \"capsule\") ~ \"capsule\",\n    str_detect(name, \"syrup\") ~ \"syrup\",\n    str_detect(name, \"oral suspension\") ~ \"oral suspension\",\n    str_detect(name, \"suspension\") ~ \"suspension\",\n    str_detect(name, \"cream|lotion\") ~ \"cream\",\n    str_detect(name, \".*gel\") ~ \"gel\",\n    str_detect(name, \"drop|drops\") ~ \"drop\",\n    str_detect(name, \"bar|bars\") ~ \"bar\",\n    str_detect(name, \"solution|solutions\") ~ \"solution\",\n    str_detect(name, \".*cap|.*caps\") ~ \"caps\",\n    str_detect(name, \"infusion\") ~ \"infusion\",\n    str_detect(name, \"injection\") ~ \"injection\",\n    str_detect(name, \"granules\") ~ \"granules\",\n    TRUE ~ \"others\"\n  )) %&gt;%\n  filter(!is.na(med_type)) %&gt;%\n  count(med_type, sort = TRUE)\n\nmed_form_df\n\n\n# A tibble: 15 × 2\n   med_type             n\n   &lt;chr&gt;            &lt;int&gt;\n 1 tablet          135267\n 2 injection        26643\n 3 capsule          18966\n 4 syrup            16003\n 5 drop              5508\n 6 cream             5502\n 7 others            5319\n 8 oral suspension   4373\n 9 suspension        2510\n10 gel               1731\n11 solution          1014\n12 infusion           767\n13 caps               275\n14 granules            78\n15 bar                 58\n\n\nAs we can see that Tablets is the most common form with {r} med_form_df[1,2] followed by Injection with {r} med_form_df[2,2], Capsules and Syrups take third and fourth place respectively.\n\n\n6.4 Most Common Side-Effects\nThis is where pivoting data comes to the use, we cannot find the most common side-effect as there are 42 columns of them with NAs in the middle which gets complicated. By pivoting data to longer format we make each side-effect has its own row which leads to removal of NA easily.\n\n\nCode\nside_effect_med %&gt;% \n  filter(side_effects != \"no common side effects seen\") %&gt;%\n  count(side_effects, sort = TRUE) %&gt;%  \n  slice_max(n, n = 10) %&gt;% \n  mutate(side_effects = str_to_title(side_effects)) %&gt;% \n  ggplot(aes(x = fct_reorder(side_effects, n), y = n)) +\n  geom_col(aes(fill = n)) +\n  scale_y_continuous(\n    labels = scales::number_format(scale = 1e-3, suffix = \"K\")\n  ) +\n  labs(x = \"Side-effects\", y = \"Frequency\", \n       title = \"Most Common Side-effects\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\nMost Common Side-effects\n\n\n\n\n\n\n6.5 Action class with most unique Side-effects\nLets find which action_class has most unique side-effects in the data\n\n\nCode\naction_class_sideeffects &lt;-\n  med_data_unique %&gt;% select(name, sideeffect0:sideeffect41,\n                           action_class) %&gt;% \n  pivot_longer(cols = sideeffect0:sideeffect41,\n               names_to = \"sideeffect_num\",\n               values_to = \"side_effect\") %&gt;%\n  # removing medicine name and sideeffect_num\n  select(-sideeffect_num, -name) %&gt;%\n  # removing duplicates so that only unique side-effect & action_class remain\n  filter(!duplicated(.))\n  \naction_class_sideeffects %&gt;% count(action_class, sort = TRUE) %&gt;% \n  drop_na() %&gt;% slice_max(n = 10, n)\n\n\n# A tibble: 10 × 2\n   action_class                        n\n   &lt;chr&gt;                           &lt;int&gt;\n 1 glucocorticoids                    88\n 2 tyrosine kinase inhibitors         79\n 3 vitamins                           73\n 4 anticancer-others                  66\n 5 antimetabolites                    66\n 6 atypical antipsychotics            65\n 7 sodium channel modulators (aed)    62\n 8 alkaloids-cytotoxic agents         56\n 9 alkylating agent                   53\n10 quinolones/ fluroquinolones        53\n\n\nDrugs with Glucocorticoids have 88 unique side-effects, followed by Tyrosine Kinase Inhibitors with 79 , Vitamins and Anticancer-others come next."
  },
  {
    "objectID": "posts/diamonds_ml/ml_diamonds.html",
    "href": "posts/diamonds_ml/ml_diamonds.html",
    "title": "Predict Price of Diamonds",
    "section": "",
    "text": "Building a model to predict the price of the diamonds using tidymodels.\nDiamonds data set is readily available to use through the ggplot2 library in the tidyverse and we will be using this data set predict the prices of the other diamonds.\nIn the data set various parameters of diamonds are given and each of these parameters may or may not effect the price of the diamonds.\n\nlibrary(tidyverse)\nlibrary(plotly)\nlibrary(DT)\n\ndata(\"diamonds\")\ndiamonds\n\n# A tibble: 53,940 × 10\n   carat cut       color clarity depth table price     x     y     z\n   &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n 2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n 3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n 4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n 5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n 6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n 7  0.24 Very Good I     VVS1     62.3    57   336  3.95  3.98  2.47\n 8  0.26 Very Good H     SI1      61.9    55   337  4.07  4.11  2.53\n 9  0.22 Fair      E     VS2      65.1    61   337  3.87  3.78  2.49\n10  0.23 Very Good H     VS1      59.4    61   338  4     4.05  2.39\n# ℹ 53,930 more rows\n\n\nData has over 50,000 observations which is good for modeling.\n\n\n\nThe diamonds data set is available to explore in ggplot2 library as mentioned above.\nLet’s check for NA’s before exploring the data\n\ndiamonds %&gt;% map( ~sum(is.na(.))) %&gt;% unlist()\n\n  carat     cut   color clarity   depth   table   price       x       y       z \n      0       0       0       0       0       0       0       0       0       0 \n\n\nIt’s really good that there are no NA’s but we have to be careful of the 0 in the numeric columns.\n\ndiamonds %&gt;% select(carat, x, y, z) %&gt;% arrange(x, y, z)\n\n# A tibble: 53,940 × 4\n   carat     x     y     z\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  1     0     0     0   \n 2  1.14  0     0     0   \n 3  1.56  0     0     0   \n 4  1.2   0     0     0   \n 5  2.25  0     0     0   \n 6  0.71  0     0     0   \n 7  0.71  0     0     0   \n 8  1.07  0     6.62  0   \n 9  0.2   3.73  3.68  2.31\n10  0.2   3.73  3.71  2.33\n# ℹ 53,930 more rows\n\n\nDiamonds cannot have a x(length), y(width), z(depth) of 0 and have weight. So let’s replace these values with NA or we can remove them out completely too.\n\ndiamonds %&gt;% mutate(x = if_else(x == \"0\", NA, x),\n                    y = if_else(y == \"0\", NA, y),\n                    z = if_else(y == \"0\", NA, z)) %&gt;% \n  datatable()\n\n\n\n\n\nNow lets visualize the distribution of the diamonds.\n\nfrequency_poly &lt;- diamonds %&gt;% ggplot(aes(carat)) + geom_freqpoly(binwidth = 0.05)\n\nggplotly(frequency_poly)\n\n\n\n\n\n\n\nFigure 1: Frequency polygon plot\n\n\n\n\nFrom the Figure 1 we can observe that - Most of the diamonds are between 0.2 to 1.5 carats. - There are peaks which means higher number of diamonds at whole and common fractions.\nMy general knowledge is that the weight i.e, carat of the diamond influences the price most. Let’s visualize that.\n\ndiamonds %&gt;% ggplot(aes(carat, price)) + geom_hex(bins = 50)\n\n\n\n\n\n\n\n\nThe price tends to follow exponential curve the log2() curve, we can confirm this by another graph.\n\ndiamonds %&gt;% filter(carat &lt; 2.5) %&gt;% \n  mutate(log_price = log10(price),\n                    log_carat = log10(carat)) %&gt;% \n  ggplot(aes(log_carat, log_price)) + geom_hex(bins = 50) +\n  geom_smooth(method = \"lm\", formula = y ~ splines::bs(x, 3),\n              se = FALSE, linewidth = 1.5)\n\n\n\n\n\n\n\nFigure 2: Log of carat vs Log of Price at base 2\n\n\n\n\n\nThe above Figure 2 shows that once we apply log2() to both price and carat the relationship mostly looks to be linear.\n\ndiamonds %&gt;% filter(carat &lt;= 2.5) %&gt;% ggplot(aes(carat, price)) +\n  geom_point(alpha = 0.1, aes(color = price)) +\n  geom_smooth(method = \"lm\", formula = y ~ splines::bs(x, 3),\n              se = FALSE, linewidth = 1.5) +\n  scale_color_viridis_c()\n\n\n\n\n\n\n\n\nWe can see that price jumps when the weight is exactly or greater than to the whole and common fractions such as 0.5, 1.0, 1.5 and 2.\n\nlibrary(patchwork)\n\nplot_parameter &lt;- function(param){\n  ggplot(diamonds, aes(fct_reorder({{param}}, price), price)) +\n    geom_boxplot() + stat_summary(fun = mean, geom = \"point\") +\n    labs(x = as_label(substitute(param)))\n}\n\n(plot_parameter(cut) + plot_parameter(color)) /\n  (plot_parameter(clarity))\n\n\n\n\n\n\n\n\nLow quality diamonds with Fair cut and low quality color seems to have very high price. So now lets use tidymodels to model the data using rand_forest\n\n\n\nAs every parameter in the data is important for the price prediction we are going to keep all the columns intact.\n\nlibrary(tidymodels)\nset.seed(2023)\n\ndiamonds_2 &lt;- diamonds %&gt;% select(-depth) %&gt;% \n  mutate(price = log2(price), carat = log2(carat))\n\ndiamonds_split &lt;- initial_split(diamonds_2, strata = carat, prop = 0.8)\ndiamonds_split\n\n&lt;Training/Testing/Total&gt;\n&lt;43150/10790/53940&gt;\n\ndiamonds_train &lt;- training(diamonds_split)\ndiamonds_test &lt;- testing(diamonds_split)\n\nI am using strata with carat as most of the diamonds are not properly distributed yet all diamonds of different weight should be well represented.\n\ndiamonds_folds &lt;- vfold_cv(diamonds_train, strata = carat)\ndiamonds_folds\n\n#  10-fold cross-validation using stratification \n# A tibble: 10 × 2\n   splits               id    \n   &lt;list&gt;               &lt;chr&gt; \n 1 &lt;split [38833/4317]&gt; Fold01\n 2 &lt;split [38833/4317]&gt; Fold02\n 3 &lt;split [38834/4316]&gt; Fold03\n 4 &lt;split [38835/4315]&gt; Fold04\n 5 &lt;split [38835/4315]&gt; Fold05\n 6 &lt;split [38835/4315]&gt; Fold06\n 7 &lt;split [38836/4314]&gt; Fold07\n 8 &lt;split [38836/4314]&gt; Fold08\n 9 &lt;split [38836/4314]&gt; Fold09\n10 &lt;split [38837/4313]&gt; Fold10\n\n\nI think rand_forest will work better on this data set but lets compare both Linear Regression models and Random Forest models.\n\nlm_spec &lt;- linear_reg() %&gt;% set_engine(\"glm\")\nlm_spec\n\nLinear Regression Model Specification (regression)\n\nComputational engine: glm \n\nrf_spec &lt;- rand_forest(trees = 1000) %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"ranger\")\nrf_spec\n\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  trees = 1000\n\nComputational engine: ranger \n\n\nWe still need to maniplulate some parts of the data like price and carat so that they are optimised which can be done using recipe library.\n\nbase_recp &lt;- \n  recipe(price ~ ., data = diamonds_train) %&gt;% \n  step_normalize(all_numeric_predictors())\n\nind_recp &lt;- base_recp %&gt;% \n  step_dummy(all_nominal_predictors())\n\nspline_recp &lt;- ind_recp %&gt;% \n  step_bs(carat)\n\nNext let’s start putting together a tidymodels workflow(), a helper object to help manage modeling pipelines with pieces that fit together like Lego blocks.\n\ndiamonds_set &lt;- \n  workflow_set(\n    list(base_recp, ind_recp, spline_recp),\n    list(lm_spec, rf_spec))\n\ndiamonds_set\n\n# A workflow set/tibble: 6 × 4\n  wflow_id             info             option    result    \n  &lt;chr&gt;                &lt;list&gt;           &lt;list&gt;    &lt;list&gt;    \n1 recipe_1_linear_reg  &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n2 recipe_1_rand_forest &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n3 recipe_2_linear_reg  &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n4 recipe_2_rand_forest &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n5 recipe_3_linear_reg  &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n6 recipe_3_rand_forest &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n\n\nLet’s fit the two models we prepared for the data. First code block contains linear regression model and the second contains the random_forest model.\n\ndoParallel::registerDoParallel()\n\ndiamonds_rs &lt;- \n  workflow_map(\n    diamonds_set,\n    \"fit_resamples\",\n    resamples = diamonds_folds\n  )\ndiamonds_rs\n\n# A workflow set/tibble: 6 × 4\n  wflow_id             info             option    result   \n  &lt;chr&gt;                &lt;list&gt;           &lt;list&gt;    &lt;list&gt;   \n1 recipe_1_linear_reg  &lt;tibble [1 × 4]&gt; &lt;opts[1]&gt; &lt;rsmp[+]&gt;\n2 recipe_1_rand_forest &lt;tibble [1 × 4]&gt; &lt;opts[1]&gt; &lt;rsmp[+]&gt;\n3 recipe_2_linear_reg  &lt;tibble [1 × 4]&gt; &lt;opts[1]&gt; &lt;rsmp[+]&gt;\n4 recipe_2_rand_forest &lt;tibble [1 × 4]&gt; &lt;opts[1]&gt; &lt;rsmp[+]&gt;\n5 recipe_3_linear_reg  &lt;tibble [1 × 4]&gt; &lt;opts[1]&gt; &lt;rsmp[+]&gt;\n6 recipe_3_rand_forest &lt;tibble [1 × 4]&gt; &lt;opts[1]&gt; &lt;rsmp[+]&gt;\n\n\n\n\n\nWe can evaluate model by using autoplot and collect_metrics functions.\n\nautoplot(diamonds_rs)\n\n\n\n\n\n\n\n\nIn the plot it seems that the difference between the rand_forest and linear_reg is very high but when we look at the metrics table we realise it’s not that much.\n\ncollect_metrics(diamonds_rs)\n\n# A tibble: 12 × 9\n   wflow_id         .config preproc model .metric .estimator  mean     n std_err\n   &lt;chr&gt;            &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n 1 recipe_1_linear… Prepro… recipe  line… rmse    standard   0.194    10 9.25e-4\n 2 recipe_1_linear… Prepro… recipe  line… rsq     standard   0.983    10 1.58e-4\n 3 recipe_1_rand_f… Prepro… recipe  rand… rmse    standard   0.137    10 1.17e-3\n 4 recipe_1_rand_f… Prepro… recipe  rand… rsq     standard   0.991    10 1.33e-4\n 5 recipe_2_linear… Prepro… recipe  line… rmse    standard   0.194    10 9.25e-4\n 6 recipe_2_linear… Prepro… recipe  line… rsq     standard   0.983    10 1.58e-4\n 7 recipe_2_rand_f… Prepro… recipe  rand… rmse    standard   0.140    10 1.44e-3\n 8 recipe_2_rand_f… Prepro… recipe  rand… rsq     standard   0.991    10 1.65e-4\n 9 recipe_3_linear… Prepro… recipe  line… rmse    standard   0.183    10 1.01e-3\n10 recipe_3_linear… Prepro… recipe  line… rsq     standard   0.984    10 1.67e-4\n11 recipe_3_rand_f… Prepro… recipe  rand… rmse    standard   0.137    10 1.06e-3\n12 recipe_3_rand_f… Prepro… recipe  rand… rsq     standard   0.991    10 1.19e-4\n\n\nFrom the metrics table we can see that recipe_1_rand_forest seems to perform the best.\n\nfinal_fit &lt;-\n  extract_workflow(diamonds_rs, \"recipe_1_rand_forest\") %&gt;%\n  fit(diamonds_train)\n\n\nranger_model &lt;- pull_workflow_fit(final_fit)\nranger_model\n\nparsnip model object\n\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, num.trees = ~1000,      num.threads = 1, verbose = FALSE, seed = sample.int(10^5,          1)) \n\nType:                             Regression \nNumber of trees:                  1000 \nSample size:                      43150 \nNumber of independent variables:  8 \nMtry:                             2 \nTarget node size:                 5 \nVariable importance mode:         none \nSplitrule:                        variance \nOOB prediction error (MSE):       0.01859615 \nR squared (OOB):                  0.9913472 \n\n\nLet’s fit the test set to the model.\n\nfinal_predic &lt;- predict(object = final_fit,\n                        new_data = diamonds_test)\n\nfinal_predic\n\n# A tibble: 10,790 × 1\n   .pred\n   &lt;dbl&gt;\n 1  8.85\n 2  8.64\n 3  8.66\n 4  8.72\n 5  8.70\n 6  8.62\n 7  8.87\n 8  8.73\n 9  8.83\n10  8.63\n# ℹ 10,780 more rows"
  },
  {
    "objectID": "posts/diamonds_ml/ml_diamonds.html#predicting-diamonds-price",
    "href": "posts/diamonds_ml/ml_diamonds.html#predicting-diamonds-price",
    "title": "Predict Price of Diamonds",
    "section": "",
    "text": "Building a model to predict the price of the diamonds using tidymodels.\nDiamonds data set is readily available to use through the ggplot2 library in the tidyverse and we will be using this data set predict the prices of the other diamonds.\nIn the data set various parameters of diamonds are given and each of these parameters may or may not effect the price of the diamonds.\n\nlibrary(tidyverse)\nlibrary(plotly)\nlibrary(DT)\n\ndata(\"diamonds\")\ndiamonds\n\n# A tibble: 53,940 × 10\n   carat cut       color clarity depth table price     x     y     z\n   &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n 2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n 3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n 4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n 5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n 6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n 7  0.24 Very Good I     VVS1     62.3    57   336  3.95  3.98  2.47\n 8  0.26 Very Good H     SI1      61.9    55   337  4.07  4.11  2.53\n 9  0.22 Fair      E     VS2      65.1    61   337  3.87  3.78  2.49\n10  0.23 Very Good H     VS1      59.4    61   338  4     4.05  2.39\n# ℹ 53,930 more rows\n\n\nData has over 50,000 observations which is good for modeling.\n\n\n\nThe diamonds data set is available to explore in ggplot2 library as mentioned above.\nLet’s check for NA’s before exploring the data\n\ndiamonds %&gt;% map( ~sum(is.na(.))) %&gt;% unlist()\n\n  carat     cut   color clarity   depth   table   price       x       y       z \n      0       0       0       0       0       0       0       0       0       0 \n\n\nIt’s really good that there are no NA’s but we have to be careful of the 0 in the numeric columns.\n\ndiamonds %&gt;% select(carat, x, y, z) %&gt;% arrange(x, y, z)\n\n# A tibble: 53,940 × 4\n   carat     x     y     z\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  1     0     0     0   \n 2  1.14  0     0     0   \n 3  1.56  0     0     0   \n 4  1.2   0     0     0   \n 5  2.25  0     0     0   \n 6  0.71  0     0     0   \n 7  0.71  0     0     0   \n 8  1.07  0     6.62  0   \n 9  0.2   3.73  3.68  2.31\n10  0.2   3.73  3.71  2.33\n# ℹ 53,930 more rows\n\n\nDiamonds cannot have a x(length), y(width), z(depth) of 0 and have weight. So let’s replace these values with NA or we can remove them out completely too.\n\ndiamonds %&gt;% mutate(x = if_else(x == \"0\", NA, x),\n                    y = if_else(y == \"0\", NA, y),\n                    z = if_else(y == \"0\", NA, z)) %&gt;% \n  datatable()\n\n\n\n\n\nNow lets visualize the distribution of the diamonds.\n\nfrequency_poly &lt;- diamonds %&gt;% ggplot(aes(carat)) + geom_freqpoly(binwidth = 0.05)\n\nggplotly(frequency_poly)\n\n\n\n\n\n\n\nFigure 1: Frequency polygon plot\n\n\n\n\nFrom the Figure 1 we can observe that - Most of the diamonds are between 0.2 to 1.5 carats. - There are peaks which means higher number of diamonds at whole and common fractions.\nMy general knowledge is that the weight i.e, carat of the diamond influences the price most. Let’s visualize that.\n\ndiamonds %&gt;% ggplot(aes(carat, price)) + geom_hex(bins = 50)\n\n\n\n\n\n\n\n\nThe price tends to follow exponential curve the log2() curve, we can confirm this by another graph.\n\ndiamonds %&gt;% filter(carat &lt; 2.5) %&gt;% \n  mutate(log_price = log10(price),\n                    log_carat = log10(carat)) %&gt;% \n  ggplot(aes(log_carat, log_price)) + geom_hex(bins = 50) +\n  geom_smooth(method = \"lm\", formula = y ~ splines::bs(x, 3),\n              se = FALSE, linewidth = 1.5)\n\n\n\n\n\n\n\nFigure 2: Log of carat vs Log of Price at base 2\n\n\n\n\n\nThe above Figure 2 shows that once we apply log2() to both price and carat the relationship mostly looks to be linear.\n\ndiamonds %&gt;% filter(carat &lt;= 2.5) %&gt;% ggplot(aes(carat, price)) +\n  geom_point(alpha = 0.1, aes(color = price)) +\n  geom_smooth(method = \"lm\", formula = y ~ splines::bs(x, 3),\n              se = FALSE, linewidth = 1.5) +\n  scale_color_viridis_c()\n\n\n\n\n\n\n\n\nWe can see that price jumps when the weight is exactly or greater than to the whole and common fractions such as 0.5, 1.0, 1.5 and 2.\n\nlibrary(patchwork)\n\nplot_parameter &lt;- function(param){\n  ggplot(diamonds, aes(fct_reorder({{param}}, price), price)) +\n    geom_boxplot() + stat_summary(fun = mean, geom = \"point\") +\n    labs(x = as_label(substitute(param)))\n}\n\n(plot_parameter(cut) + plot_parameter(color)) /\n  (plot_parameter(clarity))\n\n\n\n\n\n\n\n\nLow quality diamonds with Fair cut and low quality color seems to have very high price. So now lets use tidymodels to model the data using rand_forest\n\n\n\nAs every parameter in the data is important for the price prediction we are going to keep all the columns intact.\n\nlibrary(tidymodels)\nset.seed(2023)\n\ndiamonds_2 &lt;- diamonds %&gt;% select(-depth) %&gt;% \n  mutate(price = log2(price), carat = log2(carat))\n\ndiamonds_split &lt;- initial_split(diamonds_2, strata = carat, prop = 0.8)\ndiamonds_split\n\n&lt;Training/Testing/Total&gt;\n&lt;43150/10790/53940&gt;\n\ndiamonds_train &lt;- training(diamonds_split)\ndiamonds_test &lt;- testing(diamonds_split)\n\nI am using strata with carat as most of the diamonds are not properly distributed yet all diamonds of different weight should be well represented.\n\ndiamonds_folds &lt;- vfold_cv(diamonds_train, strata = carat)\ndiamonds_folds\n\n#  10-fold cross-validation using stratification \n# A tibble: 10 × 2\n   splits               id    \n   &lt;list&gt;               &lt;chr&gt; \n 1 &lt;split [38833/4317]&gt; Fold01\n 2 &lt;split [38833/4317]&gt; Fold02\n 3 &lt;split [38834/4316]&gt; Fold03\n 4 &lt;split [38835/4315]&gt; Fold04\n 5 &lt;split [38835/4315]&gt; Fold05\n 6 &lt;split [38835/4315]&gt; Fold06\n 7 &lt;split [38836/4314]&gt; Fold07\n 8 &lt;split [38836/4314]&gt; Fold08\n 9 &lt;split [38836/4314]&gt; Fold09\n10 &lt;split [38837/4313]&gt; Fold10\n\n\nI think rand_forest will work better on this data set but lets compare both Linear Regression models and Random Forest models.\n\nlm_spec &lt;- linear_reg() %&gt;% set_engine(\"glm\")\nlm_spec\n\nLinear Regression Model Specification (regression)\n\nComputational engine: glm \n\nrf_spec &lt;- rand_forest(trees = 1000) %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"ranger\")\nrf_spec\n\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  trees = 1000\n\nComputational engine: ranger \n\n\nWe still need to maniplulate some parts of the data like price and carat so that they are optimised which can be done using recipe library.\n\nbase_recp &lt;- \n  recipe(price ~ ., data = diamonds_train) %&gt;% \n  step_normalize(all_numeric_predictors())\n\nind_recp &lt;- base_recp %&gt;% \n  step_dummy(all_nominal_predictors())\n\nspline_recp &lt;- ind_recp %&gt;% \n  step_bs(carat)\n\nNext let’s start putting together a tidymodels workflow(), a helper object to help manage modeling pipelines with pieces that fit together like Lego blocks.\n\ndiamonds_set &lt;- \n  workflow_set(\n    list(base_recp, ind_recp, spline_recp),\n    list(lm_spec, rf_spec))\n\ndiamonds_set\n\n# A workflow set/tibble: 6 × 4\n  wflow_id             info             option    result    \n  &lt;chr&gt;                &lt;list&gt;           &lt;list&gt;    &lt;list&gt;    \n1 recipe_1_linear_reg  &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n2 recipe_1_rand_forest &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n3 recipe_2_linear_reg  &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n4 recipe_2_rand_forest &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n5 recipe_3_linear_reg  &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n6 recipe_3_rand_forest &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n\n\nLet’s fit the two models we prepared for the data. First code block contains linear regression model and the second contains the random_forest model.\n\ndoParallel::registerDoParallel()\n\ndiamonds_rs &lt;- \n  workflow_map(\n    diamonds_set,\n    \"fit_resamples\",\n    resamples = diamonds_folds\n  )\ndiamonds_rs\n\n# A workflow set/tibble: 6 × 4\n  wflow_id             info             option    result   \n  &lt;chr&gt;                &lt;list&gt;           &lt;list&gt;    &lt;list&gt;   \n1 recipe_1_linear_reg  &lt;tibble [1 × 4]&gt; &lt;opts[1]&gt; &lt;rsmp[+]&gt;\n2 recipe_1_rand_forest &lt;tibble [1 × 4]&gt; &lt;opts[1]&gt; &lt;rsmp[+]&gt;\n3 recipe_2_linear_reg  &lt;tibble [1 × 4]&gt; &lt;opts[1]&gt; &lt;rsmp[+]&gt;\n4 recipe_2_rand_forest &lt;tibble [1 × 4]&gt; &lt;opts[1]&gt; &lt;rsmp[+]&gt;\n5 recipe_3_linear_reg  &lt;tibble [1 × 4]&gt; &lt;opts[1]&gt; &lt;rsmp[+]&gt;\n6 recipe_3_rand_forest &lt;tibble [1 × 4]&gt; &lt;opts[1]&gt; &lt;rsmp[+]&gt;\n\n\n\n\n\nWe can evaluate model by using autoplot and collect_metrics functions.\n\nautoplot(diamonds_rs)\n\n\n\n\n\n\n\n\nIn the plot it seems that the difference between the rand_forest and linear_reg is very high but when we look at the metrics table we realise it’s not that much.\n\ncollect_metrics(diamonds_rs)\n\n# A tibble: 12 × 9\n   wflow_id         .config preproc model .metric .estimator  mean     n std_err\n   &lt;chr&gt;            &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n 1 recipe_1_linear… Prepro… recipe  line… rmse    standard   0.194    10 9.25e-4\n 2 recipe_1_linear… Prepro… recipe  line… rsq     standard   0.983    10 1.58e-4\n 3 recipe_1_rand_f… Prepro… recipe  rand… rmse    standard   0.137    10 1.17e-3\n 4 recipe_1_rand_f… Prepro… recipe  rand… rsq     standard   0.991    10 1.33e-4\n 5 recipe_2_linear… Prepro… recipe  line… rmse    standard   0.194    10 9.25e-4\n 6 recipe_2_linear… Prepro… recipe  line… rsq     standard   0.983    10 1.58e-4\n 7 recipe_2_rand_f… Prepro… recipe  rand… rmse    standard   0.140    10 1.44e-3\n 8 recipe_2_rand_f… Prepro… recipe  rand… rsq     standard   0.991    10 1.65e-4\n 9 recipe_3_linear… Prepro… recipe  line… rmse    standard   0.183    10 1.01e-3\n10 recipe_3_linear… Prepro… recipe  line… rsq     standard   0.984    10 1.67e-4\n11 recipe_3_rand_f… Prepro… recipe  rand… rmse    standard   0.137    10 1.06e-3\n12 recipe_3_rand_f… Prepro… recipe  rand… rsq     standard   0.991    10 1.19e-4\n\n\nFrom the metrics table we can see that recipe_1_rand_forest seems to perform the best.\n\nfinal_fit &lt;-\n  extract_workflow(diamonds_rs, \"recipe_1_rand_forest\") %&gt;%\n  fit(diamonds_train)\n\n\nranger_model &lt;- pull_workflow_fit(final_fit)\nranger_model\n\nparsnip model object\n\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, num.trees = ~1000,      num.threads = 1, verbose = FALSE, seed = sample.int(10^5,          1)) \n\nType:                             Regression \nNumber of trees:                  1000 \nSample size:                      43150 \nNumber of independent variables:  8 \nMtry:                             2 \nTarget node size:                 5 \nVariable importance mode:         none \nSplitrule:                        variance \nOOB prediction error (MSE):       0.01859615 \nR squared (OOB):                  0.9913472 \n\n\nLet’s fit the test set to the model.\n\nfinal_predic &lt;- predict(object = final_fit,\n                        new_data = diamonds_test)\n\nfinal_predic\n\n# A tibble: 10,790 × 1\n   .pred\n   &lt;dbl&gt;\n 1  8.85\n 2  8.64\n 3  8.66\n 4  8.72\n 5  8.70\n 6  8.62\n 7  8.87\n 8  8.73\n 9  8.83\n10  8.63\n# ℹ 10,780 more rows"
  },
  {
    "objectID": "posts/cyclist_trip_analysis/cyclist_bike_202207_202306.html",
    "href": "posts/cyclist_trip_analysis/cyclist_bike_202207_202306.html",
    "title": "CYCLIST BIKE SHARE",
    "section": "",
    "text": "The analysis is done on Cyclist Trip Data obtained from Coursera Google Data Analytics course as part of Cap Stone Project.\nThe data contains month wise travel usage of bikes from the year of 2015-2023. We will be concentrating on data gathered in between July-2022 to June-2023 which will cover an entire year.\nLet’s load the required packages first\n\nLoading tidyverse and gt packages\n\n\nlibrary(tidyverse)\nlibrary(gt)\n\n\n\n\nLet’s look at the structure of the data in one of the downloaded .csv files.\n\n\ntrpdata_july_2022&lt;-read_csv(\"F:/Data_Sci/Cap_Stone_Project/Cyclist_trip_data/202207-divvy-tripdata/202207-divvy-tripdata.csv\")\n\nRows: 823488 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (7): ride_id, rideable_type, start_station_name, start_station_id, end_...\ndbl  (4): start_lat, start_lng, end_lat, end_lng\ndttm (2): started_at, ended_at\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nglimpse(trpdata_july_2022)\n\nRows: 823,488\nColumns: 13\n$ ride_id            &lt;chr&gt; \"954144C2F67B1932\", \"292E027607D218B6\", \"5776585258…\n$ rideable_type      &lt;chr&gt; \"classic_bike\", \"classic_bike\", \"classic_bike\", \"cl…\n$ started_at         &lt;dttm&gt; 2022-07-05 08:12:47, 2022-07-26 12:53:38, 2022-07-…\n$ ended_at           &lt;dttm&gt; 2022-07-05 08:24:32, 2022-07-26 12:55:31, 2022-07-…\n$ start_station_name &lt;chr&gt; \"Ashland Ave & Blackhawk St\", \"Buckingham Fountain …\n$ start_station_id   &lt;chr&gt; \"13224\", \"15541\", \"15541\", \"15541\", \"TA1307000117\",…\n$ end_station_name   &lt;chr&gt; \"Kingsbury St & Kinzie St\", \"Michigan Ave & 8th St\"…\n$ end_station_id     &lt;chr&gt; \"KA1503000043\", \"623\", \"623\", \"TA1307000164\", \"TA13…\n$ start_lat          &lt;dbl&gt; 41.90707, 41.86962, 41.86962, 41.86962, 41.89147, 4…\n$ start_lng          &lt;dbl&gt; -87.66725, -87.62398, -87.62398, -87.62398, -87.626…\n$ end_lat            &lt;dbl&gt; 41.88918, 41.87277, 41.87277, 41.79526, 41.93625, 4…\n$ end_lng            &lt;dbl&gt; -87.63851, -87.62398, -87.62398, -87.59647, -87.652…\n$ member_casual      &lt;chr&gt; \"member\", \"casual\", \"casual\", \"casual\", \"member\", \"…\n\n\n\nLet’s look at the columns and try to understand what they represent\n\nride_id is the unique identification token generated for each ride that was initiated.\nrideable_type indicates the type of bike used for the ride.\nstarted_at and ended_at give us the time when the ride began and the ride ended respectively.\nstart_station_name and end_station_name give us the names of stations where ride began and ended respectively.\nstart_station_id and end_station_id are unique ID’s given to stations.\nstart_lat and start_lng represent co-ordinates where the ride began.\nend_lat and end_lng represent co-ordinates where the ride stopped.\nmember_casual identifies if the rider is a member or casual rider of the bike.\n\n\nThe trpdata_july_2022 contains 823488 rows and 13 columns. In the results we can see all the columns and their data types.\n\nLets load data of remaining 11 months.\n\n\ntrpdata_aug_2022 &lt;- read_csv(\"F:/Data_Sci/Cap_Stone_Project/Cyclist_trip_data/202208-divvy-tripdata/202208-divvy-tripdata.csv\")\n\ntrpdata_sept_2022&lt;- read_csv(\"F:/Data_Sci/Cap_Stone_Project/Cyclist_trip_data/202209-divvy-tripdata/202209-divvy-publictripdata.csv\")\n\ntrpdata_oct_2022&lt;- read_csv(\"F:/Data_Sci/Cap_Stone_Project/Cyclist_trip_data/202210-divvy-tripdata/202210-divvy-tripdata_raw.csv\")\n\ntrpdata_nov_2022&lt;- read_csv(\"F:/Data_Sci/Cap_Stone_Project/Cyclist_trip_data/202211-divvy-tripdata/202211-divvy-tripdata.csv\")\n\ntrpdata_dec_2022 &lt;- read_csv(\"F:/Data_Sci/Cap_Stone_Project/Cyclist_trip_data/202212-divvy-tripdata/202212-divvy-tripdata.csv\")\n\ntrpdata_jan_2023 &lt;- read_csv(\"F:/Data_Sci/Cap_Stone_Project/Cyclist_trip_data/202301-divvy-tripdata/202301-divvy-tripdata.csv\")\n\ntrpdata_feb_2023 &lt;- read_csv(\"F:/Data_Sci/Cap_Stone_Project/Cyclist_trip_data/202302-divvy-tripdata/202302-divvy-tripdata.csv\")\n\ntrpdata_mar_2023 &lt;- read_csv(\"F:/Data_Sci/Cap_Stone_Project/Cyclist_trip_data/202303-divvy-tripdata/202303-divvy-tripdata.csv\")\n\ntrpdata_apr_2023 &lt;- read_csv(\"F:/Data_Sci/Cap_Stone_Project/Cyclist_trip_data/202304-divvy-tripdata/202304-divvy-tripdata.csv\")\n\ntrpdata_may_2023 &lt;- read_csv(\"F:/Data_Sci/Cap_Stone_Project/Cyclist_trip_data/202305-divvy-tripdata/202305-divvy-tripdata.csv\")\n\ntrpdata_june_2023 &lt;- read_csv(\"F:/Data_Sci/Cap_Stone_Project/Cyclist_trip_data/202306-divvy-tripdata/202306-divvy-tripdata.csv\")\n\nAs structure of .csv’s is same across the all the files lets combine all the .csv files into a single data frame which contains data of all 12 months.\n\nCombining all the monthly data to one previous year data(data_one_year_raw).\n\n\ndata_one_year_raw &lt;- rbind(trpdata_july_2022, trpdata_aug_2022,\n                     trpdata_sept_2022, trpdata_oct_2022,\n                     trpdata_nov_2022, trpdata_dec_2022,\n                     trpdata_jan_2023, trpdata_feb_2023,\n                     trpdata_mar_2023, trpdata_apr_2023,\n                     trpdata_may_2023, trpdata_june_2023)\n\nglimpse(data_one_year_raw)\n\nRows: 5,779,444\nColumns: 13\n$ ride_id            &lt;chr&gt; \"954144C2F67B1932\", \"292E027607D218B6\", \"5776585258…\n$ rideable_type      &lt;chr&gt; \"classic_bike\", \"classic_bike\", \"classic_bike\", \"cl…\n$ started_at         &lt;dttm&gt; 2022-07-05 08:12:47, 2022-07-26 12:53:38, 2022-07-…\n$ ended_at           &lt;dttm&gt; 2022-07-05 08:24:32, 2022-07-26 12:55:31, 2022-07-…\n$ start_station_name &lt;chr&gt; \"Ashland Ave & Blackhawk St\", \"Buckingham Fountain …\n$ start_station_id   &lt;chr&gt; \"13224\", \"15541\", \"15541\", \"15541\", \"TA1307000117\",…\n$ end_station_name   &lt;chr&gt; \"Kingsbury St & Kinzie St\", \"Michigan Ave & 8th St\"…\n$ end_station_id     &lt;chr&gt; \"KA1503000043\", \"623\", \"623\", \"TA1307000164\", \"TA13…\n$ start_lat          &lt;dbl&gt; 41.90707, 41.86962, 41.86962, 41.86962, 41.89147, 4…\n$ start_lng          &lt;dbl&gt; -87.66725, -87.62398, -87.62398, -87.62398, -87.626…\n$ end_lat            &lt;dbl&gt; 41.88918, 41.87277, 41.87277, 41.79526, 41.93625, 4…\n$ end_lng            &lt;dbl&gt; -87.63851, -87.62398, -87.62398, -87.59647, -87.652…\n$ member_casual      &lt;chr&gt; \"member\", \"casual\", \"casual\", \"casual\", \"member\", \"…\n\n\n\ndata_one_year_raw data frame contains data from the month of July-2022 to June-2023.\n\n\n\n\n\nChecking and counting “NA” in each column of the data frame. Data is much better without “NA” as they can cause problems while aggregating data and calculating averages and sums. We can use map function to perform a function to all of the columns.\n\n\nna_in_cols &lt;- data_one_year_raw %&gt;% map(is.na) %&gt;% map(sum) %&gt;% unlist()\n\nna_in_cols\n\n           ride_id      rideable_type         started_at           ended_at \n                 0                  0                  0                  0 \nstart_station_name   start_station_id   end_station_name     end_station_id \n            857860             857992             915655             915796 \n         start_lat          start_lng            end_lat            end_lng \n                 0                  0               5795               5795 \n     member_casual \n                 0 \n\n\n\nAs NA’s are not present in the times columns i.e, started_at and ended_at we don’t need to worry ourselves about writing na.rm during aggregation and manipulation of data but it is a good practice to do so.\nFinding the length or duration of the rides by making a new column ride_length in minutes and making sure that the ride_length is not negative by using if_else function. Eliminating stations where station names and longitude and latitude co-ordinates are not present.\n\n\n# As we remove all the NA's it is better to save the data as \"data_one_year\".\ndata_one_year &lt;- data_one_year_raw %&gt;% \n  mutate(ride_length = difftime(ended_at, started_at,\n                                units = \"min\")) %&gt;%\n  mutate(ride_length = as.numeric(ride_length))\n\ndata_one_year &lt;- data_one_year %&gt;%\n  mutate(ride_length = if_else(ride_length &lt; 0, 0, ride_length)) %&gt;% \n  filter(ride_length &gt;= 2,\n         start_station_name != \"\" & end_station_name != \"\" & \n         !is.na(start_lat) & !is.na(start_lng) &\n         !is.na(end_lat) & !is.na(end_lng)) %&gt;% arrange(ride_length) %&gt;% \n  select(ride_id, rideable_type, ride_length,\n         started_at, ended_at, member_casual)\n\n\nglimpse(data_one_year)\n\nRows: 4,243,652\nColumns: 6\n$ ride_id       &lt;chr&gt; \"898EAA520DDCF78F\", \"50BACAD085808776\", \"961FDB38764FE54…\n$ rideable_type &lt;chr&gt; \"classic_bike\", \"classic_bike\", \"classic_bike\", \"classic…\n$ ride_length   &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,…\n$ started_at    &lt;dttm&gt; 2022-07-27 17:51:57, 2022-07-31 20:02:35, 2022-07-06 18…\n$ ended_at      &lt;dttm&gt; 2022-07-27 17:53:57, 2022-07-31 20:04:35, 2022-07-06 18…\n$ member_casual &lt;chr&gt; \"member\", \"member\", \"casual\", \"member\", \"casual\", \"membe…\n\n\n\n\n\n\n\n\n\nAggregating data to see “Average minutes per ride” grouped by “bike type” and “rider type” after removing rides less than 2 minutes (As rides less than 2 minutes tend to have the same start and stop stations).\n\n\ndata_one_year_aggregate &lt;- data_one_year %&gt;% \n  select(ride_id, rideable_type, member_casual, started_at, ended_at,\n         ride_length, everything()) %&gt;%\n  filter(ride_length &gt;= 2) %&gt;% \n  summarise(\"Number of Rides\" = n(),\n            \"Ride Length\" = sum(ride_length, na.rm = TRUE),\n            \"Max Ride Length\" = round(max(ride_length), 2),\n            \"Avg Ride Length in Minutes\" = round(mean(ride_length), 2),\n            .by = c(member_casual, rideable_type)) %&gt;% \n  arrange(desc(\"Avg Ride Length in Minutes\")) %&gt;% \n  gt() %&gt;% tab_header(title = \"Average length of Rides\") %&gt;% \n  cols_label(member_casual = \"Rider type\",\n             rideable_type = \"Bike type\")\n\ndata_one_year_aggregate\n\n\n\nTable 1: Average minutes per ride\n\n\n\n\n\n\n\n\n\nAverage length of Rides\n\n\nRider type\nBike type\nNumber of Rides\nRide Length\nMax Ride Length\nAvg Ride Length in Minutes\n\n\n\n\nmember\nclassic_bike\n1630991\n21996488\n1497.87\n13.49\n\n\ncasual\nclassic_bike\n781530\n19383358\n1497.75\n24.80\n\n\ncasual\nelectric_bike\n709649\n11372659\n479.98\n16.03\n\n\nmember\nelectric_bike\n984688\n10968684\n480.00\n11.14\n\n\ncasual\ndocked_bike\n136794\n6899998\n32035.45\n50.44\n\n\n\n\n\n\n\n\n\n\nWe can clearly notice in Table 1 that member riders have more number of rides with both classic and electric bikes while the average ride length is higher with casual riders.\n\nCalculating and visualizing Average ride length by “Rider type”.\n\n\naverage_ride_by_rideable_type &lt;- data_one_year %&gt;%\n  rename(\"Rider type\" = member_casual, \"Bike type\" = rideable_type) %&gt;% \n  summarise(ride_length = sum(ride_length, na.rm = TRUE),\n            ride_count = n(),\n            avg_ride_length = ride_length/ride_count,\n            .by = c(`Rider type`, `Bike type`)) %&gt;% \n  ggplot(aes(`Rider type`, avg_ride_length)) + \n  geom_col(aes(fill = `Bike type`), position = \"dodge\") + \n  labs(x = \"Bike type\", y = \"Avg Length of Ride(Minutes)\",\n       title  = \"Average ride length by Bike type\") +\n  theme_minimal() +\n  theme(plot.title = element_text(size = 18),\n        legend.position = \"bottom\")\n\naverage_ride_by_rideable_type\n\n\n\n\n\n\n\nFigure 1: Average Ride Length by Rider type and Member type\n\n\n\n\n\nThe above Figure 1 clearly shows that members average ride lengths between bike types doesn’t differ much for member riders but differs with casual riders upto 8 minutes.\n\n\n\n\n\n\nNote\n\n\n\nFurther down in the analysis “docked_bike” type is dropped as no proper documentation is available in the course.\n\n\n\n\n\n\n\n\nCalculating and visualizing ride patterns in a week for number of rides.\n\n\nrideable_order &lt;- c(\"classic_bike\", \"electric_bike\", \"docked_bike\")\n\nrides_on_days &lt;- data_one_year %&gt;%\n  filter(rideable_type != \"docked_bike\") %&gt;%\n  mutate(month = month(started_at, label = TRUE,\n                       abbr = FALSE)) %&gt;% \n  mutate(rideable_type = factor(rideable_type,\n                                levels = rideable_order)) %&gt;%  ggplot(aes(wday(started_at, label = TRUE, abbr = FALSE))) + \n  geom_bar(aes(fill = member_casual), position = \"dodge\") +\n  facet_wrap(~month, nrow = 3) + \n  labs(x = \"Day of the Week\", y = \"Number of rides\",\n       title = \"Riding pattrens on Weekdays of each Month\",\n       subtitle = \"From July-2022 to June-2023\",\n       fill = \"Type of Rider\") +\n  theme_light() +\n  theme(legend.position = \"top\",\n        axis.text.x = element_text(angle = 45, hjust = 1),\n        plot.title = element_text(size = 18))\n\nrides_on_days \n\n\n\n\n\n\n\nFigure 2: Riding pattrens in Weekdays of each Month\n\n\n\n\n\nThe above Figure 2 clearly shows how the number of rides change due to seasons. In winters the number of rides decrease very drastically may be because of temperature and snow. In Summers the number of rides are at its peak.\nThe number of rides driven by member riders are increases through the week especially in working week days but for casual riders the rides increase in the weekends. The Figure 2 shows number of rides on Saturdays and Sundays by casual members overtake membership riders in the months of July and August.\n\n\n\nAggregating data for the visualization.\n\nrides_on_days &lt;- data_one_year %&gt;%\n  mutate(day = wday(started_at, label = TRUE, abbr = FALSE),\n         month = month(started_at, label = TRUE, abbr = FALSE)) %&gt;% \n  summarise(ride_count = n(),\n            sum_ride_length = sum(ride_length, na.rm = TRUE),\n            avg_ride_length = mean(ride_length, na.rm = TRUE),\n            .by = c(month, day, member_casual))\n\nrides_on_days \n\n# A tibble: 168 × 6\n   month day       member_casual ride_count sum_ride_length avg_ride_length\n   &lt;ord&gt; &lt;ord&gt;     &lt;chr&gt;              &lt;int&gt;           &lt;dbl&gt;           &lt;dbl&gt;\n 1 July  Wednesday member             45928         603429.            13.1\n 2 July  Sunday    member             44328         685475.            15.5\n 3 July  Wednesday casual             31631         703360.            22.2\n 4 July  Saturday  member             51852         816009.            15.7\n 5 July  Thursday  casual             34895         759013.            21.8\n 6 July  Friday    member             46398         614466.            13.2\n 7 July  Friday    casual             41198         959168.            23.3\n 8 July  Monday    casual             32960         917333.            27.8\n 9 July  Monday    member             38746         530041.            13.7\n10 July  Sunday    casual             59528        1714086.            28.8\n# ℹ 158 more rows\n\n\nLet’s visualize the aggregated data\n\nrides_on_days_len &lt;- rides_on_days %&gt;%\n  ggplot(aes(day, sum_ride_length))+\n  geom_col(aes(fill = member_casual), position = \"dodge\")+\n  facet_wrap(~month, ncol = 3)+\n  labs(x = \"Day of the Week\", y = \"Total Length of Rides (Minutes)\",\n       title = \"Total Minutes driven by Riders\",\n       fill = \"Type of Rider\") +\n  theme(legend.position = \"top\",\n        axis.text.x = element_text(angle = 45, hjust = 1),\n        plot.title = element_text(size = 18))\n\nrides_on_days_len\n\n\n\n\n\n\n\nFigure 3: Total Ride lengths through out the year by member types.\n\n\n\n\n\n\nrides_on_days_len_avg &lt;- rides_on_days %&gt;%\n  ggplot(aes(day, avg_ride_length))+\n  geom_col(aes(fill = member_casual), position = \"dodge\")+\n  facet_wrap(~month, ncol = 3) +\n  labs(x = \"Day of the Week\", y = \"Average Length of Rides (Minutes)\",\n       title = \"Average Minutes driven by Riders\",\n       fill = \"Type of Rider\") +\n  theme(legend.position = \"top\",\n        axis.text.x = element_text(angle = 45, hjust = 1),\n        plot.title = element_text(size = 18))\n\nrides_on_days_len_avg\n\n\n\n\n\n\n\nFigure 4: Average Ride lengths through out year by member types.\n\n\n\n\n\nThe ride length is varying across months and seasons just as number of rides but average ride length is not fluctuating that much across the year.\n\n\n\nLet’s look at when the rides are starting to know at what time of day the rides peak and are at the lowest.\n\nrides_on_time_of_day &lt;- data_one_year %&gt;%\n  mutate(time_of_day = format(as.POSIXct(ended_at), \"%H\"),\n         wk_day = wday(started_at, label = TRUE, abbr = FALSE)) %&gt;% \n  summarise(ride_id = n(),\n            .by = c(time_of_day, member_casual))\n\nrides_on_time_of_day %&gt;%\n  ggplot(aes(time_of_day, ride_id, fill = ride_id )) +\n  geom_col() +\n  labs(x = \"Hour of the day\", y = \"Number of Rides\",\n       fill = \"Max Rides\") +\n  facet_wrap(~member_casual, ncol = 1) +\n  scale_y_continuous(\n  labels = scales::number_format(scale = 1e-3, suffix = \"K\")) +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nMost of the rides start at 5:00 PM in the evening showing most of the rides begin after office hours for both caual and member riders but members peak twice at 8:00 AM and 5:00 PM but the casual riders peak only once at 5:00 PM.\n\n\n\n\n\n\nRemoving “NA” and blanks from the stations columns.\n\n\ndata_one_year &lt;- data_one_year_raw %&gt;%\n  mutate(ride_length = difftime(ended_at, started_at,\n                                units = \"min\")) %&gt;%\n  mutate(ride_length = as.numeric(ride_length)) %&gt;% \n  mutate(ride_length = if_else(ride_length &lt; 0, 0, ride_length)) %&gt;% \n  filter(ride_length &gt;= 2) %&gt;% \n  drop_na(start_station_name, end_station_name ) %&gt;% \n  filter(start_station_name != \"\" & end_station_name != \"\",\n         started_at != ended_at)\n\nglimpse(data_one_year)\n\nRows: 4,243,662\nColumns: 14\n$ ride_id            &lt;chr&gt; \"954144C2F67B1932\", \"57765852588AD6E0\", \"B5B6BE4431…\n$ rideable_type      &lt;chr&gt; \"classic_bike\", \"classic_bike\", \"classic_bike\", \"cl…\n$ started_at         &lt;dttm&gt; 2022-07-05 08:12:47, 2022-07-03 13:58:49, 2022-07-…\n$ ended_at           &lt;dttm&gt; 2022-07-05 08:24:32, 2022-07-03 14:06:32, 2022-07-…\n$ start_station_name &lt;chr&gt; \"Ashland Ave & Blackhawk St\", \"Buckingham Fountain …\n$ start_station_id   &lt;chr&gt; \"13224\", \"15541\", \"15541\", \"TA1307000117\", \"15535\",…\n$ end_station_name   &lt;chr&gt; \"Kingsbury St & Kinzie St\", \"Michigan Ave & 8th St\"…\n$ end_station_id     &lt;chr&gt; \"KA1503000043\", \"623\", \"TA1307000164\", \"TA130700005…\n$ start_lat          &lt;dbl&gt; 41.90707, 41.86962, 41.86962, 41.89147, 41.88461, 4…\n$ start_lng          &lt;dbl&gt; -87.66725, -87.62398, -87.62398, -87.62676, -87.644…\n$ end_lat            &lt;dbl&gt; 41.88918, 41.87277, 41.79526, 41.93625, 41.86712, 4…\n$ end_lng            &lt;dbl&gt; -87.63851, -87.62398, -87.59647, -87.65266, -87.641…\n$ member_casual      &lt;chr&gt; \"member\", \"casual\", \"casual\", \"member\", \"member\", \"…\n$ ride_length        &lt;dbl&gt; 11.750000, 7.716667, 58.483333, 26.300000, 8.716667…\n\n\n\nMaking a new column to identify travelled routes.\n\n\ndata_one_year &lt;- data_one_year %&gt;% \n  mutate(stations_travelled = paste(start_station_name, \n                                     \"-\", end_station_name))\n\nglimpse(data_one_year)\n\nRows: 4,243,662\nColumns: 15\n$ ride_id            &lt;chr&gt; \"954144C2F67B1932\", \"57765852588AD6E0\", \"B5B6BE4431…\n$ rideable_type      &lt;chr&gt; \"classic_bike\", \"classic_bike\", \"classic_bike\", \"cl…\n$ started_at         &lt;dttm&gt; 2022-07-05 08:12:47, 2022-07-03 13:58:49, 2022-07-…\n$ ended_at           &lt;dttm&gt; 2022-07-05 08:24:32, 2022-07-03 14:06:32, 2022-07-…\n$ start_station_name &lt;chr&gt; \"Ashland Ave & Blackhawk St\", \"Buckingham Fountain …\n$ start_station_id   &lt;chr&gt; \"13224\", \"15541\", \"15541\", \"TA1307000117\", \"15535\",…\n$ end_station_name   &lt;chr&gt; \"Kingsbury St & Kinzie St\", \"Michigan Ave & 8th St\"…\n$ end_station_id     &lt;chr&gt; \"KA1503000043\", \"623\", \"TA1307000164\", \"TA130700005…\n$ start_lat          &lt;dbl&gt; 41.90707, 41.86962, 41.86962, 41.89147, 41.88461, 4…\n$ start_lng          &lt;dbl&gt; -87.66725, -87.62398, -87.62398, -87.62676, -87.644…\n$ end_lat            &lt;dbl&gt; 41.88918, 41.87277, 41.79526, 41.93625, 41.86712, 4…\n$ end_lng            &lt;dbl&gt; -87.63851, -87.62398, -87.59647, -87.65266, -87.641…\n$ member_casual      &lt;chr&gt; \"member\", \"casual\", \"casual\", \"member\", \"member\", \"…\n$ ride_length        &lt;dbl&gt; 11.750000, 7.716667, 58.483333, 26.300000, 8.716667…\n$ stations_travelled &lt;chr&gt; \"Ashland Ave & Blackhawk St - Kingsbury St & Kinzie…\n\n\n\nFinding which route is most traveled by casual riders.\n\n\nmost_travelled_routes_casual &lt;- data_one_year %&gt;%\n  filter(member_casual == \"casual\",\n         ride_length &gt;= 2) %&gt;% \n  summarise(ride_count = n(),\n            avg_ride_length = round(mean(ride_length), 2),\n            .by = c(stations_travelled)) %&gt;%\n  arrange(desc(ride_count))\n\nhead(most_travelled_routes_casual)\n\n# A tibble: 6 × 3\n  stations_travelled                                  ride_count avg_ride_length\n  &lt;chr&gt;                                                    &lt;int&gt;           &lt;dbl&gt;\n1 Streeter Dr & Grand Ave - Streeter Dr & Grand Ave         8259            46.3\n2 DuSable Lake Shore Dr & Monroe St - DuSable Lake S…       5726            38.3\n3 DuSable Lake Shore Dr & Monroe St - Streeter Dr & …       4840            27.1\n4 Michigan Ave & Oak St - Michigan Ave & Oak St             3754            50.9\n5 Millennium Park - Millennium Park                         3188            45.4\n6 Streeter Dr & Grand Ave - DuSable Lake Shore Dr & …       2663            27.8\n\nNROW(most_travelled_routes_casual)\n\n[1] 130373\n\n\nStreeter Dr & Grand Ave - Streeter Dr & Grand Ave stands to be the most popular station with 9698 rides by casual riders.\n\nmost_travelled_routes_member &lt;- data_one_year  %&gt;%\n  filter(member_casual == \"member\") %&gt;% \n  summarise(ride_count = n(),\n            total_ride_length = sum(ride_length),\n            ride_length = round(mean(ride_length), 2),\n            .by = stations_travelled) %&gt;% arrange(desc(ride_count))\n\nhead(most_travelled_routes_member)\n\n# A tibble: 6 × 4\n  stations_travelled                    ride_count total_ride_length ride_length\n  &lt;chr&gt;                                      &lt;int&gt;             &lt;dbl&gt;       &lt;dbl&gt;\n1 Ellis Ave & 60th St - University Ave…       6149            25928.        4.22\n2 University Ave & 57th St - Ellis Ave…       5771            26605.        4.61\n3 Ellis Ave & 60th St - Ellis Ave & 55…       5676            28427.        5.01\n4 Ellis Ave & 55th St - Ellis Ave & 60…       5347            27187.        5.08\n5 State St & 33rd St - Calumet Ave & 3…       4037            17795.        4.41\n6 Calumet Ave & 33rd St - State St & 3…       3836            15538.        4.05\n\nNROW(most_travelled_routes_member)\n\n[1] 144802\n\n\nEllis Ave & 60th St - University Ave & 57th St stands as the most traveled route by member riders with 6153 rides per anum.\n\nFinding which station has most ride starting points and which station has most ending points.\n\n\nmost_starting_points &lt;- data_one_year %&gt;% \n  summarise(ride_count = n(),\n            .by = start_station_name) %&gt;%\n  select(start_station_name, ride_count) %&gt;%\n  slice_max(ride_count, n = 10)\n\nmost_starting_points\n\n# A tibble: 10 × 2\n   start_station_name                 ride_count\n   &lt;chr&gt;                                   &lt;int&gt;\n 1 Streeter Dr & Grand Ave                 63899\n 2 DuSable Lake Shore Dr & Monroe St       36757\n 3 Michigan Ave & Oak St                   35050\n 4 DuSable Lake Shore Dr & North Blvd      34167\n 5 Wells St & Concord Ln                   32175\n 6 Clark St & Elm St                       31832\n 7 Kingsbury St & Kinzie St                30820\n 8 Millennium Park                         29894\n 9 Theater on the Lake                     28864\n10 Wells St & Elm St                       27152\n\nmost_starting_points$ride_count %&gt;% sum()\n\n[1] 350610\n\nmost_ending_points &lt;- data_one_year %&gt;% \n  summarise(ride_count = n(),\n            .by = end_station_name) %&gt;%\n  select(end_station_name, ride_count)  %&gt;% \n  slice_max(ride_count, n = 10)\n\nmost_ending_points\n\n# A tibble: 10 × 2\n   end_station_name                   ride_count\n   &lt;chr&gt;                                   &lt;int&gt;\n 1 Streeter Dr & Grand Ave                 65542\n 2 DuSable Lake Shore Dr & North Blvd      37093\n 3 Michigan Ave & Oak St                   35977\n 4 DuSable Lake Shore Dr & Monroe St       35629\n 5 Wells St & Concord Ln                   32879\n 6 Clark St & Elm St                       31394\n 7 Millennium Park                         31023\n 8 Kingsbury St & Kinzie St                29805\n 9 Theater on the Lake                     29482\n10 Wells St & Elm St                       27360\n\nmost_ending_points$ride_count %&gt;% sum()\n\n[1] 356184\n\n\nStreeter Dr & Grand Ave found to be the most popular station as most rides start and end at that station.\n\n\n\nJust because we filtered the data with NA’s that does not mean that the data is not helpful, it just means that it does not our fulfill specific need when calculating or manipulating data.\nLet’s look at NA’s in the data once again.\n\nna_in_cols &lt;- data_one_year_raw %&gt;% map( ~sum(is.na(.))) %&gt;% unlist()\n\nna_in_cols\n\n           ride_id      rideable_type         started_at           ended_at \n                 0                  0                  0                  0 \nstart_station_name   start_station_id   end_station_name     end_station_id \n            857860             857992             915655             915796 \n         start_lat          start_lng            end_lat            end_lng \n                 0                  0               5795               5795 \n     member_casual \n                 0 \n\n\n\nWe can see that the start_station_name and end_station_name have majority of NA’s it means that rides are starting and ending where stations are not there.\n\n\nprop_na &lt;- na_in_cols[\"start_station_name\"]/nrow(data_one_year_raw)\n\nprop_na\n\nstart_station_name \n          0.148433 \n\n\n\n14.84% of data in start_station_name is missing and good thing is that none of the start_lng and start_lat have any NA’s and we can use this for find the most traveled routes.\n\n\ndata_na_one_year &lt;- data_one_year_raw %&gt;% \n  filter(is.na(start_station_name) | start_station_name == \"\") %&gt;% \n  drop_na(end_lat, end_lng)\n  \nglimpse(data_na_one_year)\n\nRows: 857,860\nColumns: 13\n$ ride_id            &lt;chr&gt; \"DCB3D2C9B63999EC\", \"D1ACA8280DA02AE3\", \"EF98673429…\n$ rideable_type      &lt;chr&gt; \"electric_bike\", \"electric_bike\", \"electric_bike\", …\n$ started_at         &lt;dttm&gt; 2022-07-04 15:04:26, 2022-07-12 14:43:51, 2022-07-…\n$ ended_at           &lt;dttm&gt; 2022-07-04 15:32:38, 2022-07-12 14:49:28, 2022-07-…\n$ start_station_name &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ start_station_id   &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ end_station_name   &lt;chr&gt; \"Ashland Ave & Blackhawk St\", \"Cornell Ave & Hyde P…\n$ end_station_id     &lt;chr&gt; \"13224\", \"KA1503000007\", \"KA1503000007\", \"847\", \"48…\n$ start_lat          &lt;dbl&gt; 41.95, 41.80, 41.80, 41.74, 42.02, 41.95, 41.95, 41…\n$ start_lng          &lt;dbl&gt; -87.64, -87.59, -87.59, -87.55, -87.69, -87.67, -87…\n$ end_lat            &lt;dbl&gt; 41.90707, 41.80241, 41.80241, 41.73000, 42.01000, 4…\n$ end_lng            &lt;dbl&gt; -87.66725, -87.58692, -87.58692, -87.55000, -87.690…\n$ member_casual      &lt;chr&gt; \"member\", \"member\", \"member\", \"member\", \"member\", \"…\n\n\n\nNow let’s make new columns start_point with start_lng and start_lat and end_point with end_lat and end_lng.\n\n\ndata_na_one_year &lt;- data_na_one_year %&gt;%\n    mutate(start_point = paste(start_lat, start_lng),\n           end_point = paste(end_lat, end_lng))\n\nglimpse(data_na_one_year)\n\nRows: 857,860\nColumns: 15\n$ ride_id            &lt;chr&gt; \"DCB3D2C9B63999EC\", \"D1ACA8280DA02AE3\", \"EF98673429…\n$ rideable_type      &lt;chr&gt; \"electric_bike\", \"electric_bike\", \"electric_bike\", …\n$ started_at         &lt;dttm&gt; 2022-07-04 15:04:26, 2022-07-12 14:43:51, 2022-07-…\n$ ended_at           &lt;dttm&gt; 2022-07-04 15:32:38, 2022-07-12 14:49:28, 2022-07-…\n$ start_station_name &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ start_station_id   &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ end_station_name   &lt;chr&gt; \"Ashland Ave & Blackhawk St\", \"Cornell Ave & Hyde P…\n$ end_station_id     &lt;chr&gt; \"13224\", \"KA1503000007\", \"KA1503000007\", \"847\", \"48…\n$ start_lat          &lt;dbl&gt; 41.95, 41.80, 41.80, 41.74, 42.02, 41.95, 41.95, 41…\n$ start_lng          &lt;dbl&gt; -87.64, -87.59, -87.59, -87.55, -87.69, -87.67, -87…\n$ end_lat            &lt;dbl&gt; 41.90707, 41.80241, 41.80241, 41.73000, 42.01000, 4…\n$ end_lng            &lt;dbl&gt; -87.66725, -87.58692, -87.58692, -87.55000, -87.690…\n$ member_casual      &lt;chr&gt; \"member\", \"member\", \"member\", \"member\", \"member\", \"…\n$ start_point        &lt;chr&gt; \"41.95 -87.64\", \"41.8 -87.59\", \"41.8 -87.59\", \"41.7…\n$ end_point          &lt;chr&gt; \"41.907066 -87.667252\", \"41.802406 -87.586924\", \"41…\n\n\n\nAggregating data to check for the most traveled routes without a start_station name.\n\nFirst join start_point and end_point to make route_travelled then count the rides by routes_travelled to see the most traveled path.\n\nmost_travelled_na_routes &lt;- data_na_one_year %&gt;%\n  filter(start_point != end_point) %&gt;% \n  mutate(route_travelled = paste(start_point, \",\", end_point)) %&gt;% \n  summarise(ride_count = n(),\n            .by = route_travelled) %&gt;%\n  slice_max(ride_count, n=10)\n\nmost_travelled_na_routes\n\n# A tibble: 10 × 2\n   route_travelled                             ride_count\n   &lt;chr&gt;                                            &lt;int&gt;\n 1 41.79 -87.6 , 41.8 -87.59                         1459\n 2 41.79 -87.59 , 41.79 -87.6                        1354\n 3 41.8 -87.59 , 41.79 -87.6                         1335\n 4 41.79 -87.6 , 41.79 -87.59                        1320\n 5 41.8 -87.6 , 41.79 -87.6                          1099\n 6 41.79 -87.6 , 41.78509714636 -87.6010727606       1058\n 7 41.79 -87.6 , 41.8 -87.6                           999\n 8 41.79 -87.6 , 41.799568 -87.594747                 917\n 9 41.79 -87.6 , 41.78 -87.6                          697\n10 41.89 -87.63 , 41.9 -87.63                         690\n\nsum(most_travelled_na_routes[\"ride_count\"])\n\n[1] 10928\n\n\n\n10928 rides are not small when compared to most traveled routes, but 10928 rides in 5 million rides is not that high.\n\n\n\n\n\nAs casual members go for long rides on the weekends, offers on weekend rides with membership buying may help attract more memberships and might also make membership riders to make weekend end trips.\nTo increase the memberships of the Cyclist Bike Share the company needs to place stations where most new rides are coming up and routes are travelled more."
  },
  {
    "objectID": "posts/cyclist_trip_analysis/cyclist_bike_202207_202306.html#introduction",
    "href": "posts/cyclist_trip_analysis/cyclist_bike_202207_202306.html#introduction",
    "title": "CYCLIST BIKE SHARE",
    "section": "",
    "text": "The analysis is done on Cyclist Trip Data obtained from Coursera Google Data Analytics course as part of Cap Stone Project.\nThe data contains month wise travel usage of bikes from the year of 2015-2023. We will be concentrating on data gathered in between July-2022 to June-2023 which will cover an entire year.\nLet’s load the required packages first\n\nLoading tidyverse and gt packages\n\n\nlibrary(tidyverse)\nlibrary(gt)\n\n\n\n\nLet’s look at the structure of the data in one of the downloaded .csv files.\n\n\ntrpdata_july_2022&lt;-read_csv(\"F:/Data_Sci/Cap_Stone_Project/Cyclist_trip_data/202207-divvy-tripdata/202207-divvy-tripdata.csv\")\n\nRows: 823488 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (7): ride_id, rideable_type, start_station_name, start_station_id, end_...\ndbl  (4): start_lat, start_lng, end_lat, end_lng\ndttm (2): started_at, ended_at\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nglimpse(trpdata_july_2022)\n\nRows: 823,488\nColumns: 13\n$ ride_id            &lt;chr&gt; \"954144C2F67B1932\", \"292E027607D218B6\", \"5776585258…\n$ rideable_type      &lt;chr&gt; \"classic_bike\", \"classic_bike\", \"classic_bike\", \"cl…\n$ started_at         &lt;dttm&gt; 2022-07-05 08:12:47, 2022-07-26 12:53:38, 2022-07-…\n$ ended_at           &lt;dttm&gt; 2022-07-05 08:24:32, 2022-07-26 12:55:31, 2022-07-…\n$ start_station_name &lt;chr&gt; \"Ashland Ave & Blackhawk St\", \"Buckingham Fountain …\n$ start_station_id   &lt;chr&gt; \"13224\", \"15541\", \"15541\", \"15541\", \"TA1307000117\",…\n$ end_station_name   &lt;chr&gt; \"Kingsbury St & Kinzie St\", \"Michigan Ave & 8th St\"…\n$ end_station_id     &lt;chr&gt; \"KA1503000043\", \"623\", \"623\", \"TA1307000164\", \"TA13…\n$ start_lat          &lt;dbl&gt; 41.90707, 41.86962, 41.86962, 41.86962, 41.89147, 4…\n$ start_lng          &lt;dbl&gt; -87.66725, -87.62398, -87.62398, -87.62398, -87.626…\n$ end_lat            &lt;dbl&gt; 41.88918, 41.87277, 41.87277, 41.79526, 41.93625, 4…\n$ end_lng            &lt;dbl&gt; -87.63851, -87.62398, -87.62398, -87.59647, -87.652…\n$ member_casual      &lt;chr&gt; \"member\", \"casual\", \"casual\", \"casual\", \"member\", \"…\n\n\n\nLet’s look at the columns and try to understand what they represent\n\nride_id is the unique identification token generated for each ride that was initiated.\nrideable_type indicates the type of bike used for the ride.\nstarted_at and ended_at give us the time when the ride began and the ride ended respectively.\nstart_station_name and end_station_name give us the names of stations where ride began and ended respectively.\nstart_station_id and end_station_id are unique ID’s given to stations.\nstart_lat and start_lng represent co-ordinates where the ride began.\nend_lat and end_lng represent co-ordinates where the ride stopped.\nmember_casual identifies if the rider is a member or casual rider of the bike.\n\n\nThe trpdata_july_2022 contains 823488 rows and 13 columns. In the results we can see all the columns and their data types.\n\nLets load data of remaining 11 months.\n\n\ntrpdata_aug_2022 &lt;- read_csv(\"F:/Data_Sci/Cap_Stone_Project/Cyclist_trip_data/202208-divvy-tripdata/202208-divvy-tripdata.csv\")\n\ntrpdata_sept_2022&lt;- read_csv(\"F:/Data_Sci/Cap_Stone_Project/Cyclist_trip_data/202209-divvy-tripdata/202209-divvy-publictripdata.csv\")\n\ntrpdata_oct_2022&lt;- read_csv(\"F:/Data_Sci/Cap_Stone_Project/Cyclist_trip_data/202210-divvy-tripdata/202210-divvy-tripdata_raw.csv\")\n\ntrpdata_nov_2022&lt;- read_csv(\"F:/Data_Sci/Cap_Stone_Project/Cyclist_trip_data/202211-divvy-tripdata/202211-divvy-tripdata.csv\")\n\ntrpdata_dec_2022 &lt;- read_csv(\"F:/Data_Sci/Cap_Stone_Project/Cyclist_trip_data/202212-divvy-tripdata/202212-divvy-tripdata.csv\")\n\ntrpdata_jan_2023 &lt;- read_csv(\"F:/Data_Sci/Cap_Stone_Project/Cyclist_trip_data/202301-divvy-tripdata/202301-divvy-tripdata.csv\")\n\ntrpdata_feb_2023 &lt;- read_csv(\"F:/Data_Sci/Cap_Stone_Project/Cyclist_trip_data/202302-divvy-tripdata/202302-divvy-tripdata.csv\")\n\ntrpdata_mar_2023 &lt;- read_csv(\"F:/Data_Sci/Cap_Stone_Project/Cyclist_trip_data/202303-divvy-tripdata/202303-divvy-tripdata.csv\")\n\ntrpdata_apr_2023 &lt;- read_csv(\"F:/Data_Sci/Cap_Stone_Project/Cyclist_trip_data/202304-divvy-tripdata/202304-divvy-tripdata.csv\")\n\ntrpdata_may_2023 &lt;- read_csv(\"F:/Data_Sci/Cap_Stone_Project/Cyclist_trip_data/202305-divvy-tripdata/202305-divvy-tripdata.csv\")\n\ntrpdata_june_2023 &lt;- read_csv(\"F:/Data_Sci/Cap_Stone_Project/Cyclist_trip_data/202306-divvy-tripdata/202306-divvy-tripdata.csv\")\n\nAs structure of .csv’s is same across the all the files lets combine all the .csv files into a single data frame which contains data of all 12 months.\n\nCombining all the monthly data to one previous year data(data_one_year_raw).\n\n\ndata_one_year_raw &lt;- rbind(trpdata_july_2022, trpdata_aug_2022,\n                     trpdata_sept_2022, trpdata_oct_2022,\n                     trpdata_nov_2022, trpdata_dec_2022,\n                     trpdata_jan_2023, trpdata_feb_2023,\n                     trpdata_mar_2023, trpdata_apr_2023,\n                     trpdata_may_2023, trpdata_june_2023)\n\nglimpse(data_one_year_raw)\n\nRows: 5,779,444\nColumns: 13\n$ ride_id            &lt;chr&gt; \"954144C2F67B1932\", \"292E027607D218B6\", \"5776585258…\n$ rideable_type      &lt;chr&gt; \"classic_bike\", \"classic_bike\", \"classic_bike\", \"cl…\n$ started_at         &lt;dttm&gt; 2022-07-05 08:12:47, 2022-07-26 12:53:38, 2022-07-…\n$ ended_at           &lt;dttm&gt; 2022-07-05 08:24:32, 2022-07-26 12:55:31, 2022-07-…\n$ start_station_name &lt;chr&gt; \"Ashland Ave & Blackhawk St\", \"Buckingham Fountain …\n$ start_station_id   &lt;chr&gt; \"13224\", \"15541\", \"15541\", \"15541\", \"TA1307000117\",…\n$ end_station_name   &lt;chr&gt; \"Kingsbury St & Kinzie St\", \"Michigan Ave & 8th St\"…\n$ end_station_id     &lt;chr&gt; \"KA1503000043\", \"623\", \"623\", \"TA1307000164\", \"TA13…\n$ start_lat          &lt;dbl&gt; 41.90707, 41.86962, 41.86962, 41.86962, 41.89147, 4…\n$ start_lng          &lt;dbl&gt; -87.66725, -87.62398, -87.62398, -87.62398, -87.626…\n$ end_lat            &lt;dbl&gt; 41.88918, 41.87277, 41.87277, 41.79526, 41.93625, 4…\n$ end_lng            &lt;dbl&gt; -87.63851, -87.62398, -87.62398, -87.59647, -87.652…\n$ member_casual      &lt;chr&gt; \"member\", \"casual\", \"casual\", \"casual\", \"member\", \"…\n\n\n\ndata_one_year_raw data frame contains data from the month of July-2022 to June-2023.\n\n\n\n\n\nChecking and counting “NA” in each column of the data frame. Data is much better without “NA” as they can cause problems while aggregating data and calculating averages and sums. We can use map function to perform a function to all of the columns.\n\n\nna_in_cols &lt;- data_one_year_raw %&gt;% map(is.na) %&gt;% map(sum) %&gt;% unlist()\n\nna_in_cols\n\n           ride_id      rideable_type         started_at           ended_at \n                 0                  0                  0                  0 \nstart_station_name   start_station_id   end_station_name     end_station_id \n            857860             857992             915655             915796 \n         start_lat          start_lng            end_lat            end_lng \n                 0                  0               5795               5795 \n     member_casual \n                 0 \n\n\n\nAs NA’s are not present in the times columns i.e, started_at and ended_at we don’t need to worry ourselves about writing na.rm during aggregation and manipulation of data but it is a good practice to do so.\nFinding the length or duration of the rides by making a new column ride_length in minutes and making sure that the ride_length is not negative by using if_else function. Eliminating stations where station names and longitude and latitude co-ordinates are not present.\n\n\n# As we remove all the NA's it is better to save the data as \"data_one_year\".\ndata_one_year &lt;- data_one_year_raw %&gt;% \n  mutate(ride_length = difftime(ended_at, started_at,\n                                units = \"min\")) %&gt;%\n  mutate(ride_length = as.numeric(ride_length))\n\ndata_one_year &lt;- data_one_year %&gt;%\n  mutate(ride_length = if_else(ride_length &lt; 0, 0, ride_length)) %&gt;% \n  filter(ride_length &gt;= 2,\n         start_station_name != \"\" & end_station_name != \"\" & \n         !is.na(start_lat) & !is.na(start_lng) &\n         !is.na(end_lat) & !is.na(end_lng)) %&gt;% arrange(ride_length) %&gt;% \n  select(ride_id, rideable_type, ride_length,\n         started_at, ended_at, member_casual)\n\n\nglimpse(data_one_year)\n\nRows: 4,243,652\nColumns: 6\n$ ride_id       &lt;chr&gt; \"898EAA520DDCF78F\", \"50BACAD085808776\", \"961FDB38764FE54…\n$ rideable_type &lt;chr&gt; \"classic_bike\", \"classic_bike\", \"classic_bike\", \"classic…\n$ ride_length   &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,…\n$ started_at    &lt;dttm&gt; 2022-07-27 17:51:57, 2022-07-31 20:02:35, 2022-07-06 18…\n$ ended_at      &lt;dttm&gt; 2022-07-27 17:53:57, 2022-07-31 20:04:35, 2022-07-06 18…\n$ member_casual &lt;chr&gt; \"member\", \"member\", \"casual\", \"member\", \"casual\", \"membe…"
  },
  {
    "objectID": "posts/cyclist_trip_analysis/cyclist_bike_202207_202306.html#analysis-of-data",
    "href": "posts/cyclist_trip_analysis/cyclist_bike_202207_202306.html#analysis-of-data",
    "title": "CYCLIST BIKE SHARE",
    "section": "",
    "text": "Aggregating data to see “Average minutes per ride” grouped by “bike type” and “rider type” after removing rides less than 2 minutes (As rides less than 2 minutes tend to have the same start and stop stations).\n\n\ndata_one_year_aggregate &lt;- data_one_year %&gt;% \n  select(ride_id, rideable_type, member_casual, started_at, ended_at,\n         ride_length, everything()) %&gt;%\n  filter(ride_length &gt;= 2) %&gt;% \n  summarise(\"Number of Rides\" = n(),\n            \"Ride Length\" = sum(ride_length, na.rm = TRUE),\n            \"Max Ride Length\" = round(max(ride_length), 2),\n            \"Avg Ride Length in Minutes\" = round(mean(ride_length), 2),\n            .by = c(member_casual, rideable_type)) %&gt;% \n  arrange(desc(\"Avg Ride Length in Minutes\")) %&gt;% \n  gt() %&gt;% tab_header(title = \"Average length of Rides\") %&gt;% \n  cols_label(member_casual = \"Rider type\",\n             rideable_type = \"Bike type\")\n\ndata_one_year_aggregate\n\n\n\nTable 1: Average minutes per ride\n\n\n\n\n\n\n\n\n\nAverage length of Rides\n\n\nRider type\nBike type\nNumber of Rides\nRide Length\nMax Ride Length\nAvg Ride Length in Minutes\n\n\n\n\nmember\nclassic_bike\n1630991\n21996488\n1497.87\n13.49\n\n\ncasual\nclassic_bike\n781530\n19383358\n1497.75\n24.80\n\n\ncasual\nelectric_bike\n709649\n11372659\n479.98\n16.03\n\n\nmember\nelectric_bike\n984688\n10968684\n480.00\n11.14\n\n\ncasual\ndocked_bike\n136794\n6899998\n32035.45\n50.44\n\n\n\n\n\n\n\n\n\n\nWe can clearly notice in Table 1 that member riders have more number of rides with both classic and electric bikes while the average ride length is higher with casual riders.\n\nCalculating and visualizing Average ride length by “Rider type”.\n\n\naverage_ride_by_rideable_type &lt;- data_one_year %&gt;%\n  rename(\"Rider type\" = member_casual, \"Bike type\" = rideable_type) %&gt;% \n  summarise(ride_length = sum(ride_length, na.rm = TRUE),\n            ride_count = n(),\n            avg_ride_length = ride_length/ride_count,\n            .by = c(`Rider type`, `Bike type`)) %&gt;% \n  ggplot(aes(`Rider type`, avg_ride_length)) + \n  geom_col(aes(fill = `Bike type`), position = \"dodge\") + \n  labs(x = \"Bike type\", y = \"Avg Length of Ride(Minutes)\",\n       title  = \"Average ride length by Bike type\") +\n  theme_minimal() +\n  theme(plot.title = element_text(size = 18),\n        legend.position = \"bottom\")\n\naverage_ride_by_rideable_type\n\n\n\n\n\n\n\nFigure 1: Average Ride Length by Rider type and Member type\n\n\n\n\n\nThe above Figure 1 clearly shows that members average ride lengths between bike types doesn’t differ much for member riders but differs with casual riders upto 8 minutes.\n\n\n\n\n\n\nNote\n\n\n\nFurther down in the analysis “docked_bike” type is dropped as no proper documentation is available in the course.\n\n\n\n\n\n\n\n\nCalculating and visualizing ride patterns in a week for number of rides.\n\n\nrideable_order &lt;- c(\"classic_bike\", \"electric_bike\", \"docked_bike\")\n\nrides_on_days &lt;- data_one_year %&gt;%\n  filter(rideable_type != \"docked_bike\") %&gt;%\n  mutate(month = month(started_at, label = TRUE,\n                       abbr = FALSE)) %&gt;% \n  mutate(rideable_type = factor(rideable_type,\n                                levels = rideable_order)) %&gt;%  ggplot(aes(wday(started_at, label = TRUE, abbr = FALSE))) + \n  geom_bar(aes(fill = member_casual), position = \"dodge\") +\n  facet_wrap(~month, nrow = 3) + \n  labs(x = \"Day of the Week\", y = \"Number of rides\",\n       title = \"Riding pattrens on Weekdays of each Month\",\n       subtitle = \"From July-2022 to June-2023\",\n       fill = \"Type of Rider\") +\n  theme_light() +\n  theme(legend.position = \"top\",\n        axis.text.x = element_text(angle = 45, hjust = 1),\n        plot.title = element_text(size = 18))\n\nrides_on_days \n\n\n\n\n\n\n\nFigure 2: Riding pattrens in Weekdays of each Month\n\n\n\n\n\nThe above Figure 2 clearly shows how the number of rides change due to seasons. In winters the number of rides decrease very drastically may be because of temperature and snow. In Summers the number of rides are at its peak.\nThe number of rides driven by member riders are increases through the week especially in working week days but for casual riders the rides increase in the weekends. The Figure 2 shows number of rides on Saturdays and Sundays by casual members overtake membership riders in the months of July and August.\n\n\n\nAggregating data for the visualization.\n\nrides_on_days &lt;- data_one_year %&gt;%\n  mutate(day = wday(started_at, label = TRUE, abbr = FALSE),\n         month = month(started_at, label = TRUE, abbr = FALSE)) %&gt;% \n  summarise(ride_count = n(),\n            sum_ride_length = sum(ride_length, na.rm = TRUE),\n            avg_ride_length = mean(ride_length, na.rm = TRUE),\n            .by = c(month, day, member_casual))\n\nrides_on_days \n\n# A tibble: 168 × 6\n   month day       member_casual ride_count sum_ride_length avg_ride_length\n   &lt;ord&gt; &lt;ord&gt;     &lt;chr&gt;              &lt;int&gt;           &lt;dbl&gt;           &lt;dbl&gt;\n 1 July  Wednesday member             45928         603429.            13.1\n 2 July  Sunday    member             44328         685475.            15.5\n 3 July  Wednesday casual             31631         703360.            22.2\n 4 July  Saturday  member             51852         816009.            15.7\n 5 July  Thursday  casual             34895         759013.            21.8\n 6 July  Friday    member             46398         614466.            13.2\n 7 July  Friday    casual             41198         959168.            23.3\n 8 July  Monday    casual             32960         917333.            27.8\n 9 July  Monday    member             38746         530041.            13.7\n10 July  Sunday    casual             59528        1714086.            28.8\n# ℹ 158 more rows\n\n\nLet’s visualize the aggregated data\n\nrides_on_days_len &lt;- rides_on_days %&gt;%\n  ggplot(aes(day, sum_ride_length))+\n  geom_col(aes(fill = member_casual), position = \"dodge\")+\n  facet_wrap(~month, ncol = 3)+\n  labs(x = \"Day of the Week\", y = \"Total Length of Rides (Minutes)\",\n       title = \"Total Minutes driven by Riders\",\n       fill = \"Type of Rider\") +\n  theme(legend.position = \"top\",\n        axis.text.x = element_text(angle = 45, hjust = 1),\n        plot.title = element_text(size = 18))\n\nrides_on_days_len\n\n\n\n\n\n\n\nFigure 3: Total Ride lengths through out the year by member types.\n\n\n\n\n\n\nrides_on_days_len_avg &lt;- rides_on_days %&gt;%\n  ggplot(aes(day, avg_ride_length))+\n  geom_col(aes(fill = member_casual), position = \"dodge\")+\n  facet_wrap(~month, ncol = 3) +\n  labs(x = \"Day of the Week\", y = \"Average Length of Rides (Minutes)\",\n       title = \"Average Minutes driven by Riders\",\n       fill = \"Type of Rider\") +\n  theme(legend.position = \"top\",\n        axis.text.x = element_text(angle = 45, hjust = 1),\n        plot.title = element_text(size = 18))\n\nrides_on_days_len_avg\n\n\n\n\n\n\n\nFigure 4: Average Ride lengths through out year by member types.\n\n\n\n\n\nThe ride length is varying across months and seasons just as number of rides but average ride length is not fluctuating that much across the year.\n\n\n\nLet’s look at when the rides are starting to know at what time of day the rides peak and are at the lowest.\n\nrides_on_time_of_day &lt;- data_one_year %&gt;%\n  mutate(time_of_day = format(as.POSIXct(ended_at), \"%H\"),\n         wk_day = wday(started_at, label = TRUE, abbr = FALSE)) %&gt;% \n  summarise(ride_id = n(),\n            .by = c(time_of_day, member_casual))\n\nrides_on_time_of_day %&gt;%\n  ggplot(aes(time_of_day, ride_id, fill = ride_id )) +\n  geom_col() +\n  labs(x = \"Hour of the day\", y = \"Number of Rides\",\n       fill = \"Max Rides\") +\n  facet_wrap(~member_casual, ncol = 1) +\n  scale_y_continuous(\n  labels = scales::number_format(scale = 1e-3, suffix = \"K\")) +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nMost of the rides start at 5:00 PM in the evening showing most of the rides begin after office hours for both caual and member riders but members peak twice at 8:00 AM and 5:00 PM but the casual riders peak only once at 5:00 PM.\n\n\n\n\n\n\nRemoving “NA” and blanks from the stations columns.\n\n\ndata_one_year &lt;- data_one_year_raw %&gt;%\n  mutate(ride_length = difftime(ended_at, started_at,\n                                units = \"min\")) %&gt;%\n  mutate(ride_length = as.numeric(ride_length)) %&gt;% \n  mutate(ride_length = if_else(ride_length &lt; 0, 0, ride_length)) %&gt;% \n  filter(ride_length &gt;= 2) %&gt;% \n  drop_na(start_station_name, end_station_name ) %&gt;% \n  filter(start_station_name != \"\" & end_station_name != \"\",\n         started_at != ended_at)\n\nglimpse(data_one_year)\n\nRows: 4,243,662\nColumns: 14\n$ ride_id            &lt;chr&gt; \"954144C2F67B1932\", \"57765852588AD6E0\", \"B5B6BE4431…\n$ rideable_type      &lt;chr&gt; \"classic_bike\", \"classic_bike\", \"classic_bike\", \"cl…\n$ started_at         &lt;dttm&gt; 2022-07-05 08:12:47, 2022-07-03 13:58:49, 2022-07-…\n$ ended_at           &lt;dttm&gt; 2022-07-05 08:24:32, 2022-07-03 14:06:32, 2022-07-…\n$ start_station_name &lt;chr&gt; \"Ashland Ave & Blackhawk St\", \"Buckingham Fountain …\n$ start_station_id   &lt;chr&gt; \"13224\", \"15541\", \"15541\", \"TA1307000117\", \"15535\",…\n$ end_station_name   &lt;chr&gt; \"Kingsbury St & Kinzie St\", \"Michigan Ave & 8th St\"…\n$ end_station_id     &lt;chr&gt; \"KA1503000043\", \"623\", \"TA1307000164\", \"TA130700005…\n$ start_lat          &lt;dbl&gt; 41.90707, 41.86962, 41.86962, 41.89147, 41.88461, 4…\n$ start_lng          &lt;dbl&gt; -87.66725, -87.62398, -87.62398, -87.62676, -87.644…\n$ end_lat            &lt;dbl&gt; 41.88918, 41.87277, 41.79526, 41.93625, 41.86712, 4…\n$ end_lng            &lt;dbl&gt; -87.63851, -87.62398, -87.59647, -87.65266, -87.641…\n$ member_casual      &lt;chr&gt; \"member\", \"casual\", \"casual\", \"member\", \"member\", \"…\n$ ride_length        &lt;dbl&gt; 11.750000, 7.716667, 58.483333, 26.300000, 8.716667…\n\n\n\nMaking a new column to identify travelled routes.\n\n\ndata_one_year &lt;- data_one_year %&gt;% \n  mutate(stations_travelled = paste(start_station_name, \n                                     \"-\", end_station_name))\n\nglimpse(data_one_year)\n\nRows: 4,243,662\nColumns: 15\n$ ride_id            &lt;chr&gt; \"954144C2F67B1932\", \"57765852588AD6E0\", \"B5B6BE4431…\n$ rideable_type      &lt;chr&gt; \"classic_bike\", \"classic_bike\", \"classic_bike\", \"cl…\n$ started_at         &lt;dttm&gt; 2022-07-05 08:12:47, 2022-07-03 13:58:49, 2022-07-…\n$ ended_at           &lt;dttm&gt; 2022-07-05 08:24:32, 2022-07-03 14:06:32, 2022-07-…\n$ start_station_name &lt;chr&gt; \"Ashland Ave & Blackhawk St\", \"Buckingham Fountain …\n$ start_station_id   &lt;chr&gt; \"13224\", \"15541\", \"15541\", \"TA1307000117\", \"15535\",…\n$ end_station_name   &lt;chr&gt; \"Kingsbury St & Kinzie St\", \"Michigan Ave & 8th St\"…\n$ end_station_id     &lt;chr&gt; \"KA1503000043\", \"623\", \"TA1307000164\", \"TA130700005…\n$ start_lat          &lt;dbl&gt; 41.90707, 41.86962, 41.86962, 41.89147, 41.88461, 4…\n$ start_lng          &lt;dbl&gt; -87.66725, -87.62398, -87.62398, -87.62676, -87.644…\n$ end_lat            &lt;dbl&gt; 41.88918, 41.87277, 41.79526, 41.93625, 41.86712, 4…\n$ end_lng            &lt;dbl&gt; -87.63851, -87.62398, -87.59647, -87.65266, -87.641…\n$ member_casual      &lt;chr&gt; \"member\", \"casual\", \"casual\", \"member\", \"member\", \"…\n$ ride_length        &lt;dbl&gt; 11.750000, 7.716667, 58.483333, 26.300000, 8.716667…\n$ stations_travelled &lt;chr&gt; \"Ashland Ave & Blackhawk St - Kingsbury St & Kinzie…\n\n\n\nFinding which route is most traveled by casual riders.\n\n\nmost_travelled_routes_casual &lt;- data_one_year %&gt;%\n  filter(member_casual == \"casual\",\n         ride_length &gt;= 2) %&gt;% \n  summarise(ride_count = n(),\n            avg_ride_length = round(mean(ride_length), 2),\n            .by = c(stations_travelled)) %&gt;%\n  arrange(desc(ride_count))\n\nhead(most_travelled_routes_casual)\n\n# A tibble: 6 × 3\n  stations_travelled                                  ride_count avg_ride_length\n  &lt;chr&gt;                                                    &lt;int&gt;           &lt;dbl&gt;\n1 Streeter Dr & Grand Ave - Streeter Dr & Grand Ave         8259            46.3\n2 DuSable Lake Shore Dr & Monroe St - DuSable Lake S…       5726            38.3\n3 DuSable Lake Shore Dr & Monroe St - Streeter Dr & …       4840            27.1\n4 Michigan Ave & Oak St - Michigan Ave & Oak St             3754            50.9\n5 Millennium Park - Millennium Park                         3188            45.4\n6 Streeter Dr & Grand Ave - DuSable Lake Shore Dr & …       2663            27.8\n\nNROW(most_travelled_routes_casual)\n\n[1] 130373\n\n\nStreeter Dr & Grand Ave - Streeter Dr & Grand Ave stands to be the most popular station with 9698 rides by casual riders.\n\nmost_travelled_routes_member &lt;- data_one_year  %&gt;%\n  filter(member_casual == \"member\") %&gt;% \n  summarise(ride_count = n(),\n            total_ride_length = sum(ride_length),\n            ride_length = round(mean(ride_length), 2),\n            .by = stations_travelled) %&gt;% arrange(desc(ride_count))\n\nhead(most_travelled_routes_member)\n\n# A tibble: 6 × 4\n  stations_travelled                    ride_count total_ride_length ride_length\n  &lt;chr&gt;                                      &lt;int&gt;             &lt;dbl&gt;       &lt;dbl&gt;\n1 Ellis Ave & 60th St - University Ave…       6149            25928.        4.22\n2 University Ave & 57th St - Ellis Ave…       5771            26605.        4.61\n3 Ellis Ave & 60th St - Ellis Ave & 55…       5676            28427.        5.01\n4 Ellis Ave & 55th St - Ellis Ave & 60…       5347            27187.        5.08\n5 State St & 33rd St - Calumet Ave & 3…       4037            17795.        4.41\n6 Calumet Ave & 33rd St - State St & 3…       3836            15538.        4.05\n\nNROW(most_travelled_routes_member)\n\n[1] 144802\n\n\nEllis Ave & 60th St - University Ave & 57th St stands as the most traveled route by member riders with 6153 rides per anum.\n\nFinding which station has most ride starting points and which station has most ending points.\n\n\nmost_starting_points &lt;- data_one_year %&gt;% \n  summarise(ride_count = n(),\n            .by = start_station_name) %&gt;%\n  select(start_station_name, ride_count) %&gt;%\n  slice_max(ride_count, n = 10)\n\nmost_starting_points\n\n# A tibble: 10 × 2\n   start_station_name                 ride_count\n   &lt;chr&gt;                                   &lt;int&gt;\n 1 Streeter Dr & Grand Ave                 63899\n 2 DuSable Lake Shore Dr & Monroe St       36757\n 3 Michigan Ave & Oak St                   35050\n 4 DuSable Lake Shore Dr & North Blvd      34167\n 5 Wells St & Concord Ln                   32175\n 6 Clark St & Elm St                       31832\n 7 Kingsbury St & Kinzie St                30820\n 8 Millennium Park                         29894\n 9 Theater on the Lake                     28864\n10 Wells St & Elm St                       27152\n\nmost_starting_points$ride_count %&gt;% sum()\n\n[1] 350610\n\nmost_ending_points &lt;- data_one_year %&gt;% \n  summarise(ride_count = n(),\n            .by = end_station_name) %&gt;%\n  select(end_station_name, ride_count)  %&gt;% \n  slice_max(ride_count, n = 10)\n\nmost_ending_points\n\n# A tibble: 10 × 2\n   end_station_name                   ride_count\n   &lt;chr&gt;                                   &lt;int&gt;\n 1 Streeter Dr & Grand Ave                 65542\n 2 DuSable Lake Shore Dr & North Blvd      37093\n 3 Michigan Ave & Oak St                   35977\n 4 DuSable Lake Shore Dr & Monroe St       35629\n 5 Wells St & Concord Ln                   32879\n 6 Clark St & Elm St                       31394\n 7 Millennium Park                         31023\n 8 Kingsbury St & Kinzie St                29805\n 9 Theater on the Lake                     29482\n10 Wells St & Elm St                       27360\n\nmost_ending_points$ride_count %&gt;% sum()\n\n[1] 356184\n\n\nStreeter Dr & Grand Ave found to be the most popular station as most rides start and end at that station.\n\n\n\nJust because we filtered the data with NA’s that does not mean that the data is not helpful, it just means that it does not our fulfill specific need when calculating or manipulating data.\nLet’s look at NA’s in the data once again.\n\nna_in_cols &lt;- data_one_year_raw %&gt;% map( ~sum(is.na(.))) %&gt;% unlist()\n\nna_in_cols\n\n           ride_id      rideable_type         started_at           ended_at \n                 0                  0                  0                  0 \nstart_station_name   start_station_id   end_station_name     end_station_id \n            857860             857992             915655             915796 \n         start_lat          start_lng            end_lat            end_lng \n                 0                  0               5795               5795 \n     member_casual \n                 0 \n\n\n\nWe can see that the start_station_name and end_station_name have majority of NA’s it means that rides are starting and ending where stations are not there.\n\n\nprop_na &lt;- na_in_cols[\"start_station_name\"]/nrow(data_one_year_raw)\n\nprop_na\n\nstart_station_name \n          0.148433 \n\n\n\n14.84% of data in start_station_name is missing and good thing is that none of the start_lng and start_lat have any NA’s and we can use this for find the most traveled routes.\n\n\ndata_na_one_year &lt;- data_one_year_raw %&gt;% \n  filter(is.na(start_station_name) | start_station_name == \"\") %&gt;% \n  drop_na(end_lat, end_lng)\n  \nglimpse(data_na_one_year)\n\nRows: 857,860\nColumns: 13\n$ ride_id            &lt;chr&gt; \"DCB3D2C9B63999EC\", \"D1ACA8280DA02AE3\", \"EF98673429…\n$ rideable_type      &lt;chr&gt; \"electric_bike\", \"electric_bike\", \"electric_bike\", …\n$ started_at         &lt;dttm&gt; 2022-07-04 15:04:26, 2022-07-12 14:43:51, 2022-07-…\n$ ended_at           &lt;dttm&gt; 2022-07-04 15:32:38, 2022-07-12 14:49:28, 2022-07-…\n$ start_station_name &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ start_station_id   &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ end_station_name   &lt;chr&gt; \"Ashland Ave & Blackhawk St\", \"Cornell Ave & Hyde P…\n$ end_station_id     &lt;chr&gt; \"13224\", \"KA1503000007\", \"KA1503000007\", \"847\", \"48…\n$ start_lat          &lt;dbl&gt; 41.95, 41.80, 41.80, 41.74, 42.02, 41.95, 41.95, 41…\n$ start_lng          &lt;dbl&gt; -87.64, -87.59, -87.59, -87.55, -87.69, -87.67, -87…\n$ end_lat            &lt;dbl&gt; 41.90707, 41.80241, 41.80241, 41.73000, 42.01000, 4…\n$ end_lng            &lt;dbl&gt; -87.66725, -87.58692, -87.58692, -87.55000, -87.690…\n$ member_casual      &lt;chr&gt; \"member\", \"member\", \"member\", \"member\", \"member\", \"…\n\n\n\nNow let’s make new columns start_point with start_lng and start_lat and end_point with end_lat and end_lng.\n\n\ndata_na_one_year &lt;- data_na_one_year %&gt;%\n    mutate(start_point = paste(start_lat, start_lng),\n           end_point = paste(end_lat, end_lng))\n\nglimpse(data_na_one_year)\n\nRows: 857,860\nColumns: 15\n$ ride_id            &lt;chr&gt; \"DCB3D2C9B63999EC\", \"D1ACA8280DA02AE3\", \"EF98673429…\n$ rideable_type      &lt;chr&gt; \"electric_bike\", \"electric_bike\", \"electric_bike\", …\n$ started_at         &lt;dttm&gt; 2022-07-04 15:04:26, 2022-07-12 14:43:51, 2022-07-…\n$ ended_at           &lt;dttm&gt; 2022-07-04 15:32:38, 2022-07-12 14:49:28, 2022-07-…\n$ start_station_name &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ start_station_id   &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ end_station_name   &lt;chr&gt; \"Ashland Ave & Blackhawk St\", \"Cornell Ave & Hyde P…\n$ end_station_id     &lt;chr&gt; \"13224\", \"KA1503000007\", \"KA1503000007\", \"847\", \"48…\n$ start_lat          &lt;dbl&gt; 41.95, 41.80, 41.80, 41.74, 42.02, 41.95, 41.95, 41…\n$ start_lng          &lt;dbl&gt; -87.64, -87.59, -87.59, -87.55, -87.69, -87.67, -87…\n$ end_lat            &lt;dbl&gt; 41.90707, 41.80241, 41.80241, 41.73000, 42.01000, 4…\n$ end_lng            &lt;dbl&gt; -87.66725, -87.58692, -87.58692, -87.55000, -87.690…\n$ member_casual      &lt;chr&gt; \"member\", \"member\", \"member\", \"member\", \"member\", \"…\n$ start_point        &lt;chr&gt; \"41.95 -87.64\", \"41.8 -87.59\", \"41.8 -87.59\", \"41.7…\n$ end_point          &lt;chr&gt; \"41.907066 -87.667252\", \"41.802406 -87.586924\", \"41…\n\n\n\nAggregating data to check for the most traveled routes without a start_station name.\n\nFirst join start_point and end_point to make route_travelled then count the rides by routes_travelled to see the most traveled path.\n\nmost_travelled_na_routes &lt;- data_na_one_year %&gt;%\n  filter(start_point != end_point) %&gt;% \n  mutate(route_travelled = paste(start_point, \",\", end_point)) %&gt;% \n  summarise(ride_count = n(),\n            .by = route_travelled) %&gt;%\n  slice_max(ride_count, n=10)\n\nmost_travelled_na_routes\n\n# A tibble: 10 × 2\n   route_travelled                             ride_count\n   &lt;chr&gt;                                            &lt;int&gt;\n 1 41.79 -87.6 , 41.8 -87.59                         1459\n 2 41.79 -87.59 , 41.79 -87.6                        1354\n 3 41.8 -87.59 , 41.79 -87.6                         1335\n 4 41.79 -87.6 , 41.79 -87.59                        1320\n 5 41.8 -87.6 , 41.79 -87.6                          1099\n 6 41.79 -87.6 , 41.78509714636 -87.6010727606       1058\n 7 41.79 -87.6 , 41.8 -87.6                           999\n 8 41.79 -87.6 , 41.799568 -87.594747                 917\n 9 41.79 -87.6 , 41.78 -87.6                          697\n10 41.89 -87.63 , 41.9 -87.63                         690\n\nsum(most_travelled_na_routes[\"ride_count\"])\n\n[1] 10928\n\n\n\n10928 rides are not small when compared to most traveled routes, but 10928 rides in 5 million rides is not that high.\n\n\n\n\n\nAs casual members go for long rides on the weekends, offers on weekend rides with membership buying may help attract more memberships and might also make membership riders to make weekend end trips.\nTo increase the memberships of the Cyclist Bike Share the company needs to place stations where most new rides are coming up and routes are travelled more."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Ajay Shankar A",
    "section": "",
    "text": "This is a blog for projects completed successfully by me with R-programming language. This blog will include projects from basic “Exploratory Data Analysis(EDA)” to complex “Machine Learning(ML)” projects."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Ajay Shankar A",
    "section": "1 EDUCATION",
    "text": "1 EDUCATION\n\nUniversity of Agricultural Sciences, Dharwad | Dharwad, Karnataka | Masters in Forest Biology and Tree Improvement | Sept 2019 - Nov 2022\nCollege of Forestry, Sirsi | Uttara Kannada, Karnataka | B.Sc in Forestry | Aug 2015 - April 2019"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "Ajay Shankar A",
    "section": "2 Experience",
    "text": "2 Experience\n\nTechnical Assistant | Social Forest Department, Siruguppa | Dec 2022 - Present\nResearch Associate | EMPRI | May 2022 - Aug 2022"
  },
  {
    "objectID": "about.html#citationsprojects",
    "href": "about.html#citationsprojects",
    "title": "Ajay Shankar A",
    "section": "3 Citations(Projects)",
    "text": "3 Citations(Projects)\n\nAvailability of Wood for Handicrafts in Karnataka - Strengthening livelihoods and job creation.\nAn Assessment of Wood Availability in Karnataka"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Projects with R language",
    "section": "",
    "text": "This is a blog for projects completed successfully by me with R-programming language. This blog will include projects from basic “Exploratory Data Analysis(EDA)” to complex “Machine Learning(ML)” projects.\n\nTo find all the other projects using other tools visit my Data Science Portfolio Website\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIris ML classification\n\n\n\n\n\n\nMachine Learning\n\n\nRandom Forest\n\n\nKNN\n\n\nEDA\n\n\n\n\n\n\n\n\n\nOct 21, 2024\n\n\nAjay A\n\n\n\n\n\n\n\n\n\n\n\n\nMedicines Side-effects and their Substitutes\n\n\n\n\n\n\nAnalysis\n\n\nCode\n\n\nEDA\n\n\n\n\n\n\n\n\n\nOct 10, 2024\n\n\nAjay A\n\n\n\n\n\n\n\n\n\n\n\n\nWhy Data Analytics?\n\n\n\n\n\n\nArticle\n\n\n\n\n\n\n\n\n\nApr 15, 2024\n\n\nAjay Shankar A\n\n\n\n\n\n\n\n\n\n\n\n\nCYCLIST BIKE SHARE\n\n\n\n\n\n\nAnalysis\n\n\nCode\n\n\nEDA\n\n\n\n\n\n\n\n\n\nDec 14, 2023\n\n\nAjay Shankar A\n\n\n\n\n\n\n\n\n\n\n\n\nPredict Price of Diamonds\n\n\n\n\n\n\nAnalysis\n\n\nR-code\n\n\nEDA\n\n\nModeling\n\n\nMachine Learning\n\n\n\n\n\n\n\n\n\nDec 10, 2023\n\n\nAjay Shankar A\n\n\n\n\n\n\n\n\n\n\n\n\nPredict the sex of the Penguin Species\n\n\n\n\n\n\nMachine Learning\n\n\nEDA\n\n\nCode\n\n\nAnalysis\n\n\n\n\n\n\n\n\n\nNov 21, 2023\n\n\nAjay Shankar A\n\n\n\n\n\n\n\n\n\n\n\n\nEffect of treatments on leaves\n\n\n\n\n\n\nAnalysis\n\n\nCode\n\n\nEDA\n\n\n\n\n\n\n\n\n\nNov 11, 2023\n\n\nAjay Shankar A\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/data_analysis_for-thesis/data_treatment_effect.html",
    "href": "posts/data_analysis_for-thesis/data_treatment_effect.html",
    "title": "Effect of treatments on leaves",
    "section": "",
    "text": "An experiment was conducted to find the rooting potential of the leaves mainly angiosperms to root when treated with different phyto-hormones.\n\n\nIn the experiment, 4 treatments were applied on 8 different species, and the observations included:\n\nNumber of roots (num_roots_n).\nLength of the longest root in centimeters (lng_long_root_cm).\nDiameter of the longest root in millimeters (dia_long_root_mm).\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(gt)\n\n\n\n\nThe data is loaded as a dataframe thesisdata and columns such as treatments(Treatment) and species(Species) are changes to factors as they are not suitable as strings.\n\n# Loading data\nthesisdata &lt;- read_csv(\"thesisdata.csv\",\n                       show_col_types = FALSE)\n# formating data\nths_data &lt;- thesisdata\nths_data$Treatment &lt;- as.factor(ths_data$Treatment)\nths_data$Treatment &lt;- \n  factor(ths_data$Treatment,levels = c(\"Control\",\n                                       \"Coconut water\", \n                                       \"IBA 1000ppm\",\n                          \"IBA 100ppm + Coconut water\"))\nths_data$Species &lt;- factor(ths_data$Species)\n\nths_data %&gt;% head()\n\n# A tibble: 6 × 5\n  Species            Treatment num_roots_n lng_long_root_cm dia_long_root_mm\n  &lt;fct&gt;              &lt;fct&gt;           &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;\n1 Conidium verigatum Control             0                0                0\n2 Conidium verigatum Control             0                0                0\n3 Conidium verigatum Control             0                0                0\n4 Conidium verigatum Control             0                0                0\n5 Conidium verigatum Control             0                0                0\n6 Conidium verigatum Control             0                0                0\n\n\n\nConfirming that only 8 species and 4 treatments are used in the experiment.\n\n\ntreatments_and_species &lt;- list(unique(ths_data$Treatment), unique(ths_data$Species))\n\nnames(treatments_and_species) &lt;- c(\"Treatment\", \"Species\")\ntreatments_and_species\n\n$Treatment\n[1] Control                    Coconut water             \n[3] IBA 1000ppm                IBA 100ppm + Coconut water\nLevels: Control Coconut water IBA 1000ppm IBA 100ppm + Coconut water\n\n$Species\n[1] Conidium verigatum    Rawolfia tetraphylla  Justicia Sps         \n[4] Zizupus rugosa        Jasminum Sps          Cyclea pelteta       \n[7] Bredelia scandens     Hemigraphis alternata\n8 Levels: Bredelia scandens Conidium verigatum ... Zizupus rugosa\n\n\n\n\n\nThe data is aggregated in Table 1 by the average of root lengths with standard deviation(SD)\n\nths_data_1 &lt;- ths_data %&gt;% group_by(Species, Treatment) %&gt;% \n  summarise(avg_n_roots = mean(num_roots_n),\n            SD_n_roots = sd(num_roots_n),\n            avg_lng_root = mean(lng_long_root_cm),\n            SD_lng_root = sd(lng_long_root_cm),\n            avg_dia_root = mean(dia_long_root_mm),\n            SD_dia_root = sd(dia_long_root_mm)) %&gt;%\n  # rounding of to 2 digits  \n  mutate(across(where(is.double), ~round(., digits = 2))) %&gt;%\n  # combining means and SD into a single column\n  unite(avg_n_roots_SD, avg_n_roots, SD_n_roots, sep = \" \\u00b1 \") %&gt;%\n  unite(avg_lng_root_SD, avg_lng_root, SD_lng_root, sep = \" \\u00b1 \") %&gt;% \n  unite(avg_dia_root_SD, avg_dia_root, SD_dia_root, sep = \" \\u00b1 \") %&gt;%\n  # using 'gt' package to get a table  \n  gt(rowname_col = \"Treatment\") %&gt;% \n  tab_header(\n    title = \"Thesis Data of the Species\",\n    subtitle = \"Influence of growth regulators on the root generation\"\n  ) %&gt;% \n  opt_align_table_header(align = \"center\") %&gt;% \n  cols_label( # renaming columns\n    avg_n_roots_SD = md(\"Mean number of roots \\u00b1 SD\"), #md is markdown language\n    avg_lng_root_SD = md(\"Mean length of longest roots \\u00b1 SD (cm)\"),\n    avg_dia_root_SD = md(\"Mean diameter of  longest roots \\u00b1 SD (mm)\")\n  ) %&gt;% # fixing columns width\n  cols_width(Treatment ~ px(150),\n             avg_n_roots_SD ~ px(150),\n             avg_lng_root_SD ~ px(150),\n             avg_dia_root_SD ~ px(150),\n            ) %&gt;% \n  cols_align(align = \"center\")\n\nths_data_1\n\n\n\nTable 1: Aggregating data and finding average of each parameter by species and treatment\n\n\n\n\n\n\n\n\n\nThesis Data of the Species\n\n\nInfluence of growth regulators on the root generation\n\n\n\nMean number of roots ± SD\nMean length of longest roots ± SD (cm)\nMean diameter of longest roots ± SD (mm)\n\n\n\n\nBredelia scandens\n\n\nControl\n0 ± 0\n0 ± 0\n0 ± 0\n\n\nCoconut water\n0 ± 0\n0 ± 0\n0 ± 0\n\n\nIBA 1000ppm\n3.22 ± 0.44\n6.06 ± 2.02\n0.89 ± 0.17\n\n\nIBA 100ppm + Coconut water\n3 ± 0.5\n6.02 ± 1.26\n0.77 ± 0.2\n\n\nConidium verigatum\n\n\nControl\n0 ± 0\n0 ± 0\n0 ± 0\n\n\nCoconut water\n2.89 ± 1.05\n4.52 ± 1.1\n0.62 ± 0.15\n\n\nIBA 1000ppm\n7.89 ± 3.62\n7.52 ± 2.55\n0.76 ± 0.15\n\n\nIBA 100ppm + Coconut water\n2.44 ± 0.88\n4.74 ± 1.09\n0.81 ± 0.18\n\n\nCyclea pelteta\n\n\nControl\n0 ± 0\n0 ± 0\n0 ± 0\n\n\nCoconut water\n3.33 ± 0.71\n2.48 ± 0.37\n0.18 ± 0.04\n\n\nIBA 1000ppm\n5.22 ± 1.39\n4.89 ± 1.17\n0.28 ± 0.07\n\n\nIBA 100ppm + Coconut water\n8.89 ± 2.03\n12.64 ± 3.23\n0.45 ± 0.1\n\n\nHemigraphis alternata\n\n\nControl\n0 ± 0\n0 ± 0\n0 ± 0\n\n\nCoconut water\n1.11 ± 0.33\n5.89 ± 0.84\n0.65 ± 0.11\n\n\nIBA 1000ppm\n2.22 ± 0.83\n8.06 ± 1.58\n0.77 ± 0.11\n\n\nIBA 100ppm + Coconut water\n1.11 ± 0.33\n5.33 ± 1.93\n0.57 ± 0.09\n\n\nJasminum Sps\n\n\nControl\n0 ± 0\n0 ± 0\n0 ± 0\n\n\nCoconut water\n1.89 ± 0.6\n8.06 ± 0.88\n0.5 ± 0.05\n\n\nIBA 1000ppm\n5.11 ± 1.17\n14.82 ± 1.59\n0.88 ± 0.19\n\n\nIBA 100ppm + Coconut water\n2.67 ± 1.22\n12.12 ± 2.27\n1.71 ± 0.25\n\n\nJusticia Sps\n\n\nControl\n0 ± 0\n0 ± 0\n0 ± 0\n\n\nCoconut water\n1.22 ± 0.44\n3.28 ± 1.24\n0.35 ± 0.21\n\n\nIBA 1000ppm\n1.89 ± 0.78\n8.64 ± 2.41\n1.08 ± 0.26\n\n\nIBA 100ppm + Coconut water\n1.22 ± 0.44\n5.47 ± 1.84\n1.35 ± 0.23\n\n\nRawolfia tetraphylla\n\n\nControl\n0.56 ± 1.13\n0.26 ± 0.51\n0.04 ± 0.09\n\n\nCoconut water\n1.78 ± 0.97\n5.4 ± 2.64\n0.36 ± 0.1\n\n\nIBA 1000ppm\n4.78 ± 2.17\n11.59 ± 4.79\n0.4 ± 0.12\n\n\nIBA 100ppm + Coconut water\n2.67 ± 1.41\n10.09 ± 2.34\n0.61 ± 0.19\n\n\nZizupus rugosa\n\n\nControl\n0 ± 0\n0 ± 0\n0 ± 0\n\n\nCoconut water\n1.22 ± 0.83\n2.94 ± 1.34\n0.38 ± 0.18\n\n\nIBA 1000ppm\n4.44 ± 1.59\n5.56 ± 2.08\n0.53 ± 0.15\n\n\nIBA 100ppm + Coconut water\n2.22 ± 0.67\n4.26 ± 1.11\n0.63 ± 0.17\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlotting a bar graph Figure 1 to see how each treatment performed on each species.\n\n\nths_data |&gt; summarise(avg_n_roots = mean(num_roots_n),\n                      avg_lng_root = mean(lng_long_root_cm),\n                      avg_dia_root = mean(dia_long_root_mm),\n                      .by = c(Species, Treatment)) %&gt;%\n  rename(\"Mean number of Roots\" = avg_n_roots,\n         \"Mean length of Longest root(cm)\" = avg_lng_root,\n         \"Mean diameter of Longest roots(mm)\" = avg_dia_root) %&gt;% \n  tidyr::pivot_longer(c(\"Mean number of Roots\", \"Mean length of Longest root(cm)\",\n                        \"Mean diameter of Longest roots(mm)\")) |&gt; \n  ggplot(aes(x = Species, y = value, fill = name)) + \n  geom_col(alpha = 0.7, position = \"dodge\") + \n  facet_wrap(~Treatment, ncol = 2) + \n  theme_bw() + labs(y = \" \", fill = \"Parameters\",\n                    title = \"Effect of treatments on rooting\") +\n theme(legend.position = \"bottom\",\n       axis.text.x = element_text(angle = 45, hjust = 1),\n       plot.title = element_text(size = 18))\n\n\n\n\n\n\n\nFigure 1: Influence of pytohormone treatments on leaves of different species\n\n\n\n\n\n\nWe can clearly see that Control treatment is not producing any roots in majority of the species.\nIBA 1000ppm is clearly showing most promising results in most of the species in the graph.\n\n\n\n\nWe are going to filter out the control treatment as it is not significant at producing roots at all and plot Figure 2 to find which treatment has better correlation at producing roots with higher diameters.\n\nths_data |&gt; filter(Treatment!= \"Control\") |&gt; \n   ggplot(aes(x = lng_long_root_cm, y = dia_long_root_mm)) + \n  geom_point(aes(color = Treatment)) + \n  geom_smooth(method = \"lm\", aes(group = Treatment, color = Treatment)) +\n  labs(title = \"Plotting Root length Vs Root Diameter\",\n       x = \"Length of the longest root\", \n       y = \"Diameter of the longest root\") +\n  theme(legend.position = \"bottom\",\n        plot.title = element_text(size = 18))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nFigure 2: Plotting a scatter plot to find correlation between root lengths and diameters\n\n\n\n\n\n\nThe graph clearly shows that the Coconut Water treatment has the highest slope but other treatments which produced higher root lengths did not have a correlation as that of Coconut Water treatment.\n\n\n\n\nLet’s conduct a anova test on the data after filtering Control as it has not given any roots.\nLet H0 be Null hypothesis H1 be Alternate hypothesis\n\n# ANOVA Model\nmodel_anova &lt;- ths_data %&gt;%\n  filter(Treatment != \"Control\") %&gt;%\n  aov(num_roots_n ~ Treatment, data = .)\n\n# Summary\nmodel_anova %&gt;% summary()\n\n             Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nTreatment     2  256.0  128.00   27.43 2.51e-11 ***\nResiduals   213  993.9    4.67                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# TukeyHSD\nmodel_anova %&gt;% TukeyHSD(conf.level = 0.95)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = num_roots_n ~ Treatment, data = .)\n\n$Treatment\n                                              diff        lwr        upr\nIBA 1000ppm-Coconut water                 2.666667  1.8169277  3.5164057\nIBA 100ppm + Coconut water-Coconut water  1.347222  0.4974832  2.1969612\nIBA 100ppm + Coconut water-IBA 1000ppm   -1.319444 -2.1691835 -0.4697054\n                                             p adj\nIBA 1000ppm-Coconut water                0.0000000\nIBA 100ppm + Coconut water-Coconut water 0.0006848\nIBA 100ppm + Coconut water-IBA 1000ppm   0.0009086\n\n\nWe can see from p adj that all the treatments means are different and we can reject the H0 hypothesis. H1 holds at 95% confidence interval."
  },
  {
    "objectID": "posts/data_analysis_for-thesis/data_treatment_effect.html#effect-of-treatments-on-leaves-to-produce-roots",
    "href": "posts/data_analysis_for-thesis/data_treatment_effect.html#effect-of-treatments-on-leaves-to-produce-roots",
    "title": "Effect of treatments on leaves",
    "section": "",
    "text": "An experiment was conducted to find the rooting potential of the leaves mainly angiosperms to root when treated with different phyto-hormones.\n\n\nIn the experiment, 4 treatments were applied on 8 different species, and the observations included:\n\nNumber of roots (num_roots_n).\nLength of the longest root in centimeters (lng_long_root_cm).\nDiameter of the longest root in millimeters (dia_long_root_mm).\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(gt)\n\n\n\n\nThe data is loaded as a dataframe thesisdata and columns such as treatments(Treatment) and species(Species) are changes to factors as they are not suitable as strings.\n\n# Loading data\nthesisdata &lt;- read_csv(\"thesisdata.csv\",\n                       show_col_types = FALSE)\n# formating data\nths_data &lt;- thesisdata\nths_data$Treatment &lt;- as.factor(ths_data$Treatment)\nths_data$Treatment &lt;- \n  factor(ths_data$Treatment,levels = c(\"Control\",\n                                       \"Coconut water\", \n                                       \"IBA 1000ppm\",\n                          \"IBA 100ppm + Coconut water\"))\nths_data$Species &lt;- factor(ths_data$Species)\n\nths_data %&gt;% head()\n\n# A tibble: 6 × 5\n  Species            Treatment num_roots_n lng_long_root_cm dia_long_root_mm\n  &lt;fct&gt;              &lt;fct&gt;           &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;\n1 Conidium verigatum Control             0                0                0\n2 Conidium verigatum Control             0                0                0\n3 Conidium verigatum Control             0                0                0\n4 Conidium verigatum Control             0                0                0\n5 Conidium verigatum Control             0                0                0\n6 Conidium verigatum Control             0                0                0\n\n\n\nConfirming that only 8 species and 4 treatments are used in the experiment.\n\n\ntreatments_and_species &lt;- list(unique(ths_data$Treatment), unique(ths_data$Species))\n\nnames(treatments_and_species) &lt;- c(\"Treatment\", \"Species\")\ntreatments_and_species\n\n$Treatment\n[1] Control                    Coconut water             \n[3] IBA 1000ppm                IBA 100ppm + Coconut water\nLevels: Control Coconut water IBA 1000ppm IBA 100ppm + Coconut water\n\n$Species\n[1] Conidium verigatum    Rawolfia tetraphylla  Justicia Sps         \n[4] Zizupus rugosa        Jasminum Sps          Cyclea pelteta       \n[7] Bredelia scandens     Hemigraphis alternata\n8 Levels: Bredelia scandens Conidium verigatum ... Zizupus rugosa\n\n\n\n\n\nThe data is aggregated in Table 1 by the average of root lengths with standard deviation(SD)\n\nths_data_1 &lt;- ths_data %&gt;% group_by(Species, Treatment) %&gt;% \n  summarise(avg_n_roots = mean(num_roots_n),\n            SD_n_roots = sd(num_roots_n),\n            avg_lng_root = mean(lng_long_root_cm),\n            SD_lng_root = sd(lng_long_root_cm),\n            avg_dia_root = mean(dia_long_root_mm),\n            SD_dia_root = sd(dia_long_root_mm)) %&gt;%\n  # rounding of to 2 digits  \n  mutate(across(where(is.double), ~round(., digits = 2))) %&gt;%\n  # combining means and SD into a single column\n  unite(avg_n_roots_SD, avg_n_roots, SD_n_roots, sep = \" \\u00b1 \") %&gt;%\n  unite(avg_lng_root_SD, avg_lng_root, SD_lng_root, sep = \" \\u00b1 \") %&gt;% \n  unite(avg_dia_root_SD, avg_dia_root, SD_dia_root, sep = \" \\u00b1 \") %&gt;%\n  # using 'gt' package to get a table  \n  gt(rowname_col = \"Treatment\") %&gt;% \n  tab_header(\n    title = \"Thesis Data of the Species\",\n    subtitle = \"Influence of growth regulators on the root generation\"\n  ) %&gt;% \n  opt_align_table_header(align = \"center\") %&gt;% \n  cols_label( # renaming columns\n    avg_n_roots_SD = md(\"Mean number of roots \\u00b1 SD\"), #md is markdown language\n    avg_lng_root_SD = md(\"Mean length of longest roots \\u00b1 SD (cm)\"),\n    avg_dia_root_SD = md(\"Mean diameter of  longest roots \\u00b1 SD (mm)\")\n  ) %&gt;% # fixing columns width\n  cols_width(Treatment ~ px(150),\n             avg_n_roots_SD ~ px(150),\n             avg_lng_root_SD ~ px(150),\n             avg_dia_root_SD ~ px(150),\n            ) %&gt;% \n  cols_align(align = \"center\")\n\nths_data_1\n\n\n\nTable 1: Aggregating data and finding average of each parameter by species and treatment\n\n\n\n\n\n\n\n\n\nThesis Data of the Species\n\n\nInfluence of growth regulators on the root generation\n\n\n\nMean number of roots ± SD\nMean length of longest roots ± SD (cm)\nMean diameter of longest roots ± SD (mm)\n\n\n\n\nBredelia scandens\n\n\nControl\n0 ± 0\n0 ± 0\n0 ± 0\n\n\nCoconut water\n0 ± 0\n0 ± 0\n0 ± 0\n\n\nIBA 1000ppm\n3.22 ± 0.44\n6.06 ± 2.02\n0.89 ± 0.17\n\n\nIBA 100ppm + Coconut water\n3 ± 0.5\n6.02 ± 1.26\n0.77 ± 0.2\n\n\nConidium verigatum\n\n\nControl\n0 ± 0\n0 ± 0\n0 ± 0\n\n\nCoconut water\n2.89 ± 1.05\n4.52 ± 1.1\n0.62 ± 0.15\n\n\nIBA 1000ppm\n7.89 ± 3.62\n7.52 ± 2.55\n0.76 ± 0.15\n\n\nIBA 100ppm + Coconut water\n2.44 ± 0.88\n4.74 ± 1.09\n0.81 ± 0.18\n\n\nCyclea pelteta\n\n\nControl\n0 ± 0\n0 ± 0\n0 ± 0\n\n\nCoconut water\n3.33 ± 0.71\n2.48 ± 0.37\n0.18 ± 0.04\n\n\nIBA 1000ppm\n5.22 ± 1.39\n4.89 ± 1.17\n0.28 ± 0.07\n\n\nIBA 100ppm + Coconut water\n8.89 ± 2.03\n12.64 ± 3.23\n0.45 ± 0.1\n\n\nHemigraphis alternata\n\n\nControl\n0 ± 0\n0 ± 0\n0 ± 0\n\n\nCoconut water\n1.11 ± 0.33\n5.89 ± 0.84\n0.65 ± 0.11\n\n\nIBA 1000ppm\n2.22 ± 0.83\n8.06 ± 1.58\n0.77 ± 0.11\n\n\nIBA 100ppm + Coconut water\n1.11 ± 0.33\n5.33 ± 1.93\n0.57 ± 0.09\n\n\nJasminum Sps\n\n\nControl\n0 ± 0\n0 ± 0\n0 ± 0\n\n\nCoconut water\n1.89 ± 0.6\n8.06 ± 0.88\n0.5 ± 0.05\n\n\nIBA 1000ppm\n5.11 ± 1.17\n14.82 ± 1.59\n0.88 ± 0.19\n\n\nIBA 100ppm + Coconut water\n2.67 ± 1.22\n12.12 ± 2.27\n1.71 ± 0.25\n\n\nJusticia Sps\n\n\nControl\n0 ± 0\n0 ± 0\n0 ± 0\n\n\nCoconut water\n1.22 ± 0.44\n3.28 ± 1.24\n0.35 ± 0.21\n\n\nIBA 1000ppm\n1.89 ± 0.78\n8.64 ± 2.41\n1.08 ± 0.26\n\n\nIBA 100ppm + Coconut water\n1.22 ± 0.44\n5.47 ± 1.84\n1.35 ± 0.23\n\n\nRawolfia tetraphylla\n\n\nControl\n0.56 ± 1.13\n0.26 ± 0.51\n0.04 ± 0.09\n\n\nCoconut water\n1.78 ± 0.97\n5.4 ± 2.64\n0.36 ± 0.1\n\n\nIBA 1000ppm\n4.78 ± 2.17\n11.59 ± 4.79\n0.4 ± 0.12\n\n\nIBA 100ppm + Coconut water\n2.67 ± 1.41\n10.09 ± 2.34\n0.61 ± 0.19\n\n\nZizupus rugosa\n\n\nControl\n0 ± 0\n0 ± 0\n0 ± 0\n\n\nCoconut water\n1.22 ± 0.83\n2.94 ± 1.34\n0.38 ± 0.18\n\n\nIBA 1000ppm\n4.44 ± 1.59\n5.56 ± 2.08\n0.53 ± 0.15\n\n\nIBA 100ppm + Coconut water\n2.22 ± 0.67\n4.26 ± 1.11\n0.63 ± 0.17\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlotting a bar graph Figure 1 to see how each treatment performed on each species.\n\n\nths_data |&gt; summarise(avg_n_roots = mean(num_roots_n),\n                      avg_lng_root = mean(lng_long_root_cm),\n                      avg_dia_root = mean(dia_long_root_mm),\n                      .by = c(Species, Treatment)) %&gt;%\n  rename(\"Mean number of Roots\" = avg_n_roots,\n         \"Mean length of Longest root(cm)\" = avg_lng_root,\n         \"Mean diameter of Longest roots(mm)\" = avg_dia_root) %&gt;% \n  tidyr::pivot_longer(c(\"Mean number of Roots\", \"Mean length of Longest root(cm)\",\n                        \"Mean diameter of Longest roots(mm)\")) |&gt; \n  ggplot(aes(x = Species, y = value, fill = name)) + \n  geom_col(alpha = 0.7, position = \"dodge\") + \n  facet_wrap(~Treatment, ncol = 2) + \n  theme_bw() + labs(y = \" \", fill = \"Parameters\",\n                    title = \"Effect of treatments on rooting\") +\n theme(legend.position = \"bottom\",\n       axis.text.x = element_text(angle = 45, hjust = 1),\n       plot.title = element_text(size = 18))\n\n\n\n\n\n\n\nFigure 1: Influence of pytohormone treatments on leaves of different species\n\n\n\n\n\n\nWe can clearly see that Control treatment is not producing any roots in majority of the species.\nIBA 1000ppm is clearly showing most promising results in most of the species in the graph.\n\n\n\n\nWe are going to filter out the control treatment as it is not significant at producing roots at all and plot Figure 2 to find which treatment has better correlation at producing roots with higher diameters.\n\nths_data |&gt; filter(Treatment!= \"Control\") |&gt; \n   ggplot(aes(x = lng_long_root_cm, y = dia_long_root_mm)) + \n  geom_point(aes(color = Treatment)) + \n  geom_smooth(method = \"lm\", aes(group = Treatment, color = Treatment)) +\n  labs(title = \"Plotting Root length Vs Root Diameter\",\n       x = \"Length of the longest root\", \n       y = \"Diameter of the longest root\") +\n  theme(legend.position = \"bottom\",\n        plot.title = element_text(size = 18))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nFigure 2: Plotting a scatter plot to find correlation between root lengths and diameters\n\n\n\n\n\n\nThe graph clearly shows that the Coconut Water treatment has the highest slope but other treatments which produced higher root lengths did not have a correlation as that of Coconut Water treatment.\n\n\n\n\nLet’s conduct a anova test on the data after filtering Control as it has not given any roots.\nLet H0 be Null hypothesis H1 be Alternate hypothesis\n\n# ANOVA Model\nmodel_anova &lt;- ths_data %&gt;%\n  filter(Treatment != \"Control\") %&gt;%\n  aov(num_roots_n ~ Treatment, data = .)\n\n# Summary\nmodel_anova %&gt;% summary()\n\n             Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nTreatment     2  256.0  128.00   27.43 2.51e-11 ***\nResiduals   213  993.9    4.67                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# TukeyHSD\nmodel_anova %&gt;% TukeyHSD(conf.level = 0.95)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = num_roots_n ~ Treatment, data = .)\n\n$Treatment\n                                              diff        lwr        upr\nIBA 1000ppm-Coconut water                 2.666667  1.8169277  3.5164057\nIBA 100ppm + Coconut water-Coconut water  1.347222  0.4974832  2.1969612\nIBA 100ppm + Coconut water-IBA 1000ppm   -1.319444 -2.1691835 -0.4697054\n                                             p adj\nIBA 1000ppm-Coconut water                0.0000000\nIBA 100ppm + Coconut water-Coconut water 0.0006848\nIBA 100ppm + Coconut water-IBA 1000ppm   0.0009086\n\n\nWe can see from p adj that all the treatments means are different and we can reject the H0 hypothesis. H1 holds at 95% confidence interval."
  },
  {
    "objectID": "posts/iris_ML/iris_ml_classification.html",
    "href": "posts/iris_ML/iris_ml_classification.html",
    "title": "Iris ML classification",
    "section": "",
    "text": "This is the “Iris” dataset. Originally published at UCI Machine Learning Repository: Iris Data Set, this small dataset from 1936 is often used for testing out machine learning algorithms and visualizations (for example, Scatter Plot). Each row of the table represents an iris flower, including its species and dimensions of its botanical parts, sepal and petal, in centimeters.\nIris data set is used widely as an example in field of data sciences and widely available to both python and R users.\n\n\n\nLet’s load the required packages\n\nlibrary(tidyverse)\nlibrary(patchwork)\n\n\n\n\n\niris_data &lt;- read_csv(\"F:/Data_Sci/Internship Projects/Iris_ML/Iris.csv\")\n\nRows: 150 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Species\ndbl (4): SepalLengthCm, SepalWidthCm, PetalLengthCm, PetalWidthCm\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\niris_data &lt;-  iris_data %&gt;% janitor::clean_names() %&gt;%\n  mutate(species = str_replace_all(species,\"Iris-\", \"\")) %&gt;% \n  mutate(species = as.factor(species))\n\nglimpse(iris_data)\n\nRows: 150\nColumns: 5\n$ sepal_length_cm &lt;dbl&gt; 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4,…\n$ sepal_width_cm  &lt;dbl&gt; 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7,…\n$ petal_length_cm &lt;dbl&gt; 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5,…\n$ petal_width_cm  &lt;dbl&gt; 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2,…\n$ species         &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setosa…\n\n\nWe have features like sepal_length_cm, sepal_width_cm, petal_length_cm and petal_width_cm and species necessary for the classification of the species.\nFirst, check for NA in the data.\n\niris_data %&gt;% map(~sum(is.na(.)))\n\n$sepal_length_cm\n[1] 0\n\n$sepal_width_cm\n[1] 0\n\n$petal_length_cm\n[1] 0\n\n$petal_width_cm\n[1] 0\n\n$species\n[1] 0\n\n\nThere no NA which is really good for the data.\n\n\n\nLet’s visualize the data with above parameters\n\nplot_iris &lt;- function(param){\n  iris_data %&gt;% ggplot(aes(species, {{param}})) + \n    geom_boxplot(aes(color = species)) +\n    theme(legend.position = \"none\")\n}\n\n(plot_iris(sepal_length_cm) +  plot_iris(petal_length_cm))/\n(plot_iris(sepal_width_cm) + plot_iris(petal_width_cm))\n\n\n\n\n\n\n\nFigure 1: Difference in observed parameters between species\n\n\n\n\n\nThere is quite a difference between the species in all parameters “setosa” &lt; “versicolor” &lt; “virginica” except for sepal_width_cm where “versicolor” &lt; “virginica” &lt; “setosa”\n\n\n\nLet’s start by loading the tidymodels package and splitting our data into training and testing sets.\n\nlibrary(tidymodels)\nset.seed(2024)\n\niris_split &lt;- initial_split(iris_data, prop = 0.8)\n\niris_train &lt;- training(iris_split)\niris_test &lt;- testing(iris_split)\n\nData is not large enough to build a model so creating resamples of the data to evaluate the model\n\nset.seed(2025)\n\niris_boot &lt;- bootstraps(iris_train, times = 5)\n\niris_boot\n\n# Bootstrap sampling \n# A tibble: 5 × 2\n  splits           id        \n  &lt;list&gt;           &lt;chr&gt;     \n1 &lt;split [120/45]&gt; Bootstrap1\n2 &lt;split [120/40]&gt; Bootstrap2\n3 &lt;split [120/38]&gt; Bootstrap3\n4 &lt;split [120/39]&gt; Bootstrap4\n5 &lt;split [120/46]&gt; Bootstrap5\n\n\nLet’s build 2 models and check which is better for the data.\n\n\n\n# random forest model\n\nrf_spec &lt;- rand_forest() %&gt;% \n  set_mode(\"classification\") %&gt;% \n  set_engine(\"ranger\")\n\nrf_spec\n\nRandom Forest Model Specification (classification)\n\nComputational engine: ranger \n\n\n\n\n\n\nknn_spec &lt;- nearest_neighbor(neighbors = 5) %&gt;% \n  set_mode(\"classification\") %&gt;% \n  set_engine(\"kknn\")\n\nknn_spec\n\nK-Nearest Neighbor Model Specification (classification)\n\nMain Arguments:\n  neighbors = 5\n\nComputational engine: kknn \n\n\n\n\n\nNext let’s start putting together a tidymodels workflow(), a helper object to help manage modeling pipelines with pieces that fit together like Lego blocks. Notice that there is no model yet: Model: None\n\niris_wf &lt;- workflow() %&gt;% \n  add_formula(species ~ .)\n\niris_wf\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Formula\nModel: None\n\n── Preprocessor ────────────────────────────────────────────────────────────────\nspecies ~ .\n\n\n\n\n\nNow we can add a model and fit the model to each of the resamples. First, we can fit the randomforest model\n\nrf_rs &lt;- iris_wf %&gt;% \n  add_model(rf_spec) %&gt;% \n  fit_resamples(\n    resamples = iris_boot,\n    control = control_resamples(save_pred = TRUE)\n  )\n\nrf_rs\n\n# Resampling results\n# Bootstrap sampling \n# A tibble: 5 × 5\n  splits           id         .metrics         .notes           .predictions\n  &lt;list&gt;           &lt;chr&gt;      &lt;list&gt;           &lt;list&gt;           &lt;list&gt;      \n1 &lt;split [120/45]&gt; Bootstrap1 &lt;tibble [3 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n2 &lt;split [120/40]&gt; Bootstrap2 &lt;tibble [3 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n3 &lt;split [120/38]&gt; Bootstrap3 &lt;tibble [3 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n4 &lt;split [120/39]&gt; Bootstrap4 &lt;tibble [3 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n5 &lt;split [120/46]&gt; Bootstrap5 &lt;tibble [3 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n\n\nNow we can add a model and fit the model to each of the resamples. First, we can fit the knn model\n\nknn_rs &lt;- iris_wf %&gt;% \n  add_model(knn_spec) %&gt;% \n  fit_resamples(\n    resamples = iris_boot,\n    control = control_resamples(save_pred = TRUE)\n  )\n\nknn_rs\n\n# Resampling results\n# Bootstrap sampling \n# A tibble: 5 × 5\n  splits           id         .metrics         .notes           .predictions\n  &lt;list&gt;           &lt;chr&gt;      &lt;list&gt;           &lt;list&gt;           &lt;list&gt;      \n1 &lt;split [120/45]&gt; Bootstrap1 &lt;tibble [3 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n2 &lt;split [120/40]&gt; Bootstrap2 &lt;tibble [3 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n3 &lt;split [120/38]&gt; Bootstrap3 &lt;tibble [3 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n4 &lt;split [120/39]&gt; Bootstrap4 &lt;tibble [3 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n5 &lt;split [120/46]&gt; Bootstrap5 &lt;tibble [3 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n\n\n\n\n\n\n\n\ncollect_metrics function collect the necessary parameters for evaluation\n\ncollect_metrics(rf_rs)\n\n# A tibble: 3 × 6\n  .metric     .estimator   mean     n  std_err .config             \n  &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    multiclass 0.947      5 0.0144   Preprocessor1_Model1\n2 brier_class multiclass 0.0332     5 0.0106   Preprocessor1_Model1\n3 roc_auc     hand_till  0.998      5 0.000846 Preprocessor1_Model1\n\ncollect_predictions(rf_rs) %&gt;% glimpse()\n\nRows: 208\nColumns: 8\n$ .pred_class      &lt;fct&gt; versicolor, virginica, virginica, setosa, versicolor,…\n$ .pred_setosa     &lt;dbl&gt; 0.0020, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.000…\n$ .pred_versicolor &lt;dbl&gt; 0.93323254, 0.00000000, 0.33701825, 0.00000000, 0.998…\n$ .pred_virginica  &lt;dbl&gt; 0.0647674603, 1.0000000000, 0.6629817460, 0.000000000…\n$ id               &lt;chr&gt; \"Bootstrap1\", \"Bootstrap1\", \"Bootstrap1\", \"Bootstrap1…\n$ .row             &lt;int&gt; 1, 6, 7, 9, 18, 19, 21, 22, 28, 30, 34, 35, 39, 40, 4…\n$ species          &lt;fct&gt; versicolor, virginica, virginica, setosa, versicolor,…\n$ .config          &lt;chr&gt; \"Preprocessor1_Model1\", \"Preprocessor1_Model1\", \"Prep…\n\n\n\n\n\n\ncollect_metrics(knn_rs)\n\n# A tibble: 3 × 6\n  .metric     .estimator   mean     n std_err .config             \n  &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    multiclass 0.922      5 0.00981 Preprocessor1_Model1\n2 brier_class multiclass 0.0660     5 0.00783 Preprocessor1_Model1\n3 roc_auc     hand_till  0.979      5 0.00687 Preprocessor1_Model1\n\ncollect_predictions(rf_rs) %&gt;% glimpse()\n\nRows: 208\nColumns: 8\n$ .pred_class      &lt;fct&gt; versicolor, virginica, virginica, setosa, versicolor,…\n$ .pred_setosa     &lt;dbl&gt; 0.0020, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.000…\n$ .pred_versicolor &lt;dbl&gt; 0.93323254, 0.00000000, 0.33701825, 0.00000000, 0.998…\n$ .pred_virginica  &lt;dbl&gt; 0.0647674603, 1.0000000000, 0.6629817460, 0.000000000…\n$ id               &lt;chr&gt; \"Bootstrap1\", \"Bootstrap1\", \"Bootstrap1\", \"Bootstrap1…\n$ .row             &lt;int&gt; 1, 6, 7, 9, 18, 19, 21, 22, 28, 30, 34, 35, 39, 40, 4…\n$ species          &lt;fct&gt; versicolor, virginica, virginica, setosa, versicolor,…\n$ .config          &lt;chr&gt; \"Preprocessor1_Model1\", \"Preprocessor1_Model1\", \"Prep…\n\n\nAs we can see that random_forest model has higher accuracy than knn model\nConfusion matrix lets us know how accurate the model is predicting the values\n\nrf_rs %&gt;% conf_mat_resampled()\n\n# A tibble: 9 × 3\n  Prediction Truth       Freq\n  &lt;fct&gt;      &lt;fct&gt;      &lt;dbl&gt;\n1 setosa     setosa      13.4\n2 setosa     versicolor   0  \n3 setosa     virginica    0  \n4 versicolor setosa       0  \n5 versicolor versicolor  13.2\n6 versicolor virginica    1.8\n7 virginica  setosa       0  \n8 virginica  versicolor   0.4\n9 virginica  virginica   12.8\n\n\nNow for the roc curve which shows us how accurate a model is for different species in the data.\n\nrf_rs %&gt;%\n  collect_predictions() %&gt;%\n  roc_curve(truth = species, .pred_setosa, .pred_versicolor,\n                    .pred_virginica) %&gt;%\n  ggplot(aes(1 - specificity, sensitivity, color = .level)) +\n  geom_abline(lty = 2, color = \"gray80\", linewidth = 1.5) +\n  geom_path(show.legend = TRUE, alpha = 0.6, linewidth = 1.2) +\n  coord_equal() + \n  labs(title = \"ROC Curve for Random Forest Classification\",\n       color = \"Class\")\n\n\n\n\nROC curve for Random Forest Model\n\n\n\n\nWhen we compare the same to the “KNN” model we can see the difference.\n\nknn_rs %&gt;%\n  collect_predictions() %&gt;%\n  roc_curve(truth = species, .pred_setosa, .pred_versicolor,\n                    .pred_virginica) %&gt;%\n  ggplot(aes(1 - specificity, sensitivity, color = .level)) +\n  geom_abline(lty = 2, color = \"gray80\", linewidth = 1.5) +\n  geom_path(show.legend = TRUE, alpha = 0.6, linewidth = 1.2) +\n  coord_equal() + \n  labs(title = \"ROC Curve for KNN Classification\",\n       color = \"Class\")\n\n\n\n\nROC curve for KNN Model\n\n\n\n\nThe “1 - Specificity” drops for the “KNN” model when compared to the “Random Forest” model, so we will use the “Random Forest” model to do predictions.\n\niris_final &lt;- iris_wf %&gt;%\n  add_model(rf_spec) %&gt;%\n  last_fit(iris_split)\n\niris_final %&gt;% collect_metrics()\n\n# A tibble: 3 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    multiclass    0.933  Preprocessor1_Model1\n2 roc_auc     hand_till     1      Preprocessor1_Model1\n3 brier_class multiclass    0.0534 Preprocessor1_Model1\n\n\n\n\n\nBased on the “iris_final” model we can predict the species based on the other parameters.\n\n# Create a new data frame for the measurements\nnew_data &lt;- tibble(\n  sepal_length_cm = 4.6,\n  sepal_width_cm = 3.8,\n  petal_length_cm = 1.4,\n  petal_width_cm = 0.2\n)\n\n# Extract the workflow from the last_fit result\nworkflow_fit &lt;- iris_final %&gt;% extract_workflow()\n\n# Make predictions using the new data\npredictions &lt;- predict(workflow_fit, new_data)\n\npredictions\n\n# A tibble: 1 × 1\n  .pred_class\n  &lt;fct&gt;      \n1 setosa"
  },
  {
    "objectID": "posts/iris_ML/iris_ml_classification.html#introduction",
    "href": "posts/iris_ML/iris_ml_classification.html#introduction",
    "title": "Iris ML classification",
    "section": "",
    "text": "This is the “Iris” dataset. Originally published at UCI Machine Learning Repository: Iris Data Set, this small dataset from 1936 is often used for testing out machine learning algorithms and visualizations (for example, Scatter Plot). Each row of the table represents an iris flower, including its species and dimensions of its botanical parts, sepal and petal, in centimeters.\nIris data set is used widely as an example in field of data sciences and widely available to both python and R users."
  },
  {
    "objectID": "posts/iris_ML/iris_ml_classification.html#importing-libraries",
    "href": "posts/iris_ML/iris_ml_classification.html#importing-libraries",
    "title": "Iris ML classification",
    "section": "",
    "text": "Let’s load the required packages\n\nlibrary(tidyverse)\nlibrary(patchwork)"
  },
  {
    "objectID": "posts/iris_ML/iris_ml_classification.html#importing-data",
    "href": "posts/iris_ML/iris_ml_classification.html#importing-data",
    "title": "Iris ML classification",
    "section": "",
    "text": "iris_data &lt;- read_csv(\"F:/Data_Sci/Internship Projects/Iris_ML/Iris.csv\")\n\nRows: 150 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Species\ndbl (4): SepalLengthCm, SepalWidthCm, PetalLengthCm, PetalWidthCm\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\niris_data &lt;-  iris_data %&gt;% janitor::clean_names() %&gt;%\n  mutate(species = str_replace_all(species,\"Iris-\", \"\")) %&gt;% \n  mutate(species = as.factor(species))\n\nglimpse(iris_data)\n\nRows: 150\nColumns: 5\n$ sepal_length_cm &lt;dbl&gt; 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4,…\n$ sepal_width_cm  &lt;dbl&gt; 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7,…\n$ petal_length_cm &lt;dbl&gt; 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5,…\n$ petal_width_cm  &lt;dbl&gt; 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2,…\n$ species         &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setosa…\n\n\nWe have features like sepal_length_cm, sepal_width_cm, petal_length_cm and petal_width_cm and species necessary for the classification of the species.\nFirst, check for NA in the data.\n\niris_data %&gt;% map(~sum(is.na(.)))\n\n$sepal_length_cm\n[1] 0\n\n$sepal_width_cm\n[1] 0\n\n$petal_length_cm\n[1] 0\n\n$petal_width_cm\n[1] 0\n\n$species\n[1] 0\n\n\nThere no NA which is really good for the data."
  },
  {
    "objectID": "posts/iris_ML/iris_ml_classification.html#analysisng-data",
    "href": "posts/iris_ML/iris_ml_classification.html#analysisng-data",
    "title": "Iris ML classification",
    "section": "",
    "text": "Let’s visualize the data with above parameters\n\nplot_iris &lt;- function(param){\n  iris_data %&gt;% ggplot(aes(species, {{param}})) + \n    geom_boxplot(aes(color = species)) +\n    theme(legend.position = \"none\")\n}\n\n(plot_iris(sepal_length_cm) +  plot_iris(petal_length_cm))/\n(plot_iris(sepal_width_cm) + plot_iris(petal_width_cm))\n\n\n\n\n\n\n\nFigure 1: Difference in observed parameters between species\n\n\n\n\n\nThere is quite a difference between the species in all parameters “setosa” &lt; “versicolor” &lt; “virginica” except for sepal_width_cm where “versicolor” &lt; “virginica” &lt; “setosa”"
  },
  {
    "objectID": "posts/iris_ML/iris_ml_classification.html#building-a-model",
    "href": "posts/iris_ML/iris_ml_classification.html#building-a-model",
    "title": "Iris ML classification",
    "section": "",
    "text": "Let’s start by loading the tidymodels package and splitting our data into training and testing sets.\n\nlibrary(tidymodels)\nset.seed(2024)\n\niris_split &lt;- initial_split(iris_data, prop = 0.8)\n\niris_train &lt;- training(iris_split)\niris_test &lt;- testing(iris_split)\n\nData is not large enough to build a model so creating resamples of the data to evaluate the model\n\nset.seed(2025)\n\niris_boot &lt;- bootstraps(iris_train, times = 5)\n\niris_boot\n\n# Bootstrap sampling \n# A tibble: 5 × 2\n  splits           id        \n  &lt;list&gt;           &lt;chr&gt;     \n1 &lt;split [120/45]&gt; Bootstrap1\n2 &lt;split [120/40]&gt; Bootstrap2\n3 &lt;split [120/38]&gt; Bootstrap3\n4 &lt;split [120/39]&gt; Bootstrap4\n5 &lt;split [120/46]&gt; Bootstrap5\n\n\nLet’s build 2 models and check which is better for the data.\n\n\n\n# random forest model\n\nrf_spec &lt;- rand_forest() %&gt;% \n  set_mode(\"classification\") %&gt;% \n  set_engine(\"ranger\")\n\nrf_spec\n\nRandom Forest Model Specification (classification)\n\nComputational engine: ranger \n\n\n\n\n\n\nknn_spec &lt;- nearest_neighbor(neighbors = 5) %&gt;% \n  set_mode(\"classification\") %&gt;% \n  set_engine(\"kknn\")\n\nknn_spec\n\nK-Nearest Neighbor Model Specification (classification)\n\nMain Arguments:\n  neighbors = 5\n\nComputational engine: kknn \n\n\n\n\n\nNext let’s start putting together a tidymodels workflow(), a helper object to help manage modeling pipelines with pieces that fit together like Lego blocks. Notice that there is no model yet: Model: None\n\niris_wf &lt;- workflow() %&gt;% \n  add_formula(species ~ .)\n\niris_wf\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Formula\nModel: None\n\n── Preprocessor ────────────────────────────────────────────────────────────────\nspecies ~ .\n\n\n\n\n\nNow we can add a model and fit the model to each of the resamples. First, we can fit the randomforest model\n\nrf_rs &lt;- iris_wf %&gt;% \n  add_model(rf_spec) %&gt;% \n  fit_resamples(\n    resamples = iris_boot,\n    control = control_resamples(save_pred = TRUE)\n  )\n\nrf_rs\n\n# Resampling results\n# Bootstrap sampling \n# A tibble: 5 × 5\n  splits           id         .metrics         .notes           .predictions\n  &lt;list&gt;           &lt;chr&gt;      &lt;list&gt;           &lt;list&gt;           &lt;list&gt;      \n1 &lt;split [120/45]&gt; Bootstrap1 &lt;tibble [3 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n2 &lt;split [120/40]&gt; Bootstrap2 &lt;tibble [3 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n3 &lt;split [120/38]&gt; Bootstrap3 &lt;tibble [3 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n4 &lt;split [120/39]&gt; Bootstrap4 &lt;tibble [3 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n5 &lt;split [120/46]&gt; Bootstrap5 &lt;tibble [3 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n\n\nNow we can add a model and fit the model to each of the resamples. First, we can fit the knn model\n\nknn_rs &lt;- iris_wf %&gt;% \n  add_model(knn_spec) %&gt;% \n  fit_resamples(\n    resamples = iris_boot,\n    control = control_resamples(save_pred = TRUE)\n  )\n\nknn_rs\n\n# Resampling results\n# Bootstrap sampling \n# A tibble: 5 × 5\n  splits           id         .metrics         .notes           .predictions\n  &lt;list&gt;           &lt;chr&gt;      &lt;list&gt;           &lt;list&gt;           &lt;list&gt;      \n1 &lt;split [120/45]&gt; Bootstrap1 &lt;tibble [3 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n2 &lt;split [120/40]&gt; Bootstrap2 &lt;tibble [3 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n3 &lt;split [120/38]&gt; Bootstrap3 &lt;tibble [3 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n4 &lt;split [120/39]&gt; Bootstrap4 &lt;tibble [3 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n5 &lt;split [120/46]&gt; Bootstrap5 &lt;tibble [3 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;"
  },
  {
    "objectID": "posts/iris_ML/iris_ml_classification.html#evaluating-the-model",
    "href": "posts/iris_ML/iris_ml_classification.html#evaluating-the-model",
    "title": "Iris ML classification",
    "section": "",
    "text": "collect_metrics function collect the necessary parameters for evaluation\n\ncollect_metrics(rf_rs)\n\n# A tibble: 3 × 6\n  .metric     .estimator   mean     n  std_err .config             \n  &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    multiclass 0.947      5 0.0144   Preprocessor1_Model1\n2 brier_class multiclass 0.0332     5 0.0106   Preprocessor1_Model1\n3 roc_auc     hand_till  0.998      5 0.000846 Preprocessor1_Model1\n\ncollect_predictions(rf_rs) %&gt;% glimpse()\n\nRows: 208\nColumns: 8\n$ .pred_class      &lt;fct&gt; versicolor, virginica, virginica, setosa, versicolor,…\n$ .pred_setosa     &lt;dbl&gt; 0.0020, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.000…\n$ .pred_versicolor &lt;dbl&gt; 0.93323254, 0.00000000, 0.33701825, 0.00000000, 0.998…\n$ .pred_virginica  &lt;dbl&gt; 0.0647674603, 1.0000000000, 0.6629817460, 0.000000000…\n$ id               &lt;chr&gt; \"Bootstrap1\", \"Bootstrap1\", \"Bootstrap1\", \"Bootstrap1…\n$ .row             &lt;int&gt; 1, 6, 7, 9, 18, 19, 21, 22, 28, 30, 34, 35, 39, 40, 4…\n$ species          &lt;fct&gt; versicolor, virginica, virginica, setosa, versicolor,…\n$ .config          &lt;chr&gt; \"Preprocessor1_Model1\", \"Preprocessor1_Model1\", \"Prep…\n\n\n\n\n\n\ncollect_metrics(knn_rs)\n\n# A tibble: 3 × 6\n  .metric     .estimator   mean     n std_err .config             \n  &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    multiclass 0.922      5 0.00981 Preprocessor1_Model1\n2 brier_class multiclass 0.0660     5 0.00783 Preprocessor1_Model1\n3 roc_auc     hand_till  0.979      5 0.00687 Preprocessor1_Model1\n\ncollect_predictions(rf_rs) %&gt;% glimpse()\n\nRows: 208\nColumns: 8\n$ .pred_class      &lt;fct&gt; versicolor, virginica, virginica, setosa, versicolor,…\n$ .pred_setosa     &lt;dbl&gt; 0.0020, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.000…\n$ .pred_versicolor &lt;dbl&gt; 0.93323254, 0.00000000, 0.33701825, 0.00000000, 0.998…\n$ .pred_virginica  &lt;dbl&gt; 0.0647674603, 1.0000000000, 0.6629817460, 0.000000000…\n$ id               &lt;chr&gt; \"Bootstrap1\", \"Bootstrap1\", \"Bootstrap1\", \"Bootstrap1…\n$ .row             &lt;int&gt; 1, 6, 7, 9, 18, 19, 21, 22, 28, 30, 34, 35, 39, 40, 4…\n$ species          &lt;fct&gt; versicolor, virginica, virginica, setosa, versicolor,…\n$ .config          &lt;chr&gt; \"Preprocessor1_Model1\", \"Preprocessor1_Model1\", \"Prep…\n\n\nAs we can see that random_forest model has higher accuracy than knn model\nConfusion matrix lets us know how accurate the model is predicting the values\n\nrf_rs %&gt;% conf_mat_resampled()\n\n# A tibble: 9 × 3\n  Prediction Truth       Freq\n  &lt;fct&gt;      &lt;fct&gt;      &lt;dbl&gt;\n1 setosa     setosa      13.4\n2 setosa     versicolor   0  \n3 setosa     virginica    0  \n4 versicolor setosa       0  \n5 versicolor versicolor  13.2\n6 versicolor virginica    1.8\n7 virginica  setosa       0  \n8 virginica  versicolor   0.4\n9 virginica  virginica   12.8\n\n\nNow for the roc curve which shows us how accurate a model is for different species in the data.\n\nrf_rs %&gt;%\n  collect_predictions() %&gt;%\n  roc_curve(truth = species, .pred_setosa, .pred_versicolor,\n                    .pred_virginica) %&gt;%\n  ggplot(aes(1 - specificity, sensitivity, color = .level)) +\n  geom_abline(lty = 2, color = \"gray80\", linewidth = 1.5) +\n  geom_path(show.legend = TRUE, alpha = 0.6, linewidth = 1.2) +\n  coord_equal() + \n  labs(title = \"ROC Curve for Random Forest Classification\",\n       color = \"Class\")\n\n\n\n\nROC curve for Random Forest Model\n\n\n\n\nWhen we compare the same to the “KNN” model we can see the difference.\n\nknn_rs %&gt;%\n  collect_predictions() %&gt;%\n  roc_curve(truth = species, .pred_setosa, .pred_versicolor,\n                    .pred_virginica) %&gt;%\n  ggplot(aes(1 - specificity, sensitivity, color = .level)) +\n  geom_abline(lty = 2, color = \"gray80\", linewidth = 1.5) +\n  geom_path(show.legend = TRUE, alpha = 0.6, linewidth = 1.2) +\n  coord_equal() + \n  labs(title = \"ROC Curve for KNN Classification\",\n       color = \"Class\")\n\n\n\n\nROC curve for KNN Model\n\n\n\n\nThe “1 - Specificity” drops for the “KNN” model when compared to the “Random Forest” model, so we will use the “Random Forest” model to do predictions.\n\niris_final &lt;- iris_wf %&gt;%\n  add_model(rf_spec) %&gt;%\n  last_fit(iris_split)\n\niris_final %&gt;% collect_metrics()\n\n# A tibble: 3 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    multiclass    0.933  Preprocessor1_Model1\n2 roc_auc     hand_till     1      Preprocessor1_Model1\n3 brier_class multiclass    0.0534 Preprocessor1_Model1\n\n\n\n\n\nBased on the “iris_final” model we can predict the species based on the other parameters.\n\n# Create a new data frame for the measurements\nnew_data &lt;- tibble(\n  sepal_length_cm = 4.6,\n  sepal_width_cm = 3.8,\n  petal_length_cm = 1.4,\n  petal_width_cm = 0.2\n)\n\n# Extract the workflow from the last_fit result\nworkflow_fit &lt;- iris_final %&gt;% extract_workflow()\n\n# Make predictions using the new data\npredictions &lt;- predict(workflow_fit, new_data)\n\npredictions\n\n# A tibble: 1 × 1\n  .pred_class\n  &lt;fct&gt;      \n1 setosa"
  },
  {
    "objectID": "posts/palmer_penguins_ml/palmerpenguins_ml.html",
    "href": "posts/palmer_penguins_ml/palmerpenguins_ml.html",
    "title": "Predict the sex of the Penguin Species",
    "section": "",
    "text": "Building a model to predict the sex of three species of penguins of Palmer Penguins data."
  },
  {
    "objectID": "posts/palmer_penguins_ml/palmerpenguins_ml.html#exploring-the-data",
    "href": "posts/palmer_penguins_ml/palmerpenguins_ml.html#exploring-the-data",
    "title": "Predict the sex of the Penguin Species",
    "section": "1 Exploring the data",
    "text": "1 Exploring the data\n\nlibrary(tidyverse)\n\nlibrary(palmerpenguins)\n\npenguins\n\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nThe data set is from palmerpenguins library which contains observations of Antarctic pebguins from the Palmer Archipelago. You can read more about how this dataset came to be in this post on the RStudio Education blog. Our modeling goal here is to predict the sex of the penguins using a classification model, based on other observations in the dataset.\nIt is easier to classify and predict species than the sex of the species as the different physical characteristics are what makes a species different from each other. But sex somewhat harder to predict.\n\npenguins %&gt;% filter(!is.na(sex)) %&gt;% \n  ggplot(aes(flipper_length_mm, bill_length_mm, color = sex,\n             size = body_mass_g)) +\n  geom_point(alpha = 0.5) +\n  facet_wrap(~species) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFrom the above graph it looks like female penguins have smaller with differet bills. Now let’s build a model but first remove year and island from the model.\n\npenguins_df &lt;- penguins %&gt;% filter(!is.na(sex)) %&gt;% select(-year, -island)\n\npenguins_df\n\n# A tibble: 333 × 6\n   species bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n   &lt;fct&gt;            &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt; \n 1 Adelie            39.1          18.7               181        3750 male  \n 2 Adelie            39.5          17.4               186        3800 female\n 3 Adelie            40.3          18                 195        3250 female\n 4 Adelie            36.7          19.3               193        3450 female\n 5 Adelie            39.3          20.6               190        3650 male  \n 6 Adelie            38.9          17.8               181        3625 female\n 7 Adelie            39.2          19.6               195        4675 male  \n 8 Adelie            41.1          17.6               182        3200 female\n 9 Adelie            38.6          21.2               191        3800 male  \n10 Adelie            34.6          21.1               198        4400 male  \n# ℹ 323 more rows"
  },
  {
    "objectID": "posts/palmer_penguins_ml/palmerpenguins_ml.html#building-a-model",
    "href": "posts/palmer_penguins_ml/palmerpenguins_ml.html#building-a-model",
    "title": "Predict the sex of the Penguin Species",
    "section": "2 Building a Model",
    "text": "2 Building a Model\nLet’s start by loading the tidymodels package and splitting our data into training and testing sets.\n\nlibrary(tidymodels)\nset.seed(123)\n\npenguin_split &lt;- initial_split(penguins_df, strata = sex)\n\npenguins_train &lt;- training(penguin_split)\npenguins_test &lt;- testing(penguin_split)\n\nAs data for building a model is not that large, let’s create resamples of training data to evaluate the model.\n\nset.seed(123)\npenguin_boot &lt;- bootstraps(penguins_train)\n\npenguin_boot\n\n# Bootstrap sampling \n# A tibble: 25 × 2\n   splits           id         \n   &lt;list&gt;           &lt;chr&gt;      \n 1 &lt;split [249/93]&gt; Bootstrap01\n 2 &lt;split [249/91]&gt; Bootstrap02\n 3 &lt;split [249/90]&gt; Bootstrap03\n 4 &lt;split [249/91]&gt; Bootstrap04\n 5 &lt;split [249/85]&gt; Bootstrap05\n 6 &lt;split [249/87]&gt; Bootstrap06\n 7 &lt;split [249/94]&gt; Bootstrap07\n 8 &lt;split [249/88]&gt; Bootstrap08\n 9 &lt;split [249/95]&gt; Bootstrap09\n10 &lt;split [249/89]&gt; Bootstrap10\n# ℹ 15 more rows\n\n\nLet’s build and compare two different models, a logistic regression model and a random forest model.\n\n# logistic regression model\nglm_spec &lt;- logistic_reg() %&gt;%\n  set_engine(\"glm\")\n\nglm_spec\n\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm \n\n# random forest model\n\nrf_spec &lt;- rand_forest() %&gt;%\n  set_mode(\"classification\") %&gt;% \n  set_engine(\"ranger\")\n\nrf_spec\n\nRandom Forest Model Specification (classification)\n\nComputational engine: ranger \n\n\nNext let’s start putting together a tidymodels workflow(), a helper object to help manage modeling pipelines with pieces that fit together like Lego blocks. Notice that there is no model yet: Model: None.\n\npenguin_wf &lt;- workflow() %&gt;% \n  add_formula(sex ~ .)\n\npenguin_wf\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Formula\nModel: None\n\n── Preprocessor ────────────────────────────────────────────────────────────────\nsex ~ .\n\n\nNow we can add a model and fit the model to each of the resamples. First, we can fit the logistic regression model\n\nglm_rs &lt;- penguin_wf %&gt;%\n  add_model(glm_spec) %&gt;%\n  fit_resamples(\n    resamples = penguin_boot,\n    control = control_resamples(save_pred = TRUE)\n  )\n\n→ A | warning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\n\nThere were issues with some computations   A: x1\n\n\nThere were issues with some computations   A: x3\nThere were issues with some computations   A: x3\n\n\n\n\nglm_rs\n\n# Resampling results\n# Bootstrap sampling \n# A tibble: 25 × 5\n   splits           id          .metrics         .notes           .predictions\n   &lt;list&gt;           &lt;chr&gt;       &lt;list&gt;           &lt;list&gt;           &lt;list&gt;      \n 1 &lt;split [249/93]&gt; Bootstrap01 &lt;tibble [3 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 2 &lt;split [249/91]&gt; Bootstrap02 &lt;tibble [3 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 3 &lt;split [249/90]&gt; Bootstrap03 &lt;tibble [3 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 4 &lt;split [249/91]&gt; Bootstrap04 &lt;tibble [3 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 5 &lt;split [249/85]&gt; Bootstrap05 &lt;tibble [3 × 4]&gt; &lt;tibble [1 × 3]&gt; &lt;tibble&gt;    \n 6 &lt;split [249/87]&gt; Bootstrap06 &lt;tibble [3 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 7 &lt;split [249/94]&gt; Bootstrap07 &lt;tibble [3 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 8 &lt;split [249/88]&gt; Bootstrap08 &lt;tibble [3 × 4]&gt; &lt;tibble [1 × 3]&gt; &lt;tibble&gt;    \n 9 &lt;split [249/95]&gt; Bootstrap09 &lt;tibble [3 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n10 &lt;split [249/89]&gt; Bootstrap10 &lt;tibble [3 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n# ℹ 15 more rows\n\nThere were issues with some computations:\n\n  - Warning(s) x3: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nRun `show_notes(.Last.tune.result)` for more information.\n\n\nSecond, we can fit the random forest model.\n\nrf_rs &lt;- penguin_wf %&gt;%\n  add_model(rf_spec) %&gt;%\n  fit_resamples(\n    resamples = penguin_boot,\n    control = control_resamples(save_pred = TRUE)\n  )\n\nrf_rs\n\n# Resampling results\n# Bootstrap sampling \n# A tibble: 25 × 5\n   splits           id          .metrics         .notes           .predictions\n   &lt;list&gt;           &lt;chr&gt;       &lt;list&gt;           &lt;list&gt;           &lt;list&gt;      \n 1 &lt;split [249/93]&gt; Bootstrap01 &lt;tibble [3 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 2 &lt;split [249/91]&gt; Bootstrap02 &lt;tibble [3 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 3 &lt;split [249/90]&gt; Bootstrap03 &lt;tibble [3 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 4 &lt;split [249/91]&gt; Bootstrap04 &lt;tibble [3 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 5 &lt;split [249/85]&gt; Bootstrap05 &lt;tibble [3 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 6 &lt;split [249/87]&gt; Bootstrap06 &lt;tibble [3 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 7 &lt;split [249/94]&gt; Bootstrap07 &lt;tibble [3 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 8 &lt;split [249/88]&gt; Bootstrap08 &lt;tibble [3 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 9 &lt;split [249/95]&gt; Bootstrap09 &lt;tibble [3 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n10 &lt;split [249/89]&gt; Bootstrap10 &lt;tibble [3 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n# ℹ 15 more rows\n\n\nWe have fit each of our candidate models to our resampled training set!"
  },
  {
    "objectID": "posts/palmer_penguins_ml/palmerpenguins_ml.html#evaluate-model",
    "href": "posts/palmer_penguins_ml/palmerpenguins_ml.html#evaluate-model",
    "title": "Predict the sex of the Penguin Species",
    "section": "3 Evaluate Model",
    "text": "3 Evaluate Model\nNow let’s check the results and how well they performed.\n\ncollect_metrics(glm_rs)\n\n# A tibble: 3 × 6\n  .metric     .estimator   mean     n std_err .config             \n  &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary     0.918     25 0.00639 Preprocessor1_Model1\n2 brier_class binary     0.0585    25 0.00424 Preprocessor1_Model1\n3 roc_auc     binary     0.979     25 0.00254 Preprocessor1_Model1\n\ncollect_notes(glm_rs)\n\n# A tibble: 3 × 4\n  id          location                    type    note                          \n  &lt;chr&gt;       &lt;chr&gt;                       &lt;chr&gt;   &lt;chr&gt;                         \n1 Bootstrap05 preprocessor 1/1, model 1/1 warning glm.fit: fitted probabilities…\n2 Bootstrap08 preprocessor 1/1, model 1/1 warning glm.fit: fitted probabilities…\n3 Bootstrap23 preprocessor 1/1, model 1/1 warning glm.fit: fitted probabilities…\n\n\nPretty nice! The function collect_metrics() extracts and formats the .metrics column from resampling results like the ones we have here.\n\ncollect_metrics(rf_rs)\n\n# A tibble: 3 × 6\n  .metric     .estimator   mean     n std_err .config             \n  &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary     0.912     25 0.00547 Preprocessor1_Model1\n2 brier_class binary     0.0664    25 0.00240 Preprocessor1_Model1\n3 roc_auc     binary     0.977     25 0.00202 Preprocessor1_Model1\n\n\nLet’s choose logistic regression model as it is a simpler model than random forest.\nLet’s check the confusion matrix for accuracy\n\nglm_rs %&gt;% conf_mat_resampled()\n\n# A tibble: 4 × 3\n  Prediction Truth   Freq\n  &lt;fct&gt;      &lt;fct&gt;  &lt;dbl&gt;\n1 female     female  41.1\n2 female     male     3  \n3 male       female   4.4\n4 male       male    42.3\n\n\nNow for the roc curve which shows us how accurate a model is.\n\nglm_rs %&gt;%\n  collect_predictions() %&gt;%\n  group_by(id) %&gt;%\n  roc_curve(sex, .pred_female) %&gt;%\n  ggplot(aes(1 - specificity, sensitivity, color = id)) +\n  geom_abline(lty = 2, color = \"gray80\", size = 1.5) +\n  geom_path(show.legend = FALSE, alpha = 0.6, linewidth = 1.2) +\n  coord_equal()\n\n\n\n\n\n\n\n\nIt is finally time for us to return to the testing set. Notice that we have not used the testing set yet during this whole analysis; the testing set is precious and can only be used to estimate performance on new data. Let’s fit one more time to the training data and evaluate on the testing data using the function last_fit().\n\npenguin_final &lt;- penguin_wf %&gt;%\n  add_model(glm_spec) %&gt;%\n  last_fit(penguin_split)\n\npenguin_final\n\n# Resampling results\n# Manual resampling \n# A tibble: 1 × 6\n  splits           id               .metrics .notes   .predictions .workflow \n  &lt;list&gt;           &lt;chr&gt;            &lt;list&gt;   &lt;list&gt;   &lt;list&gt;       &lt;list&gt;    \n1 &lt;split [249/84]&gt; train/test split &lt;tibble&gt; &lt;tibble&gt; &lt;tibble&gt;     &lt;workflow&gt;\n\n\nThe metrics and predictions here are on the testing data.\n\ncollect_metrics(penguin_final)\n\n# A tibble: 3 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary         0.857 Preprocessor1_Model1\n2 roc_auc     binary         0.938 Preprocessor1_Model1\n3 brier_class binary         0.101 Preprocessor1_Model1\n\ncollect_predictions(penguin_final) %&gt;%\n  conf_mat(sex, .pred_class)\n\n          Truth\nPrediction female male\n    female     37    7\n    male        5   35\n\n\n\npenguin_final$.workflow[[1]] %&gt;%\n  tidy(exponentiate = TRUE)\n\n# A tibble: 7 × 5\n  term              estimate std.error statistic     p.value\n  &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n1 (Intercept)       5.75e-46  19.6        -5.31  0.000000110\n2 speciesChinstrap  1.37e- 4   2.34       -3.79  0.000148   \n3 speciesGentoo     1.14e- 5   3.75       -3.03  0.00243    \n4 bill_length_mm    1.91e+ 0   0.180       3.60  0.000321   \n5 bill_depth_mm     8.36e+ 0   0.478       4.45  0.00000868 \n6 flipper_length_mm 1.06e+ 0   0.0611      0.926 0.355      \n7 body_mass_g       1.01e+ 0   0.00176     4.59  0.00000442 \n\n\n\nThe largest odds ratio is for bill depth, with the second largest for bill length. An increase of 1 mm in bill depth corresponds to almost 4x higher odds of being male. The characteristics of a penguin’s bill must be associated with their sex.\nWe don’t have strong evidence that flipper length is different between male and female penguins, controlling for the other measures; maybe we should explore that by changing that first plot!\n\n\npenguins %&gt;% filter(!is.na(sex)) %&gt;% \n  ggplot(aes(bill_depth_mm, bill_length_mm, color = sex)) +\n  geom_point(alpha = 0.5) +\n  facet_wrap(~species) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThis graph shows much more separation between male and female penguins."
  }
]