[
  {
    "objectID": "posts/palmer_penguins_ml/palmerpenguins_ml.html",
    "href": "posts/palmer_penguins_ml/palmerpenguins_ml.html",
    "title": "Palmer Penguins",
    "section": "",
    "text": "Building a model to predict the sex of three species of penguins of Palmer Penguins data."
  },
  {
    "objectID": "posts/palmer_penguins_ml/palmerpenguins_ml.html#exploring-the-data",
    "href": "posts/palmer_penguins_ml/palmerpenguins_ml.html#exploring-the-data",
    "title": "Palmer Penguins",
    "section": "Exploring the data",
    "text": "Exploring the data\n\nlibrary(tidyverse)\n\nlibrary(palmerpenguins)\n\npenguins\n\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nThe data set is from palmerpenguins library which contains observations of Antarctic pebguins from the Palmer Archipelago. You can read more about how this dataset came to be in this post on the RStudio Education blog. Our modeling goal here is to predict the sex of the penguins using a classification model, based on other observations in the dataset.\nIt is easier to classify and predict species than the sex of the species as the different physical characteristics are what makes a species different from each other. But sex somewhat harder to predict.\n\npenguins %&gt;% filter(!is.na(sex)) %&gt;% \n  ggplot(aes(flipper_length_mm, bill_length_mm, color = sex,\n             size = body_mass_g)) +\n  geom_point(alpha = 0.5) +\n  facet_wrap(~species) +\n  theme_minimal()\n\n\n\n\nFrom the above graph it looks like female penguins have smaller with differet bills. Now let’s build a model but first remove year and island from the model.\n\npenguins_df &lt;- penguins %&gt;% filter(!is.na(sex)) %&gt;% select(-year, -island)\n\npenguins_df\n\n# A tibble: 333 × 6\n   species bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n   &lt;fct&gt;            &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt; \n 1 Adelie            39.1          18.7               181        3750 male  \n 2 Adelie            39.5          17.4               186        3800 female\n 3 Adelie            40.3          18                 195        3250 female\n 4 Adelie            36.7          19.3               193        3450 female\n 5 Adelie            39.3          20.6               190        3650 male  \n 6 Adelie            38.9          17.8               181        3625 female\n 7 Adelie            39.2          19.6               195        4675 male  \n 8 Adelie            41.1          17.6               182        3200 female\n 9 Adelie            38.6          21.2               191        3800 male  \n10 Adelie            34.6          21.1               198        4400 male  \n# ℹ 323 more rows"
  },
  {
    "objectID": "posts/palmer_penguins_ml/palmerpenguins_ml.html#building-a-model",
    "href": "posts/palmer_penguins_ml/palmerpenguins_ml.html#building-a-model",
    "title": "Palmer Penguins",
    "section": "BUilding a Model",
    "text": "BUilding a Model\nLet’s start by loading the tidymodels package and splitting our data into training and testing sets.\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.1.1 ──\n\n\n✔ broom        1.0.5     ✔ rsample      1.2.0\n✔ dials        1.2.0     ✔ tune         1.1.2\n✔ infer        1.0.5     ✔ workflows    1.1.3\n✔ modeldata    1.2.0     ✔ workflowsets 1.0.1\n✔ parsnip      1.1.1     ✔ yardstick    1.2.0\n✔ recipes      1.0.8     \n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Use suppressPackageStartupMessages() to eliminate package startup messages\n\nset.seed(123)\n\npenguin_split &lt;- initial_split(penguins_df, strata = sex)\n\npenguins_train &lt;- training(penguin_split)\npenguins_test &lt;- testing(penguin_split)\n\nAs data for building a model is not that large, let’s create resamples of training data to evaluate the model.\n\nset.seed(123)\npenguin_boot &lt;- bootstraps(penguins_train)\n\npenguin_boot\n\n# Bootstrap sampling \n# A tibble: 25 × 2\n   splits           id         \n   &lt;list&gt;           &lt;chr&gt;      \n 1 &lt;split [249/93]&gt; Bootstrap01\n 2 &lt;split [249/91]&gt; Bootstrap02\n 3 &lt;split [249/90]&gt; Bootstrap03\n 4 &lt;split [249/91]&gt; Bootstrap04\n 5 &lt;split [249/85]&gt; Bootstrap05\n 6 &lt;split [249/87]&gt; Bootstrap06\n 7 &lt;split [249/94]&gt; Bootstrap07\n 8 &lt;split [249/88]&gt; Bootstrap08\n 9 &lt;split [249/95]&gt; Bootstrap09\n10 &lt;split [249/89]&gt; Bootstrap10\n# ℹ 15 more rows\n\n\nLet’s build and compare two different models, a logistic regression model and a random forest model.\n\n# logistic regression model\nglm_spec &lt;- logistic_reg() %&gt;%\n  set_engine(\"glm\")\n\nglm_spec\n\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm \n\n# random forest model\n\nrf_spec &lt;- rand_forest() %&gt;%\n  set_mode(\"classification\") %&gt;% \n  set_engine(\"ranger\")\n\nrf_spec\n\nRandom Forest Model Specification (classification)\n\nComputational engine: ranger \n\n\nNext let’s start putting together a tidymodels workflow(), a helper object to help manage modeling pipelines with pieces that fit together like Lego blocks. Notice that there is no model yet: Model: None.\n\npenguin_wf &lt;- workflow() %&gt;% \n  add_formula(sex ~ .)\n\npenguin_wf\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Formula\nModel: None\n\n── Preprocessor ────────────────────────────────────────────────────────────────\nsex ~ .\n\n\nNow we canadd a model and fit the model to each of the resamples. First, we can fit the logistic regression model\n\nglm_rs &lt;- penguin_wf %&gt;%\n  add_model(glm_spec) %&gt;%\n  fit_resamples(\n    resamples = penguin_boot,\n    control = control_resamples(save_pred = TRUE)\n  )\n\n→ A | warning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\n\nThere were issues with some computations   A: x1\n\n\nThere were issues with some computations   A: x3\n\n\n\n\nglm_rs\n\n# Resampling results\n# Bootstrap sampling \n# A tibble: 25 × 5\n   splits           id          .metrics         .notes           .predictions\n   &lt;list&gt;           &lt;chr&gt;       &lt;list&gt;           &lt;list&gt;           &lt;list&gt;      \n 1 &lt;split [249/93]&gt; Bootstrap01 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 2 &lt;split [249/91]&gt; Bootstrap02 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 3 &lt;split [249/90]&gt; Bootstrap03 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 4 &lt;split [249/91]&gt; Bootstrap04 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 5 &lt;split [249/85]&gt; Bootstrap05 &lt;tibble [2 × 4]&gt; &lt;tibble [1 × 3]&gt; &lt;tibble&gt;    \n 6 &lt;split [249/87]&gt; Bootstrap06 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 7 &lt;split [249/94]&gt; Bootstrap07 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 8 &lt;split [249/88]&gt; Bootstrap08 &lt;tibble [2 × 4]&gt; &lt;tibble [1 × 3]&gt; &lt;tibble&gt;    \n 9 &lt;split [249/95]&gt; Bootstrap09 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n10 &lt;split [249/89]&gt; Bootstrap10 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n# ℹ 15 more rows\n\nThere were issues with some computations:\n\n  - Warning(s) x3: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nRun `show_notes(.Last.tune.result)` for more information.\n\n\nSecond, we can fit the random forest model.\n\nrf_rs &lt;- penguin_wf %&gt;%\n  add_model(rf_spec) %&gt;%\n  fit_resamples(\n    resamples = penguin_boot,\n    control = control_resamples(save_pred = TRUE)\n  )\n\nrf_rs\n\n# Resampling results\n# Bootstrap sampling \n# A tibble: 25 × 5\n   splits           id          .metrics         .notes           .predictions\n   &lt;list&gt;           &lt;chr&gt;       &lt;list&gt;           &lt;list&gt;           &lt;list&gt;      \n 1 &lt;split [249/93]&gt; Bootstrap01 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 2 &lt;split [249/91]&gt; Bootstrap02 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 3 &lt;split [249/90]&gt; Bootstrap03 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 4 &lt;split [249/91]&gt; Bootstrap04 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 5 &lt;split [249/85]&gt; Bootstrap05 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 6 &lt;split [249/87]&gt; Bootstrap06 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 7 &lt;split [249/94]&gt; Bootstrap07 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 8 &lt;split [249/88]&gt; Bootstrap08 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 9 &lt;split [249/95]&gt; Bootstrap09 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n10 &lt;split [249/89]&gt; Bootstrap10 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n# ℹ 15 more rows\n\n\nWe have fit each of our candidate models to our resampled training set!"
  },
  {
    "objectID": "posts/palmer_penguins_ml/palmerpenguins_ml.html#evaluate-model",
    "href": "posts/palmer_penguins_ml/palmerpenguins_ml.html#evaluate-model",
    "title": "Palmer Penguins",
    "section": "Evaluate Model",
    "text": "Evaluate Model\nNow let’s check the results and how well they performed.\n\ncollect_metrics(glm_rs)\n\n# A tibble: 2 × 6\n  .metric  .estimator  mean     n std_err .config             \n  &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary     0.918    25 0.00639 Preprocessor1_Model1\n2 roc_auc  binary     0.979    25 0.00254 Preprocessor1_Model1\n\ncollect_notes(glm_rs)\n\n# A tibble: 3 × 4\n  id          location                    type    note                          \n  &lt;chr&gt;       &lt;chr&gt;                       &lt;chr&gt;   &lt;chr&gt;                         \n1 Bootstrap05 preprocessor 1/1, model 1/1 warning glm.fit: fitted probabilities…\n2 Bootstrap08 preprocessor 1/1, model 1/1 warning glm.fit: fitted probabilities…\n3 Bootstrap23 preprocessor 1/1, model 1/1 warning glm.fit: fitted probabilities…\n\n\nPretty nice! The function collect_metrics() extracts and formats the .metrics column from resampling results like the ones we have here.\n\ncollect_metrics(rf_rs)\n\n# A tibble: 2 × 6\n  .metric  .estimator  mean     n std_err .config             \n  &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary     0.912    25 0.00547 Preprocessor1_Model1\n2 roc_auc  binary     0.977    25 0.00202 Preprocessor1_Model1\n\n\nLet’s choose logistic model as it is simpler.\n\nglm_rs %&gt;% conf_mat_resampled()\n\n# A tibble: 4 × 3\n  Prediction Truth   Freq\n  &lt;fct&gt;      &lt;fct&gt;  &lt;dbl&gt;\n1 female     female  41.1\n2 female     male     3  \n3 male       female   4.4\n4 male       male    42.3\n\n\nAbout the same, which is good. We can also make an ROC curve.\n\nglm_rs %&gt;%\n  collect_predictions() %&gt;%\n  group_by(id) %&gt;%\n  roc_curve(sex, .pred_female) %&gt;%\n  ggplot(aes(1 - specificity, sensitivity, color = id)) +\n  geom_abline(lty = 2, color = \"gray80\", size = 1.5) +\n  geom_path(show.legend = FALSE, alpha = 0.6, size = 1.2) +\n  coord_equal()\n\n\n\n\nIt is finally time for us to return to the testing set. Notice that we have not used the testing set yet during this whole analysis; the testing set is precious and can only be used to estimate performance on new data. Let’s fit one more time to the training data and evaluate on the testing data using the function last_fit().\n\npenguin_final &lt;- penguin_wf %&gt;%\n  add_model(glm_spec) %&gt;%\n  last_fit(penguin_split)\n\npenguin_final\n\n# Resampling results\n# Manual resampling \n# A tibble: 1 × 6\n  splits           id               .metrics .notes   .predictions .workflow \n  &lt;list&gt;           &lt;chr&gt;            &lt;list&gt;   &lt;list&gt;   &lt;list&gt;       &lt;list&gt;    \n1 &lt;split [249/84]&gt; train/test split &lt;tibble&gt; &lt;tibble&gt; &lt;tibble&gt;     &lt;workflow&gt;\n\n\nThe metrics and predictions here are on the testing data.\n\ncollect_metrics(penguin_final)\n\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary         0.857 Preprocessor1_Model1\n2 roc_auc  binary         0.938 Preprocessor1_Model1\n\ncollect_predictions(penguin_final) %&gt;%\n  conf_mat(sex, .pred_class)\n\n          Truth\nPrediction female male\n    female     37    7\n    male        5   35\n\n\n\npenguin_final$.workflow[[1]] %&gt;%\n  tidy(exponentiate = TRUE)\n\n# A tibble: 7 × 5\n  term              estimate std.error statistic     p.value\n  &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n1 (Intercept)       5.75e-46  19.6        -5.31  0.000000110\n2 speciesChinstrap  1.37e- 4   2.34       -3.79  0.000148   \n3 speciesGentoo     1.14e- 5   3.75       -3.03  0.00243    \n4 bill_length_mm    1.91e+ 0   0.180       3.60  0.000321   \n5 bill_depth_mm     8.36e+ 0   0.478       4.45  0.00000868 \n6 flipper_length_mm 1.06e+ 0   0.0611      0.926 0.355      \n7 body_mass_g       1.01e+ 0   0.00176     4.59  0.00000442 \n\n\n\nThe largest odds ratio is for bill depth, with the second largest for bill length. An increase of 1 mm in bill depth corresponds to almost 4x higher odds of being male. The characteristics of a penguin’s bill must be associated with their sex.\nWe don’t have strong evidence that flipper length is different between male and female penguins, controlling for the other measures; maybe we should explore that by changing that first plot!\n\n\npenguins %&gt;% filter(!is.na(sex)) %&gt;% \n  ggplot(aes(bill_depth_mm, bill_length_mm, color = sex,\n             size = body_mass_g)) +\n  geom_point(alpha = 0.5) +\n  facet_wrap(~species) +\n  theme_minimal()\n\n\n\n\nThis graph shows much more separation between male and female penguins."
  },
  {
    "objectID": "posts/cyclist_trip_analysis/divy_tripdata_202207_202306.html",
    "href": "posts/cyclist_trip_analysis/divy_tripdata_202207_202306.html",
    "title": "CYCLIST BIKE SHARE",
    "section": "",
    "text": "The analysis is done on Cyclist Trip Data obtained from Coursera Google Data Analytics course as part of Cap Stone Project.\nThe data contains month wise travel usage of bikes from the year of 2015-2023. We will be concentrating on data gathered in between July-2022 to June-2023 which will cover an entire year.\nLet’s load the required packages first\n\nLoading tidyverse and gt packages\n\n\nlibrary(tidyverse)\nlibrary(gt)\n\n\n\n\nLet’s look at the structure of the data in one of the downloaded .csv files.\n\n\ntrpdata_july_2022&lt;-read_csv(\"F:/Data_Sci/Cap_Stone_Project/Cyclist_trip_data/202207-divvy-tripdata/202207-divvy-tripdata.csv\")\n\nRows: 823488 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (7): ride_id, rideable_type, start_station_name, start_station_id, end_...\ndbl  (4): start_lat, start_lng, end_lat, end_lng\ndttm (2): started_at, ended_at\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nglimpse(trpdata_july_2022)\n\nRows: 823,488\nColumns: 13\n$ ride_id            &lt;chr&gt; \"954144C2F67B1932\", \"292E027607D218B6\", \"5776585258…\n$ rideable_type      &lt;chr&gt; \"classic_bike\", \"classic_bike\", \"classic_bike\", \"cl…\n$ started_at         &lt;dttm&gt; 2022-07-05 08:12:47, 2022-07-26 12:53:38, 2022-07-…\n$ ended_at           &lt;dttm&gt; 2022-07-05 08:24:32, 2022-07-26 12:55:31, 2022-07-…\n$ start_station_name &lt;chr&gt; \"Ashland Ave & Blackhawk St\", \"Buckingham Fountain …\n$ start_station_id   &lt;chr&gt; \"13224\", \"15541\", \"15541\", \"15541\", \"TA1307000117\",…\n$ end_station_name   &lt;chr&gt; \"Kingsbury St & Kinzie St\", \"Michigan Ave & 8th St\"…\n$ end_station_id     &lt;chr&gt; \"KA1503000043\", \"623\", \"623\", \"TA1307000164\", \"TA13…\n$ start_lat          &lt;dbl&gt; 41.90707, 41.86962, 41.86962, 41.86962, 41.89147, 4…\n$ start_lng          &lt;dbl&gt; -87.66725, -87.62398, -87.62398, -87.62398, -87.626…\n$ end_lat            &lt;dbl&gt; 41.88918, 41.87277, 41.87277, 41.79526, 41.93625, 4…\n$ end_lng            &lt;dbl&gt; -87.63851, -87.62398, -87.62398, -87.59647, -87.652…\n$ member_casual      &lt;chr&gt; \"member\", \"casual\", \"casual\", \"casual\", \"member\", \"…\n\n\n\nLet’s look at the columns and try to understand what they represent\n\nride_id is the unique identification token generated for each ride that was initiated.\nrideable_type indicates the type of bike used for the ride.\nstarted_at and ended_at give us the time when the ride began and the ride ended respectively.\nstart_station_name and end_station_name give us the names of stations where ride began and ended respectively.\nstart_station_id and end_station_id are unique ID’s given to stations.\nstart_lat and start_lng represent co-ordinates where the ride began.\nend_lat and end_lng represent co-ordinates where the ride stopped.\nmember_casual identifies if the rider is a member or casual rider of the bike.\n\n\nThe trpdata_july_2022 contains 823488 rows and 13 columns. In the results we can see all the columns and their data types.\n\nLets load data of remaining 11 months.\n\n\ntrpdata_aug_2022 &lt;- read_csv(\"F:/Data_Sci/Cap_Stone_Project/Cyclist_trip_data/202208-divvy-tripdata/202208-divvy-tripdata.csv\")\n\ntrpdata_sept_2022&lt;- read_csv(\"F:/Data_Sci/Cap_Stone_Project/Cyclist_trip_data/202209-divvy-tripdata/202209-divvy-publictripdata.csv\")\n\ntrpdata_oct_2022&lt;- read_csv(\"F:/Data_Sci/Cap_Stone_Project/Cyclist_trip_data/202210-divvy-tripdata/202210-divvy-tripdata_raw.csv\")\n\ntrpdata_nov_2022&lt;- read_csv(\"F:/Data_Sci/Cap_Stone_Project/Cyclist_trip_data/202211-divvy-tripdata/202211-divvy-tripdata.csv\")\n\ntrpdata_dec_2022 &lt;- read_csv(\"F:/Data_Sci/Cap_Stone_Project/Cyclist_trip_data/202212-divvy-tripdata/202212-divvy-tripdata.csv\")\n\ntrpdata_jan_2023 &lt;- read_csv(\"F:/Data_Sci/Cap_Stone_Project/Cyclist_trip_data/202301-divvy-tripdata/202301-divvy-tripdata.csv\")\n\ntrpdata_feb_2023 &lt;- read_csv(\"F:/Data_Sci/Cap_Stone_Project/Cyclist_trip_data/202302-divvy-tripdata/202302-divvy-tripdata.csv\")\n\ntrpdata_mar_2023 &lt;- read_csv(\"F:/Data_Sci/Cap_Stone_Project/Cyclist_trip_data/202303-divvy-tripdata/202303-divvy-tripdata.csv\")\n\ntrpdata_apr_2023 &lt;- read_csv(\"F:/Data_Sci/Cap_Stone_Project/Cyclist_trip_data/202304-divvy-tripdata/202304-divvy-tripdata.csv\")\n\ntrpdata_may_2023 &lt;- read_csv(\"F:/Data_Sci/Cap_Stone_Project/Cyclist_trip_data/202305-divvy-tripdata/202305-divvy-tripdata.csv\")\n\ntrpdata_june_2023 &lt;- read_csv(\"F:/Data_Sci/Cap_Stone_Project/Cyclist_trip_data/202306-divvy-tripdata/202306-divvy-tripdata.csv\")\n\nAs structure of .csv’s is same across the all the files lets combine all the .csv files into a single data frame which contains data of all 12 months.\n\nCombining all the monthly data to one previous year data(data_one_year).\n\n\ndata_one_year &lt;- rbind(trpdata_july_2022, trpdata_aug_2022,\n                     trpdata_sept_2022, trpdata_oct_2022,\n                     trpdata_nov_2022, trpdata_dec_2022,\n                     trpdata_jan_2023, trpdata_feb_2023,\n                     trpdata_mar_2023, trpdata_apr_2023,\n                     trpdata_may_2023, trpdata_june_2023)\n\nglimpse(data_one_year)\n\nRows: 5,779,444\nColumns: 13\n$ ride_id            &lt;chr&gt; \"954144C2F67B1932\", \"292E027607D218B6\", \"5776585258…\n$ rideable_type      &lt;chr&gt; \"classic_bike\", \"classic_bike\", \"classic_bike\", \"cl…\n$ started_at         &lt;dttm&gt; 2022-07-05 08:12:47, 2022-07-26 12:53:38, 2022-07-…\n$ ended_at           &lt;dttm&gt; 2022-07-05 08:24:32, 2022-07-26 12:55:31, 2022-07-…\n$ start_station_name &lt;chr&gt; \"Ashland Ave & Blackhawk St\", \"Buckingham Fountain …\n$ start_station_id   &lt;chr&gt; \"13224\", \"15541\", \"15541\", \"15541\", \"TA1307000117\",…\n$ end_station_name   &lt;chr&gt; \"Kingsbury St & Kinzie St\", \"Michigan Ave & 8th St\"…\n$ end_station_id     &lt;chr&gt; \"KA1503000043\", \"623\", \"623\", \"TA1307000164\", \"TA13…\n$ start_lat          &lt;dbl&gt; 41.90707, 41.86962, 41.86962, 41.86962, 41.89147, 4…\n$ start_lng          &lt;dbl&gt; -87.66725, -87.62398, -87.62398, -87.62398, -87.626…\n$ end_lat            &lt;dbl&gt; 41.88918, 41.87277, 41.87277, 41.79526, 41.93625, 4…\n$ end_lng            &lt;dbl&gt; -87.63851, -87.62398, -87.62398, -87.59647, -87.652…\n$ member_casual      &lt;chr&gt; \"member\", \"casual\", \"casual\", \"casual\", \"member\", \"…\n\n\n\ndata_one_year data frame contains data from July-2022 to June-2023.\n\n\n\n\n\nChecking and counting “NA” in each column of the data frame. Data is much better without “NA” as they can cause problems while aggregating data and calculating averages and sums.\n\n\nna_in_cols &lt;- data_one_year %&gt;% map(is.na) %&gt;% map(sum) %&gt;% unlist()\n\nna_in_cols\n\n           ride_id      rideable_type         started_at           ended_at \n                 0                  0                  0                  0 \nstart_station_name   start_station_id   end_station_name     end_station_id \n            857860             857992             915655             915796 \n         start_lat          start_lng            end_lat            end_lng \n                 0                  0               5795               5795 \n     member_casual \n                 0 \n\n\n\nAs NA’s are not present in the times columns i.e, started_at and ended_at we don’t need to worry ourselves about writing na.rm during aggregation and manipulation of data.\nFinding the length of rides by making a new column ride_length in minutes and making sure that the ride_length is not negative by using if_else function. Eliminating stations where station names and longitude and latitude co-ordinates are not present.\n\n\n# As we remove all the NA's it is better to save data in a variable.\ndata_one_year_raw &lt;- data_one_year %&gt;% \n  mutate(ride_length = difftime(ended_at, started_at,\n                                units = \"min\")) %&gt;%\n  mutate(ride_length = as.numeric(ride_length))\n\ndata_one_year &lt;- data_one_year_raw %&gt;% \n  mutate(ride_length = if_else(ride_length &lt; 0, 0, ride_length)) %&gt;% \n  filter(start_station_name != \"\" & end_station_name != \"\" & \n         !is.na(start_lat) & !is.na(start_lng) &\n         !is.na(end_lat) & !is.na(end_lng)) %&gt;% arrange(ride_length)\n\n\nglimpse(data_one_year)\n\nRows: 4,409,335\nColumns: 14\n$ ride_id            &lt;chr&gt; \"86CD09DA24761714\", \"27024CD08288BD45\", \"029D853B5C…\n$ rideable_type      &lt;chr&gt; \"electric_bike\", \"electric_bike\", \"classic_bike\", \"…\n$ started_at         &lt;dttm&gt; 2022-07-20 16:21:48, 2022-07-30 23:42:46, 2022-07-…\n$ ended_at           &lt;dttm&gt; 2022-07-20 16:21:48, 2022-07-30 23:42:46, 2022-07-…\n$ start_station_name &lt;chr&gt; \"Racine Ave & Fullerton Ave\", \"Albany Ave & 26th St…\n$ start_station_id   &lt;chr&gt; \"TA1306000026\", \"15691\", \"chargingstx5\", \"chargings…\n$ end_station_name   &lt;chr&gt; \"Racine Ave & Fullerton Ave\", \"Albany Ave & 26th St…\n$ end_station_id     &lt;chr&gt; \"TA1306000026\", \"15691\", \"chargingstx5\", \"chargings…\n$ start_lat          &lt;dbl&gt; 41.92556, 41.84452, 41.94335, 41.94335, 41.94335, 4…\n$ start_lng          &lt;dbl&gt; -87.65859, -87.70209, -87.67067, -87.67067, -87.670…\n$ end_lat            &lt;dbl&gt; 41.92556, 41.84448, 41.94335, 41.94335, 41.94335, 4…\n$ end_lng            &lt;dbl&gt; -87.65840, -87.70201, -87.67067, -87.67067, -87.670…\n$ member_casual      &lt;chr&gt; \"member\", \"casual\", \"member\", \"member\", \"casual\", \"…\n$ ride_length        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n\n\n\n\n\n\n\n\n\nAggregating data to see “Average minutes per ride” grouped by “bike type” and “rider type” after removing rides less than 2 minutes (As rides less than 2 minutes tend to have the same start and stop stations.).\n\n\ndata_one_year_aggregate &lt;- data_one_year %&gt;% \n  select(ride_id, rideable_type, member_casual, started_at, ended_at,\n         ride_length, everything()) %&gt;%\n  filter(ride_length &gt;= 2) %&gt;% \n  summarise(\"Number of Rides\" = n(),\n            \"Ride Length\" = sum(ride_length, na.rm = TRUE),\n            \"Max Ride Length\" = round(max(ride_length), 2),\n            \"Avg Ride Length in Minutes\" = round(mean(ride_length), 2),\n            .by = c(member_casual, rideable_type)) %&gt;% \n  arrange(desc(\"Avg Ride Length in Minutes\")) %&gt;% \n  gt() %&gt;% tab_header(title = \"Average length of Rides\") %&gt;% \n  cols_label(member_casual = \"Rider type\",\n             rideable_type = \"Bike type\")\n\ndata_one_year_aggregate\n\n\n\n\n\nTable 1:  Average minutes per ride \n  \n    \n      Average length of Rides\n    \n    \n    \n      Rider type\n      Bike type\n      Number of Rides\n      Ride Length\n      Max Ride Length\n      Avg Ride Length in Minutes\n    \n  \n  \n    member\nclassic_bike\n1630991\n21996488\n1497.87\n13.49\n    casual\nclassic_bike\n781530\n19383358\n1497.75\n24.80\n    casual\nelectric_bike\n709649\n11372659\n479.98\n16.03\n    member\nelectric_bike\n984688\n10968684\n480.00\n11.14\n    casual\ndocked_bike\n136794\n6899998\n32035.45\n50.44\n  \n  \n  \n\n\n\n\n\nWe can clearly notice in Table 1 that member riders have more number of rides with both classic and electric bikes while the average ride length is higher with casual riders.\n\nCalculating and visualizing Average ride length by “Rider type”.\n\n\naverage_ride_by_rideable_type &lt;- data_one_year %&gt;%\n  rename(\"Rider type\" = member_casual, \"Bike type\" = rideable_type) %&gt;% \n  summarise(ride_length = sum(ride_length, na.rm = TRUE),\n            ride_count = n(),\n            avg_ride_length = ride_length/ride_count,\n            .by = c(`Rider type`, `Bike type`)) %&gt;% \n  ggplot(aes(`Rider type`, avg_ride_length)) + \n  geom_col(aes(fill = `Bike type`), position = \"dodge\") + \n  labs(x = \"Bike type\", y = \"Avg Length of Ride(Minutes)\",\n       title  = \"Average ride length by Bike type\") +\n  theme_minimal() +\n  theme(plot.title = element_text(size = 18),\n        legend.position = \"bottom\")\n\naverage_ride_by_rideable_type\n\n\n\n\nFigure 1: Average Ride Length by Rider type and Member type\n\n\n\n\nThe above Figure 1 clearly shows that members average ride lengths between bike types doesn’t differ much for member riders but differs with casual riders upto 8 minutes.\n\nFurther down in the analysis “docked_bike” type is dropped as no proper documentation is available in the course.\n\n\n\n\n\nCalculating and visualizing ride patterns in a week for number of rides.\n\n\nrideable_order &lt;- c(\"classic_bike\", \"electric_bike\", \"docked_bike\")\n\nrides_on_days &lt;- data_one_year %&gt;%\n  filter(rideable_type != \"docked_bike\") %&gt;%\n  mutate(month = month(started_at, label = TRUE, abbr = FALSE)) %&gt;% \n  mutate(rideable_type = factor(rideable_type, levels = rideable_order)) %&gt;% ggplot(aes(wday(started_at, label = TRUE, abbr = FALSE))) + \n  geom_bar(aes(fill = member_casual), position = \"dodge\") +\n  facet_wrap(~month, nrow = 3) + \n  labs(x = \"Day of the Week\", y = \"Number of rides\",\n       title = \"Riding pattrens on Weekdays of each Month\",\n       subtitle = \"From July-2022 to June-2023\",\n       fill = \"Type of Rider\") +\n  theme_light() +\n  theme(legend.position = \"top\",\n        axis.text.x = element_text(angle = 45, hjust = 1),\n        plot.title = element_text(size = 18))\n\nrides_on_days\n\n\n\n\nFigure 2: Riding pattrens in Weekdays of each Month\n\n\n\n\nThe above Figure 2 clearly shows how the number of rides change due to seasons. In winters the number of rides decrease very drastically may be because of temperature and snow. In Summers the number of rides are at its peak.\nThe number of rides driven by member riders are increases through the week especially in working week days but for casual riders the rides increase in the weekends. The Figure 2 shows number of rides on Saturdays and Sundays by casual members overtake membership riders in the months of July and August.\n\nComparing variation in ride lengths of average and total ride lengths by bike type.\n\nAggregating data for the visualization.\n\nrides_on_days &lt;- data_one_year %&gt;%\n  mutate(day = wday(started_at, label = TRUE, abbr = FALSE),\n         month = month(started_at, label = TRUE, abbr = FALSE)) %&gt;% \n  summarise(ride_count = n(),\n            sum_ride_length = sum(ride_length, na.rm = TRUE),\n            avg_ride_length = mean(ride_length, na.rm = TRUE),\n            .by = c(month, day, member_casual))\n\nrides_on_days\n\n# A tibble: 168 × 6\n   month day       member_casual ride_count sum_ride_length avg_ride_length\n   &lt;ord&gt; &lt;ord&gt;     &lt;chr&gt;              &lt;int&gt;           &lt;dbl&gt;           &lt;dbl&gt;\n 1 July  Wednesday member             47725         605175.            12.7\n 2 July  Saturday  casual             74543        2057158.            27.6\n 3 July  Tuesday   member             46360         588327.            12.7\n 4 July  Tuesday   casual             31415         705946.            22.5\n 5 July  Saturday  member             53796         817724.            15.2\n 6 July  Friday    casual             42333         960160             22.7\n 7 July  Thursday  casual             35800         759804.            21.2\n 8 July  Sunday    casual             61198        1715527.            28.0\n 9 July  Thursday  member             48572         623503.            12.8\n10 July  Friday    member             48221         616243.            12.8\n# ℹ 158 more rows\n\n\nLet’s visualize the aggregated data\n\nrides_on_days_len &lt;- rides_on_days %&gt;%\n  ggplot(aes(day, sum_ride_length))+\n  geom_col(aes(fill = member_casual), position = \"dodge\")+\n  facet_wrap(~month, ncol = 3)+\n  labs(x = \"Day of the Week\", y = \"Total Length of Rides (Minutes)\",\n       title = \"Total Minutes driven by Riders\",\n       fill = \"Type of Rider\") +\n  theme(legend.position = \"top\",\n        axis.text.x = element_text(angle = 45, hjust = 1),\n        plot.title = element_text(size = 18))\n\nrides_on_days_len\n\n\n\n\nFigure 3: Total Ride lengths through out the year by member types.\n\n\n\n\n\nrides_on_days_len_avg &lt;- rides_on_days %&gt;%\n  ggplot(aes(day, avg_ride_length))+\n  geom_col(aes(fill = member_casual), position = \"dodge\")+\n  facet_wrap(~month, ncol = 3) +\n  labs(x = \"Day of the Week\", y = \"Average Length of Rides (Minutes)\",\n       title = \"Average Minutes driven by Riders\",\n       fill = \"Type of Rider\") +\n  theme(legend.position = \"top\",\n        axis.text.x = element_text(angle = 45, hjust = 1),\n        plot.title = element_text(size = 18))\n\nrides_on_days_len_avg\n\n\n\n\nFigure 4: Average Ride lengths through out year by member types.\n\n\n\n\nThe ride length is varying across months and seasons just as number of rides but average ride length is not fluctuating that much across the year.\n\n\n\n\nRemoving “NA” and blanks from the stations columns.\n\n\ndata_one_year &lt;- data_one_year %&gt;%\n  drop_na(start_station_name, end_station_name ) %&gt;% \n  filter(start_station_name != \"\" & end_station_name != \"\",\n         started_at != ended_at) \n\nglimpse(data_one_year)\n\nRows: 4,409,072\nColumns: 14\n$ ride_id            &lt;chr&gt; \"029D853B5C38426E\", \"C1D6D749139CB6C0\", \"D3E7C0B68E…\n$ rideable_type      &lt;chr&gt; \"classic_bike\", \"classic_bike\", \"classic_bike\", \"cl…\n$ started_at         &lt;dttm&gt; 2022-07-26 20:07:33, 2022-07-26 20:08:04, 2022-07-…\n$ ended_at           &lt;dttm&gt; 2022-07-26 19:59:34, 2022-07-26 19:59:34, 2022-07-…\n$ start_station_name &lt;chr&gt; \"Lincoln Ave & Roscoe St*\", \"Lincoln Ave & Roscoe S…\n$ start_station_id   &lt;chr&gt; \"chargingstx5\", \"chargingstx5\", \"chargingstx5\", \"ch…\n$ end_station_name   &lt;chr&gt; \"Lincoln Ave & Roscoe St*\", \"Lincoln Ave & Roscoe S…\n$ end_station_id     &lt;chr&gt; \"chargingstx5\", \"chargingstx5\", \"chargingstx5\", \"ch…\n$ start_lat          &lt;dbl&gt; 41.94335, 41.94335, 41.94335, 41.94335, 41.93945, 4…\n$ start_lng          &lt;dbl&gt; -87.67067, -87.67067, -87.67067, -87.67067, -87.663…\n$ end_lat            &lt;dbl&gt; 41.94335, 41.94335, 41.94335, 41.94335, 41.93948, 4…\n$ end_lng            &lt;dbl&gt; -87.67067, -87.67067, -87.67067, -87.67067, -87.663…\n$ member_casual      &lt;chr&gt; \"member\", \"member\", \"casual\", \"casual\", \"member\", \"…\n$ ride_length        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n\n\n\nMaking a new column to identify travelled routes.\n\n\ndata_one_year &lt;- data_one_year %&gt;% \n  mutate(stations_travelled = paste(start_station_name, \n                                     \"-\", end_station_name))\n\nglimpse(data_one_year)\n\nRows: 4,409,072\nColumns: 15\n$ ride_id            &lt;chr&gt; \"029D853B5C38426E\", \"C1D6D749139CB6C0\", \"D3E7C0B68E…\n$ rideable_type      &lt;chr&gt; \"classic_bike\", \"classic_bike\", \"classic_bike\", \"cl…\n$ started_at         &lt;dttm&gt; 2022-07-26 20:07:33, 2022-07-26 20:08:04, 2022-07-…\n$ ended_at           &lt;dttm&gt; 2022-07-26 19:59:34, 2022-07-26 19:59:34, 2022-07-…\n$ start_station_name &lt;chr&gt; \"Lincoln Ave & Roscoe St*\", \"Lincoln Ave & Roscoe S…\n$ start_station_id   &lt;chr&gt; \"chargingstx5\", \"chargingstx5\", \"chargingstx5\", \"ch…\n$ end_station_name   &lt;chr&gt; \"Lincoln Ave & Roscoe St*\", \"Lincoln Ave & Roscoe S…\n$ end_station_id     &lt;chr&gt; \"chargingstx5\", \"chargingstx5\", \"chargingstx5\", \"ch…\n$ start_lat          &lt;dbl&gt; 41.94335, 41.94335, 41.94335, 41.94335, 41.93945, 4…\n$ start_lng          &lt;dbl&gt; -87.67067, -87.67067, -87.67067, -87.67067, -87.663…\n$ end_lat            &lt;dbl&gt; 41.94335, 41.94335, 41.94335, 41.94335, 41.93948, 4…\n$ end_lng            &lt;dbl&gt; -87.67067, -87.67067, -87.67067, -87.67067, -87.663…\n$ member_casual      &lt;chr&gt; \"member\", \"member\", \"casual\", \"casual\", \"member\", \"…\n$ ride_length        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ stations_travelled &lt;chr&gt; \"Lincoln Ave & Roscoe St* - Lincoln Ave & Roscoe St…\n\n\n\nFinding which route is most traveled by casual riders.\n\n\nmost_travelled_routes_casual &lt;- data_one_year %&gt;%\n  filter(member_casual == \"casual\") %&gt;% \n  summarise(ride_count = n(),\n            avg_ride_length = round(mean(ride_length), 2),\n            .by = c(stations_travelled)) %&gt;%\n  arrange(desc(ride_count))\n\nhead(most_travelled_routes_casual)\n\n# A tibble: 6 × 3\n  stations_travelled                                  ride_count avg_ride_length\n  &lt;chr&gt;                                                    &lt;int&gt;           &lt;dbl&gt;\n1 Streeter Dr & Grand Ave - Streeter Dr & Grand Ave         9698            39.6\n2 DuSable Lake Shore Dr & Monroe St - DuSable Lake S…       6584            33.4\n3 DuSable Lake Shore Dr & Monroe St - Streeter Dr & …       4840            27.1\n4 Michigan Ave & Oak St - Michigan Ave & Oak St             4292            44.6\n5 Millennium Park - Millennium Park                         3884            37.4\n6 Montrose Harbor - Montrose Harbor                         2711            48.3\n\nNROW(most_travelled_routes_casual)\n\n[1] 130660\n\n\nStreeter Dr & Grand Ave - Streeter Dr & Grand Ave stands to be the most popular station with 9698 rides by casual riders.\n\nmost_travelled_routes_member &lt;- data_one_year  %&gt;%\n  filter(member_casual == \"member\") %&gt;% \n  summarise(ride_count = n(),\n            total_ride_length = sum(ride_length),\n            ride_length = round(mean(ride_length), 2),\n            .by = stations_travelled) %&gt;% arrange(desc(ride_count))\n\nhead(most_travelled_routes_member)\n\n# A tibble: 6 × 4\n  stations_travelled                    ride_count total_ride_length ride_length\n  &lt;chr&gt;                                      &lt;int&gt;             &lt;dbl&gt;       &lt;dbl&gt;\n1 Ellis Ave & 60th St - University Ave…       6153            25936.        4.22\n2 University Ave & 57th St - Ellis Ave…       5786            26634.        4.6 \n3 Ellis Ave & 60th St - Ellis Ave & 55…       5676            28427.        5.01\n4 Ellis Ave & 55th St - Ellis Ave & 60…       5347            27187.        5.08\n5 State St & 33rd St - Calumet Ave & 3…       4156            18014.        4.33\n6 Calumet Ave & 33rd St - State St & 3…       4027            15887.        3.95\n\nNROW(most_travelled_routes_member)\n\n[1] 145104\n\n\nEllis Ave & 60th St - University Ave & 57th St stands as the most traveled route by member riders with 6153 rides per anum.\n\nFinding which station has most ride starting points and which station has most ending points.\n\n\nmost_starting_points &lt;- data_one_year %&gt;% \n  summarise(ride_count = n(),\n            .by = start_station_name) %&gt;%\n  select(start_station_name, ride_count) %&gt;%\n  slice_max(ride_count, n = 10)\n\nmost_starting_points\n\n# A tibble: 10 × 2\n   start_station_name                 ride_count\n   &lt;chr&gt;                                   &lt;int&gt;\n 1 Streeter Dr & Grand Ave                 65892\n 2 DuSable Lake Shore Dr & Monroe St       37939\n 3 Michigan Ave & Oak St                   36036\n 4 DuSable Lake Shore Dr & North Blvd      35091\n 5 Wells St & Concord Ln                   33250\n 6 Clark St & Elm St                       32751\n 7 Kingsbury St & Kinzie St                31876\n 8 Millennium Park                         30917\n 9 Theater on the Lake                     29600\n10 Wells St & Elm St                       28063\n\nmost_starting_points$ride_count %&gt;% sum()\n\n[1] 361415\n\nmost_ending_points &lt;- data_one_year %&gt;% \n  summarise(ride_count = n(),\n            .by = end_station_name) %&gt;%\n  select(end_station_name, ride_count)  %&gt;% \n  slice_max(ride_count, n = 10)\n\nmost_ending_points\n\n# A tibble: 10 × 2\n   end_station_name                   ride_count\n   &lt;chr&gt;                                   &lt;int&gt;\n 1 Streeter Dr & Grand Ave                 67536\n 2 DuSable Lake Shore Dr & North Blvd      38026\n 3 Michigan Ave & Oak St                   36976\n 4 DuSable Lake Shore Dr & Monroe St       36806\n 5 Wells St & Concord Ln                   33814\n 6 Clark St & Elm St                       32325\n 7 Millennium Park                         32046\n 8 Kingsbury St & Kinzie St                31058\n 9 Theater on the Lake                     30214\n10 Wells St & Elm St                       28212\n\nmost_ending_points$ride_count %&gt;% sum()\n\n[1] 367013\n\n\nStreeter Dr & Grand Ave found to be the most popular station as most rides start and end at that station.\n\n\n\nJust because we filtered the data with NA’s that does not mean that the data is not helpful, it just means that it does not our fulfill specific need when calculating or manipulating data.\nLet’s look at NA’s in the data once again.\n\nna_in_cols &lt;- data_one_year_raw %&gt;% map( ~sum(is.na(.))) %&gt;% unlist()\n\nna_in_cols\n\n           ride_id      rideable_type         started_at           ended_at \n                 0                  0                  0                  0 \nstart_station_name   start_station_id   end_station_name     end_station_id \n            857860             857992             915655             915796 \n         start_lat          start_lng            end_lat            end_lng \n                 0                  0               5795               5795 \n     member_casual        ride_length \n                 0                  0 \n\n\n\nWe can see that the start_station_name and end_station_name have majority of NA’s it means that rides are starting and ending where stations are not there.\n\n\nprop_na &lt;- na_in_cols[\"start_station_name\"]/nrow(data_one_year_raw)\n\nprop_na\n\nstart_station_name \n          0.148433 \n\n\n\n14.8432963% of data in start_station_name is missing and good thing is that none of the start_lng and start_lat have any NA’s and we can use this for find the most traveled routes.\n\n\ndata_na_one_year &lt;- data_one_year_raw %&gt;% \n  filter(is.na(start_station_name) | start_station_name == \"\") %&gt;% \n  drop_na(end_lat, end_lng)\n  \nglimpse(data_na_one_year)\n\nRows: 857,860\nColumns: 14\n$ ride_id            &lt;chr&gt; \"DCB3D2C9B63999EC\", \"D1ACA8280DA02AE3\", \"EF98673429…\n$ rideable_type      &lt;chr&gt; \"electric_bike\", \"electric_bike\", \"electric_bike\", …\n$ started_at         &lt;dttm&gt; 2022-07-04 15:04:26, 2022-07-12 14:43:51, 2022-07-…\n$ ended_at           &lt;dttm&gt; 2022-07-04 15:32:38, 2022-07-12 14:49:28, 2022-07-…\n$ start_station_name &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ start_station_id   &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ end_station_name   &lt;chr&gt; \"Ashland Ave & Blackhawk St\", \"Cornell Ave & Hyde P…\n$ end_station_id     &lt;chr&gt; \"13224\", \"KA1503000007\", \"KA1503000007\", \"847\", \"48…\n$ start_lat          &lt;dbl&gt; 41.95, 41.80, 41.80, 41.74, 42.02, 41.95, 41.95, 41…\n$ start_lng          &lt;dbl&gt; -87.64, -87.59, -87.59, -87.55, -87.69, -87.67, -87…\n$ end_lat            &lt;dbl&gt; 41.90707, 41.80241, 41.80241, 41.73000, 42.01000, 4…\n$ end_lng            &lt;dbl&gt; -87.66725, -87.58692, -87.58692, -87.55000, -87.690…\n$ member_casual      &lt;chr&gt; \"member\", \"member\", \"member\", \"member\", \"member\", \"…\n$ ride_length        &lt;dbl&gt; 28.200000, 5.616667, 2.600000, 20.350000, 3.700000,…\n\n\n\nNow let’s make new columns start_point with start_lng and start_lat and end_point with end_lat and end_lng.\n\n\ndata_na_one_year &lt;- data_na_one_year %&gt;%\n    mutate(start_point = paste(start_lat, start_lng),\n           end_point = paste(end_lat, end_lng))\n\nglimpse(data_na_one_year)\n\nRows: 857,860\nColumns: 16\n$ ride_id            &lt;chr&gt; \"DCB3D2C9B63999EC\", \"D1ACA8280DA02AE3\", \"EF98673429…\n$ rideable_type      &lt;chr&gt; \"electric_bike\", \"electric_bike\", \"electric_bike\", …\n$ started_at         &lt;dttm&gt; 2022-07-04 15:04:26, 2022-07-12 14:43:51, 2022-07-…\n$ ended_at           &lt;dttm&gt; 2022-07-04 15:32:38, 2022-07-12 14:49:28, 2022-07-…\n$ start_station_name &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ start_station_id   &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ end_station_name   &lt;chr&gt; \"Ashland Ave & Blackhawk St\", \"Cornell Ave & Hyde P…\n$ end_station_id     &lt;chr&gt; \"13224\", \"KA1503000007\", \"KA1503000007\", \"847\", \"48…\n$ start_lat          &lt;dbl&gt; 41.95, 41.80, 41.80, 41.74, 42.02, 41.95, 41.95, 41…\n$ start_lng          &lt;dbl&gt; -87.64, -87.59, -87.59, -87.55, -87.69, -87.67, -87…\n$ end_lat            &lt;dbl&gt; 41.90707, 41.80241, 41.80241, 41.73000, 42.01000, 4…\n$ end_lng            &lt;dbl&gt; -87.66725, -87.58692, -87.58692, -87.55000, -87.690…\n$ member_casual      &lt;chr&gt; \"member\", \"member\", \"member\", \"member\", \"member\", \"…\n$ ride_length        &lt;dbl&gt; 28.200000, 5.616667, 2.600000, 20.350000, 3.700000,…\n$ start_point        &lt;chr&gt; \"41.95 -87.64\", \"41.8 -87.59\", \"41.8 -87.59\", \"41.7…\n$ end_point          &lt;chr&gt; \"41.907066 -87.667252\", \"41.802406 -87.586924\", \"41…\n\n\n\nAggregating data to check for the most traveled routes without a start_station name.\n\nFirst join start_point and end_point to make route_travelled then count the rides by routes_travelled to see the most traveled path.\n\nmost_travelled_na_routes &lt;- data_na_one_year %&gt;%\n  filter(start_point != end_point) %&gt;% \n  mutate(route_travelled = paste(start_point, \",\", end_point)) %&gt;% \n  summarise(ride_count = n(),\n            .by = route_travelled) %&gt;%\n  slice_max(ride_count, n=10)\n\nmost_travelled_na_routes\n\n# A tibble: 10 × 2\n   route_travelled                             ride_count\n   &lt;chr&gt;                                            &lt;int&gt;\n 1 41.79 -87.6 , 41.8 -87.59                         1459\n 2 41.79 -87.59 , 41.79 -87.6                        1354\n 3 41.8 -87.59 , 41.79 -87.6                         1335\n 4 41.79 -87.6 , 41.79 -87.59                        1320\n 5 41.8 -87.6 , 41.79 -87.6                          1099\n 6 41.79 -87.6 , 41.78509714636 -87.6010727606       1058\n 7 41.79 -87.6 , 41.8 -87.6                           999\n 8 41.79 -87.6 , 41.799568 -87.594747                 917\n 9 41.79 -87.6 , 41.78 -87.6                          697\n10 41.89 -87.63 , 41.9 -87.63                         690\n\n\n\n1459rides are not small when compared to most traveled routes.\nTo increase the memberships of the Cyclist Bike Share the company needs to place stations where most unknown routes are traveled by the riders."
  },
  {
    "objectID": "posts/cyclist_trip_analysis/divy_tripdata_202207_202306.html#introduction",
    "href": "posts/cyclist_trip_analysis/divy_tripdata_202207_202306.html#introduction",
    "title": "CYCLIST BIKE SHARE",
    "section": "",
    "text": "The analysis is done on Cyclist Trip Data obtained from Coursera Google Data Analytics course as part of Cap Stone Project.\nThe data contains month wise travel usage of bikes from the year of 2015-2023. We will be concentrating on data gathered in between July-2022 to June-2023 which will cover an entire year.\nLet’s load the required packages first\n\nLoading tidyverse and gt packages\n\n\nlibrary(tidyverse)\nlibrary(gt)\n\n\n\n\nLet’s look at the structure of the data in one of the downloaded .csv files.\n\n\ntrpdata_july_2022&lt;-read_csv(\"F:/Data_Sci/Cap_Stone_Project/Cyclist_trip_data/202207-divvy-tripdata/202207-divvy-tripdata.csv\")\n\nRows: 823488 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (7): ride_id, rideable_type, start_station_name, start_station_id, end_...\ndbl  (4): start_lat, start_lng, end_lat, end_lng\ndttm (2): started_at, ended_at\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nglimpse(trpdata_july_2022)\n\nRows: 823,488\nColumns: 13\n$ ride_id            &lt;chr&gt; \"954144C2F67B1932\", \"292E027607D218B6\", \"5776585258…\n$ rideable_type      &lt;chr&gt; \"classic_bike\", \"classic_bike\", \"classic_bike\", \"cl…\n$ started_at         &lt;dttm&gt; 2022-07-05 08:12:47, 2022-07-26 12:53:38, 2022-07-…\n$ ended_at           &lt;dttm&gt; 2022-07-05 08:24:32, 2022-07-26 12:55:31, 2022-07-…\n$ start_station_name &lt;chr&gt; \"Ashland Ave & Blackhawk St\", \"Buckingham Fountain …\n$ start_station_id   &lt;chr&gt; \"13224\", \"15541\", \"15541\", \"15541\", \"TA1307000117\",…\n$ end_station_name   &lt;chr&gt; \"Kingsbury St & Kinzie St\", \"Michigan Ave & 8th St\"…\n$ end_station_id     &lt;chr&gt; \"KA1503000043\", \"623\", \"623\", \"TA1307000164\", \"TA13…\n$ start_lat          &lt;dbl&gt; 41.90707, 41.86962, 41.86962, 41.86962, 41.89147, 4…\n$ start_lng          &lt;dbl&gt; -87.66725, -87.62398, -87.62398, -87.62398, -87.626…\n$ end_lat            &lt;dbl&gt; 41.88918, 41.87277, 41.87277, 41.79526, 41.93625, 4…\n$ end_lng            &lt;dbl&gt; -87.63851, -87.62398, -87.62398, -87.59647, -87.652…\n$ member_casual      &lt;chr&gt; \"member\", \"casual\", \"casual\", \"casual\", \"member\", \"…\n\n\n\nLet’s look at the columns and try to understand what they represent\n\nride_id is the unique identification token generated for each ride that was initiated.\nrideable_type indicates the type of bike used for the ride.\nstarted_at and ended_at give us the time when the ride began and the ride ended respectively.\nstart_station_name and end_station_name give us the names of stations where ride began and ended respectively.\nstart_station_id and end_station_id are unique ID’s given to stations.\nstart_lat and start_lng represent co-ordinates where the ride began.\nend_lat and end_lng represent co-ordinates where the ride stopped.\nmember_casual identifies if the rider is a member or casual rider of the bike.\n\n\nThe trpdata_july_2022 contains 823488 rows and 13 columns. In the results we can see all the columns and their data types.\n\nLets load data of remaining 11 months.\n\n\ntrpdata_aug_2022 &lt;- read_csv(\"F:/Data_Sci/Cap_Stone_Project/Cyclist_trip_data/202208-divvy-tripdata/202208-divvy-tripdata.csv\")\n\ntrpdata_sept_2022&lt;- read_csv(\"F:/Data_Sci/Cap_Stone_Project/Cyclist_trip_data/202209-divvy-tripdata/202209-divvy-publictripdata.csv\")\n\ntrpdata_oct_2022&lt;- read_csv(\"F:/Data_Sci/Cap_Stone_Project/Cyclist_trip_data/202210-divvy-tripdata/202210-divvy-tripdata_raw.csv\")\n\ntrpdata_nov_2022&lt;- read_csv(\"F:/Data_Sci/Cap_Stone_Project/Cyclist_trip_data/202211-divvy-tripdata/202211-divvy-tripdata.csv\")\n\ntrpdata_dec_2022 &lt;- read_csv(\"F:/Data_Sci/Cap_Stone_Project/Cyclist_trip_data/202212-divvy-tripdata/202212-divvy-tripdata.csv\")\n\ntrpdata_jan_2023 &lt;- read_csv(\"F:/Data_Sci/Cap_Stone_Project/Cyclist_trip_data/202301-divvy-tripdata/202301-divvy-tripdata.csv\")\n\ntrpdata_feb_2023 &lt;- read_csv(\"F:/Data_Sci/Cap_Stone_Project/Cyclist_trip_data/202302-divvy-tripdata/202302-divvy-tripdata.csv\")\n\ntrpdata_mar_2023 &lt;- read_csv(\"F:/Data_Sci/Cap_Stone_Project/Cyclist_trip_data/202303-divvy-tripdata/202303-divvy-tripdata.csv\")\n\ntrpdata_apr_2023 &lt;- read_csv(\"F:/Data_Sci/Cap_Stone_Project/Cyclist_trip_data/202304-divvy-tripdata/202304-divvy-tripdata.csv\")\n\ntrpdata_may_2023 &lt;- read_csv(\"F:/Data_Sci/Cap_Stone_Project/Cyclist_trip_data/202305-divvy-tripdata/202305-divvy-tripdata.csv\")\n\ntrpdata_june_2023 &lt;- read_csv(\"F:/Data_Sci/Cap_Stone_Project/Cyclist_trip_data/202306-divvy-tripdata/202306-divvy-tripdata.csv\")\n\nAs structure of .csv’s is same across the all the files lets combine all the .csv files into a single data frame which contains data of all 12 months.\n\nCombining all the monthly data to one previous year data(data_one_year).\n\n\ndata_one_year &lt;- rbind(trpdata_july_2022, trpdata_aug_2022,\n                     trpdata_sept_2022, trpdata_oct_2022,\n                     trpdata_nov_2022, trpdata_dec_2022,\n                     trpdata_jan_2023, trpdata_feb_2023,\n                     trpdata_mar_2023, trpdata_apr_2023,\n                     trpdata_may_2023, trpdata_june_2023)\n\nglimpse(data_one_year)\n\nRows: 5,779,444\nColumns: 13\n$ ride_id            &lt;chr&gt; \"954144C2F67B1932\", \"292E027607D218B6\", \"5776585258…\n$ rideable_type      &lt;chr&gt; \"classic_bike\", \"classic_bike\", \"classic_bike\", \"cl…\n$ started_at         &lt;dttm&gt; 2022-07-05 08:12:47, 2022-07-26 12:53:38, 2022-07-…\n$ ended_at           &lt;dttm&gt; 2022-07-05 08:24:32, 2022-07-26 12:55:31, 2022-07-…\n$ start_station_name &lt;chr&gt; \"Ashland Ave & Blackhawk St\", \"Buckingham Fountain …\n$ start_station_id   &lt;chr&gt; \"13224\", \"15541\", \"15541\", \"15541\", \"TA1307000117\",…\n$ end_station_name   &lt;chr&gt; \"Kingsbury St & Kinzie St\", \"Michigan Ave & 8th St\"…\n$ end_station_id     &lt;chr&gt; \"KA1503000043\", \"623\", \"623\", \"TA1307000164\", \"TA13…\n$ start_lat          &lt;dbl&gt; 41.90707, 41.86962, 41.86962, 41.86962, 41.89147, 4…\n$ start_lng          &lt;dbl&gt; -87.66725, -87.62398, -87.62398, -87.62398, -87.626…\n$ end_lat            &lt;dbl&gt; 41.88918, 41.87277, 41.87277, 41.79526, 41.93625, 4…\n$ end_lng            &lt;dbl&gt; -87.63851, -87.62398, -87.62398, -87.59647, -87.652…\n$ member_casual      &lt;chr&gt; \"member\", \"casual\", \"casual\", \"casual\", \"member\", \"…\n\n\n\ndata_one_year data frame contains data from July-2022 to June-2023.\n\n\n\n\n\nChecking and counting “NA” in each column of the data frame. Data is much better without “NA” as they can cause problems while aggregating data and calculating averages and sums.\n\n\nna_in_cols &lt;- data_one_year %&gt;% map(is.na) %&gt;% map(sum) %&gt;% unlist()\n\nna_in_cols\n\n           ride_id      rideable_type         started_at           ended_at \n                 0                  0                  0                  0 \nstart_station_name   start_station_id   end_station_name     end_station_id \n            857860             857992             915655             915796 \n         start_lat          start_lng            end_lat            end_lng \n                 0                  0               5795               5795 \n     member_casual \n                 0 \n\n\n\nAs NA’s are not present in the times columns i.e, started_at and ended_at we don’t need to worry ourselves about writing na.rm during aggregation and manipulation of data.\nFinding the length of rides by making a new column ride_length in minutes and making sure that the ride_length is not negative by using if_else function. Eliminating stations where station names and longitude and latitude co-ordinates are not present.\n\n\n# As we remove all the NA's it is better to save data in a variable.\ndata_one_year_raw &lt;- data_one_year %&gt;% \n  mutate(ride_length = difftime(ended_at, started_at,\n                                units = \"min\")) %&gt;%\n  mutate(ride_length = as.numeric(ride_length))\n\ndata_one_year &lt;- data_one_year_raw %&gt;% \n  mutate(ride_length = if_else(ride_length &lt; 0, 0, ride_length)) %&gt;% \n  filter(start_station_name != \"\" & end_station_name != \"\" & \n         !is.na(start_lat) & !is.na(start_lng) &\n         !is.na(end_lat) & !is.na(end_lng)) %&gt;% arrange(ride_length)\n\n\nglimpse(data_one_year)\n\nRows: 4,409,335\nColumns: 14\n$ ride_id            &lt;chr&gt; \"86CD09DA24761714\", \"27024CD08288BD45\", \"029D853B5C…\n$ rideable_type      &lt;chr&gt; \"electric_bike\", \"electric_bike\", \"classic_bike\", \"…\n$ started_at         &lt;dttm&gt; 2022-07-20 16:21:48, 2022-07-30 23:42:46, 2022-07-…\n$ ended_at           &lt;dttm&gt; 2022-07-20 16:21:48, 2022-07-30 23:42:46, 2022-07-…\n$ start_station_name &lt;chr&gt; \"Racine Ave & Fullerton Ave\", \"Albany Ave & 26th St…\n$ start_station_id   &lt;chr&gt; \"TA1306000026\", \"15691\", \"chargingstx5\", \"chargings…\n$ end_station_name   &lt;chr&gt; \"Racine Ave & Fullerton Ave\", \"Albany Ave & 26th St…\n$ end_station_id     &lt;chr&gt; \"TA1306000026\", \"15691\", \"chargingstx5\", \"chargings…\n$ start_lat          &lt;dbl&gt; 41.92556, 41.84452, 41.94335, 41.94335, 41.94335, 4…\n$ start_lng          &lt;dbl&gt; -87.65859, -87.70209, -87.67067, -87.67067, -87.670…\n$ end_lat            &lt;dbl&gt; 41.92556, 41.84448, 41.94335, 41.94335, 41.94335, 4…\n$ end_lng            &lt;dbl&gt; -87.65840, -87.70201, -87.67067, -87.67067, -87.670…\n$ member_casual      &lt;chr&gt; \"member\", \"casual\", \"member\", \"member\", \"casual\", \"…\n$ ride_length        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …"
  },
  {
    "objectID": "posts/cyclist_trip_analysis/divy_tripdata_202207_202306.html#analysis-of-data",
    "href": "posts/cyclist_trip_analysis/divy_tripdata_202207_202306.html#analysis-of-data",
    "title": "CYCLIST BIKE SHARE",
    "section": "",
    "text": "Aggregating data to see “Average minutes per ride” grouped by “bike type” and “rider type” after removing rides less than 2 minutes (As rides less than 2 minutes tend to have the same start and stop stations.).\n\n\ndata_one_year_aggregate &lt;- data_one_year %&gt;% \n  select(ride_id, rideable_type, member_casual, started_at, ended_at,\n         ride_length, everything()) %&gt;%\n  filter(ride_length &gt;= 2) %&gt;% \n  summarise(\"Number of Rides\" = n(),\n            \"Ride Length\" = sum(ride_length, na.rm = TRUE),\n            \"Max Ride Length\" = round(max(ride_length), 2),\n            \"Avg Ride Length in Minutes\" = round(mean(ride_length), 2),\n            .by = c(member_casual, rideable_type)) %&gt;% \n  arrange(desc(\"Avg Ride Length in Minutes\")) %&gt;% \n  gt() %&gt;% tab_header(title = \"Average length of Rides\") %&gt;% \n  cols_label(member_casual = \"Rider type\",\n             rideable_type = \"Bike type\")\n\ndata_one_year_aggregate\n\n\n\n\n\nTable 1:  Average minutes per ride \n  \n    \n      Average length of Rides\n    \n    \n    \n      Rider type\n      Bike type\n      Number of Rides\n      Ride Length\n      Max Ride Length\n      Avg Ride Length in Minutes\n    \n  \n  \n    member\nclassic_bike\n1630991\n21996488\n1497.87\n13.49\n    casual\nclassic_bike\n781530\n19383358\n1497.75\n24.80\n    casual\nelectric_bike\n709649\n11372659\n479.98\n16.03\n    member\nelectric_bike\n984688\n10968684\n480.00\n11.14\n    casual\ndocked_bike\n136794\n6899998\n32035.45\n50.44\n  \n  \n  \n\n\n\n\n\nWe can clearly notice in Table 1 that member riders have more number of rides with both classic and electric bikes while the average ride length is higher with casual riders.\n\nCalculating and visualizing Average ride length by “Rider type”.\n\n\naverage_ride_by_rideable_type &lt;- data_one_year %&gt;%\n  rename(\"Rider type\" = member_casual, \"Bike type\" = rideable_type) %&gt;% \n  summarise(ride_length = sum(ride_length, na.rm = TRUE),\n            ride_count = n(),\n            avg_ride_length = ride_length/ride_count,\n            .by = c(`Rider type`, `Bike type`)) %&gt;% \n  ggplot(aes(`Rider type`, avg_ride_length)) + \n  geom_col(aes(fill = `Bike type`), position = \"dodge\") + \n  labs(x = \"Bike type\", y = \"Avg Length of Ride(Minutes)\",\n       title  = \"Average ride length by Bike type\") +\n  theme_minimal() +\n  theme(plot.title = element_text(size = 18),\n        legend.position = \"bottom\")\n\naverage_ride_by_rideable_type\n\n\n\n\nFigure 1: Average Ride Length by Rider type and Member type\n\n\n\n\nThe above Figure 1 clearly shows that members average ride lengths between bike types doesn’t differ much for member riders but differs with casual riders upto 8 minutes.\n\nFurther down in the analysis “docked_bike” type is dropped as no proper documentation is available in the course.\n\n\n\n\n\nCalculating and visualizing ride patterns in a week for number of rides.\n\n\nrideable_order &lt;- c(\"classic_bike\", \"electric_bike\", \"docked_bike\")\n\nrides_on_days &lt;- data_one_year %&gt;%\n  filter(rideable_type != \"docked_bike\") %&gt;%\n  mutate(month = month(started_at, label = TRUE, abbr = FALSE)) %&gt;% \n  mutate(rideable_type = factor(rideable_type, levels = rideable_order)) %&gt;% ggplot(aes(wday(started_at, label = TRUE, abbr = FALSE))) + \n  geom_bar(aes(fill = member_casual), position = \"dodge\") +\n  facet_wrap(~month, nrow = 3) + \n  labs(x = \"Day of the Week\", y = \"Number of rides\",\n       title = \"Riding pattrens on Weekdays of each Month\",\n       subtitle = \"From July-2022 to June-2023\",\n       fill = \"Type of Rider\") +\n  theme_light() +\n  theme(legend.position = \"top\",\n        axis.text.x = element_text(angle = 45, hjust = 1),\n        plot.title = element_text(size = 18))\n\nrides_on_days\n\n\n\n\nFigure 2: Riding pattrens in Weekdays of each Month\n\n\n\n\nThe above Figure 2 clearly shows how the number of rides change due to seasons. In winters the number of rides decrease very drastically may be because of temperature and snow. In Summers the number of rides are at its peak.\nThe number of rides driven by member riders are increases through the week especially in working week days but for casual riders the rides increase in the weekends. The Figure 2 shows number of rides on Saturdays and Sundays by casual members overtake membership riders in the months of July and August.\n\nComparing variation in ride lengths of average and total ride lengths by bike type.\n\nAggregating data for the visualization.\n\nrides_on_days &lt;- data_one_year %&gt;%\n  mutate(day = wday(started_at, label = TRUE, abbr = FALSE),\n         month = month(started_at, label = TRUE, abbr = FALSE)) %&gt;% \n  summarise(ride_count = n(),\n            sum_ride_length = sum(ride_length, na.rm = TRUE),\n            avg_ride_length = mean(ride_length, na.rm = TRUE),\n            .by = c(month, day, member_casual))\n\nrides_on_days\n\n# A tibble: 168 × 6\n   month day       member_casual ride_count sum_ride_length avg_ride_length\n   &lt;ord&gt; &lt;ord&gt;     &lt;chr&gt;              &lt;int&gt;           &lt;dbl&gt;           &lt;dbl&gt;\n 1 July  Wednesday member             47725         605175.            12.7\n 2 July  Saturday  casual             74543        2057158.            27.6\n 3 July  Tuesday   member             46360         588327.            12.7\n 4 July  Tuesday   casual             31415         705946.            22.5\n 5 July  Saturday  member             53796         817724.            15.2\n 6 July  Friday    casual             42333         960160             22.7\n 7 July  Thursday  casual             35800         759804.            21.2\n 8 July  Sunday    casual             61198        1715527.            28.0\n 9 July  Thursday  member             48572         623503.            12.8\n10 July  Friday    member             48221         616243.            12.8\n# ℹ 158 more rows\n\n\nLet’s visualize the aggregated data\n\nrides_on_days_len &lt;- rides_on_days %&gt;%\n  ggplot(aes(day, sum_ride_length))+\n  geom_col(aes(fill = member_casual), position = \"dodge\")+\n  facet_wrap(~month, ncol = 3)+\n  labs(x = \"Day of the Week\", y = \"Total Length of Rides (Minutes)\",\n       title = \"Total Minutes driven by Riders\",\n       fill = \"Type of Rider\") +\n  theme(legend.position = \"top\",\n        axis.text.x = element_text(angle = 45, hjust = 1),\n        plot.title = element_text(size = 18))\n\nrides_on_days_len\n\n\n\n\nFigure 3: Total Ride lengths through out the year by member types.\n\n\n\n\n\nrides_on_days_len_avg &lt;- rides_on_days %&gt;%\n  ggplot(aes(day, avg_ride_length))+\n  geom_col(aes(fill = member_casual), position = \"dodge\")+\n  facet_wrap(~month, ncol = 3) +\n  labs(x = \"Day of the Week\", y = \"Average Length of Rides (Minutes)\",\n       title = \"Average Minutes driven by Riders\",\n       fill = \"Type of Rider\") +\n  theme(legend.position = \"top\",\n        axis.text.x = element_text(angle = 45, hjust = 1),\n        plot.title = element_text(size = 18))\n\nrides_on_days_len_avg\n\n\n\n\nFigure 4: Average Ride lengths through out year by member types.\n\n\n\n\nThe ride length is varying across months and seasons just as number of rides but average ride length is not fluctuating that much across the year.\n\n\n\n\nRemoving “NA” and blanks from the stations columns.\n\n\ndata_one_year &lt;- data_one_year %&gt;%\n  drop_na(start_station_name, end_station_name ) %&gt;% \n  filter(start_station_name != \"\" & end_station_name != \"\",\n         started_at != ended_at) \n\nglimpse(data_one_year)\n\nRows: 4,409,072\nColumns: 14\n$ ride_id            &lt;chr&gt; \"029D853B5C38426E\", \"C1D6D749139CB6C0\", \"D3E7C0B68E…\n$ rideable_type      &lt;chr&gt; \"classic_bike\", \"classic_bike\", \"classic_bike\", \"cl…\n$ started_at         &lt;dttm&gt; 2022-07-26 20:07:33, 2022-07-26 20:08:04, 2022-07-…\n$ ended_at           &lt;dttm&gt; 2022-07-26 19:59:34, 2022-07-26 19:59:34, 2022-07-…\n$ start_station_name &lt;chr&gt; \"Lincoln Ave & Roscoe St*\", \"Lincoln Ave & Roscoe S…\n$ start_station_id   &lt;chr&gt; \"chargingstx5\", \"chargingstx5\", \"chargingstx5\", \"ch…\n$ end_station_name   &lt;chr&gt; \"Lincoln Ave & Roscoe St*\", \"Lincoln Ave & Roscoe S…\n$ end_station_id     &lt;chr&gt; \"chargingstx5\", \"chargingstx5\", \"chargingstx5\", \"ch…\n$ start_lat          &lt;dbl&gt; 41.94335, 41.94335, 41.94335, 41.94335, 41.93945, 4…\n$ start_lng          &lt;dbl&gt; -87.67067, -87.67067, -87.67067, -87.67067, -87.663…\n$ end_lat            &lt;dbl&gt; 41.94335, 41.94335, 41.94335, 41.94335, 41.93948, 4…\n$ end_lng            &lt;dbl&gt; -87.67067, -87.67067, -87.67067, -87.67067, -87.663…\n$ member_casual      &lt;chr&gt; \"member\", \"member\", \"casual\", \"casual\", \"member\", \"…\n$ ride_length        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n\n\n\nMaking a new column to identify travelled routes.\n\n\ndata_one_year &lt;- data_one_year %&gt;% \n  mutate(stations_travelled = paste(start_station_name, \n                                     \"-\", end_station_name))\n\nglimpse(data_one_year)\n\nRows: 4,409,072\nColumns: 15\n$ ride_id            &lt;chr&gt; \"029D853B5C38426E\", \"C1D6D749139CB6C0\", \"D3E7C0B68E…\n$ rideable_type      &lt;chr&gt; \"classic_bike\", \"classic_bike\", \"classic_bike\", \"cl…\n$ started_at         &lt;dttm&gt; 2022-07-26 20:07:33, 2022-07-26 20:08:04, 2022-07-…\n$ ended_at           &lt;dttm&gt; 2022-07-26 19:59:34, 2022-07-26 19:59:34, 2022-07-…\n$ start_station_name &lt;chr&gt; \"Lincoln Ave & Roscoe St*\", \"Lincoln Ave & Roscoe S…\n$ start_station_id   &lt;chr&gt; \"chargingstx5\", \"chargingstx5\", \"chargingstx5\", \"ch…\n$ end_station_name   &lt;chr&gt; \"Lincoln Ave & Roscoe St*\", \"Lincoln Ave & Roscoe S…\n$ end_station_id     &lt;chr&gt; \"chargingstx5\", \"chargingstx5\", \"chargingstx5\", \"ch…\n$ start_lat          &lt;dbl&gt; 41.94335, 41.94335, 41.94335, 41.94335, 41.93945, 4…\n$ start_lng          &lt;dbl&gt; -87.67067, -87.67067, -87.67067, -87.67067, -87.663…\n$ end_lat            &lt;dbl&gt; 41.94335, 41.94335, 41.94335, 41.94335, 41.93948, 4…\n$ end_lng            &lt;dbl&gt; -87.67067, -87.67067, -87.67067, -87.67067, -87.663…\n$ member_casual      &lt;chr&gt; \"member\", \"member\", \"casual\", \"casual\", \"member\", \"…\n$ ride_length        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ stations_travelled &lt;chr&gt; \"Lincoln Ave & Roscoe St* - Lincoln Ave & Roscoe St…\n\n\n\nFinding which route is most traveled by casual riders.\n\n\nmost_travelled_routes_casual &lt;- data_one_year %&gt;%\n  filter(member_casual == \"casual\") %&gt;% \n  summarise(ride_count = n(),\n            avg_ride_length = round(mean(ride_length), 2),\n            .by = c(stations_travelled)) %&gt;%\n  arrange(desc(ride_count))\n\nhead(most_travelled_routes_casual)\n\n# A tibble: 6 × 3\n  stations_travelled                                  ride_count avg_ride_length\n  &lt;chr&gt;                                                    &lt;int&gt;           &lt;dbl&gt;\n1 Streeter Dr & Grand Ave - Streeter Dr & Grand Ave         9698            39.6\n2 DuSable Lake Shore Dr & Monroe St - DuSable Lake S…       6584            33.4\n3 DuSable Lake Shore Dr & Monroe St - Streeter Dr & …       4840            27.1\n4 Michigan Ave & Oak St - Michigan Ave & Oak St             4292            44.6\n5 Millennium Park - Millennium Park                         3884            37.4\n6 Montrose Harbor - Montrose Harbor                         2711            48.3\n\nNROW(most_travelled_routes_casual)\n\n[1] 130660\n\n\nStreeter Dr & Grand Ave - Streeter Dr & Grand Ave stands to be the most popular station with 9698 rides by casual riders.\n\nmost_travelled_routes_member &lt;- data_one_year  %&gt;%\n  filter(member_casual == \"member\") %&gt;% \n  summarise(ride_count = n(),\n            total_ride_length = sum(ride_length),\n            ride_length = round(mean(ride_length), 2),\n            .by = stations_travelled) %&gt;% arrange(desc(ride_count))\n\nhead(most_travelled_routes_member)\n\n# A tibble: 6 × 4\n  stations_travelled                    ride_count total_ride_length ride_length\n  &lt;chr&gt;                                      &lt;int&gt;             &lt;dbl&gt;       &lt;dbl&gt;\n1 Ellis Ave & 60th St - University Ave…       6153            25936.        4.22\n2 University Ave & 57th St - Ellis Ave…       5786            26634.        4.6 \n3 Ellis Ave & 60th St - Ellis Ave & 55…       5676            28427.        5.01\n4 Ellis Ave & 55th St - Ellis Ave & 60…       5347            27187.        5.08\n5 State St & 33rd St - Calumet Ave & 3…       4156            18014.        4.33\n6 Calumet Ave & 33rd St - State St & 3…       4027            15887.        3.95\n\nNROW(most_travelled_routes_member)\n\n[1] 145104\n\n\nEllis Ave & 60th St - University Ave & 57th St stands as the most traveled route by member riders with 6153 rides per anum.\n\nFinding which station has most ride starting points and which station has most ending points.\n\n\nmost_starting_points &lt;- data_one_year %&gt;% \n  summarise(ride_count = n(),\n            .by = start_station_name) %&gt;%\n  select(start_station_name, ride_count) %&gt;%\n  slice_max(ride_count, n = 10)\n\nmost_starting_points\n\n# A tibble: 10 × 2\n   start_station_name                 ride_count\n   &lt;chr&gt;                                   &lt;int&gt;\n 1 Streeter Dr & Grand Ave                 65892\n 2 DuSable Lake Shore Dr & Monroe St       37939\n 3 Michigan Ave & Oak St                   36036\n 4 DuSable Lake Shore Dr & North Blvd      35091\n 5 Wells St & Concord Ln                   33250\n 6 Clark St & Elm St                       32751\n 7 Kingsbury St & Kinzie St                31876\n 8 Millennium Park                         30917\n 9 Theater on the Lake                     29600\n10 Wells St & Elm St                       28063\n\nmost_starting_points$ride_count %&gt;% sum()\n\n[1] 361415\n\nmost_ending_points &lt;- data_one_year %&gt;% \n  summarise(ride_count = n(),\n            .by = end_station_name) %&gt;%\n  select(end_station_name, ride_count)  %&gt;% \n  slice_max(ride_count, n = 10)\n\nmost_ending_points\n\n# A tibble: 10 × 2\n   end_station_name                   ride_count\n   &lt;chr&gt;                                   &lt;int&gt;\n 1 Streeter Dr & Grand Ave                 67536\n 2 DuSable Lake Shore Dr & North Blvd      38026\n 3 Michigan Ave & Oak St                   36976\n 4 DuSable Lake Shore Dr & Monroe St       36806\n 5 Wells St & Concord Ln                   33814\n 6 Clark St & Elm St                       32325\n 7 Millennium Park                         32046\n 8 Kingsbury St & Kinzie St                31058\n 9 Theater on the Lake                     30214\n10 Wells St & Elm St                       28212\n\nmost_ending_points$ride_count %&gt;% sum()\n\n[1] 367013\n\n\nStreeter Dr & Grand Ave found to be the most popular station as most rides start and end at that station.\n\n\n\nJust because we filtered the data with NA’s that does not mean that the data is not helpful, it just means that it does not our fulfill specific need when calculating or manipulating data.\nLet’s look at NA’s in the data once again.\n\nna_in_cols &lt;- data_one_year_raw %&gt;% map( ~sum(is.na(.))) %&gt;% unlist()\n\nna_in_cols\n\n           ride_id      rideable_type         started_at           ended_at \n                 0                  0                  0                  0 \nstart_station_name   start_station_id   end_station_name     end_station_id \n            857860             857992             915655             915796 \n         start_lat          start_lng            end_lat            end_lng \n                 0                  0               5795               5795 \n     member_casual        ride_length \n                 0                  0 \n\n\n\nWe can see that the start_station_name and end_station_name have majority of NA’s it means that rides are starting and ending where stations are not there.\n\n\nprop_na &lt;- na_in_cols[\"start_station_name\"]/nrow(data_one_year_raw)\n\nprop_na\n\nstart_station_name \n          0.148433 \n\n\n\n14.8432963% of data in start_station_name is missing and good thing is that none of the start_lng and start_lat have any NA’s and we can use this for find the most traveled routes.\n\n\ndata_na_one_year &lt;- data_one_year_raw %&gt;% \n  filter(is.na(start_station_name) | start_station_name == \"\") %&gt;% \n  drop_na(end_lat, end_lng)\n  \nglimpse(data_na_one_year)\n\nRows: 857,860\nColumns: 14\n$ ride_id            &lt;chr&gt; \"DCB3D2C9B63999EC\", \"D1ACA8280DA02AE3\", \"EF98673429…\n$ rideable_type      &lt;chr&gt; \"electric_bike\", \"electric_bike\", \"electric_bike\", …\n$ started_at         &lt;dttm&gt; 2022-07-04 15:04:26, 2022-07-12 14:43:51, 2022-07-…\n$ ended_at           &lt;dttm&gt; 2022-07-04 15:32:38, 2022-07-12 14:49:28, 2022-07-…\n$ start_station_name &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ start_station_id   &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ end_station_name   &lt;chr&gt; \"Ashland Ave & Blackhawk St\", \"Cornell Ave & Hyde P…\n$ end_station_id     &lt;chr&gt; \"13224\", \"KA1503000007\", \"KA1503000007\", \"847\", \"48…\n$ start_lat          &lt;dbl&gt; 41.95, 41.80, 41.80, 41.74, 42.02, 41.95, 41.95, 41…\n$ start_lng          &lt;dbl&gt; -87.64, -87.59, -87.59, -87.55, -87.69, -87.67, -87…\n$ end_lat            &lt;dbl&gt; 41.90707, 41.80241, 41.80241, 41.73000, 42.01000, 4…\n$ end_lng            &lt;dbl&gt; -87.66725, -87.58692, -87.58692, -87.55000, -87.690…\n$ member_casual      &lt;chr&gt; \"member\", \"member\", \"member\", \"member\", \"member\", \"…\n$ ride_length        &lt;dbl&gt; 28.200000, 5.616667, 2.600000, 20.350000, 3.700000,…\n\n\n\nNow let’s make new columns start_point with start_lng and start_lat and end_point with end_lat and end_lng.\n\n\ndata_na_one_year &lt;- data_na_one_year %&gt;%\n    mutate(start_point = paste(start_lat, start_lng),\n           end_point = paste(end_lat, end_lng))\n\nglimpse(data_na_one_year)\n\nRows: 857,860\nColumns: 16\n$ ride_id            &lt;chr&gt; \"DCB3D2C9B63999EC\", \"D1ACA8280DA02AE3\", \"EF98673429…\n$ rideable_type      &lt;chr&gt; \"electric_bike\", \"electric_bike\", \"electric_bike\", …\n$ started_at         &lt;dttm&gt; 2022-07-04 15:04:26, 2022-07-12 14:43:51, 2022-07-…\n$ ended_at           &lt;dttm&gt; 2022-07-04 15:32:38, 2022-07-12 14:49:28, 2022-07-…\n$ start_station_name &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ start_station_id   &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ end_station_name   &lt;chr&gt; \"Ashland Ave & Blackhawk St\", \"Cornell Ave & Hyde P…\n$ end_station_id     &lt;chr&gt; \"13224\", \"KA1503000007\", \"KA1503000007\", \"847\", \"48…\n$ start_lat          &lt;dbl&gt; 41.95, 41.80, 41.80, 41.74, 42.02, 41.95, 41.95, 41…\n$ start_lng          &lt;dbl&gt; -87.64, -87.59, -87.59, -87.55, -87.69, -87.67, -87…\n$ end_lat            &lt;dbl&gt; 41.90707, 41.80241, 41.80241, 41.73000, 42.01000, 4…\n$ end_lng            &lt;dbl&gt; -87.66725, -87.58692, -87.58692, -87.55000, -87.690…\n$ member_casual      &lt;chr&gt; \"member\", \"member\", \"member\", \"member\", \"member\", \"…\n$ ride_length        &lt;dbl&gt; 28.200000, 5.616667, 2.600000, 20.350000, 3.700000,…\n$ start_point        &lt;chr&gt; \"41.95 -87.64\", \"41.8 -87.59\", \"41.8 -87.59\", \"41.7…\n$ end_point          &lt;chr&gt; \"41.907066 -87.667252\", \"41.802406 -87.586924\", \"41…\n\n\n\nAggregating data to check for the most traveled routes without a start_station name.\n\nFirst join start_point and end_point to make route_travelled then count the rides by routes_travelled to see the most traveled path.\n\nmost_travelled_na_routes &lt;- data_na_one_year %&gt;%\n  filter(start_point != end_point) %&gt;% \n  mutate(route_travelled = paste(start_point, \",\", end_point)) %&gt;% \n  summarise(ride_count = n(),\n            .by = route_travelled) %&gt;%\n  slice_max(ride_count, n=10)\n\nmost_travelled_na_routes\n\n# A tibble: 10 × 2\n   route_travelled                             ride_count\n   &lt;chr&gt;                                            &lt;int&gt;\n 1 41.79 -87.6 , 41.8 -87.59                         1459\n 2 41.79 -87.59 , 41.79 -87.6                        1354\n 3 41.8 -87.59 , 41.79 -87.6                         1335\n 4 41.79 -87.6 , 41.79 -87.59                        1320\n 5 41.8 -87.6 , 41.79 -87.6                          1099\n 6 41.79 -87.6 , 41.78509714636 -87.6010727606       1058\n 7 41.79 -87.6 , 41.8 -87.6                           999\n 8 41.79 -87.6 , 41.799568 -87.594747                 917\n 9 41.79 -87.6 , 41.78 -87.6                          697\n10 41.89 -87.63 , 41.9 -87.63                         690\n\n\n\n1459rides are not small when compared to most traveled routes.\nTo increase the memberships of the Cyclist Bike Share the company needs to place stations where most unknown routes are traveled by the riders."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Ajay Shankar A",
    "section": "",
    "text": "This is a blog for projects completed successfully by me with R-programming language. This blog will include projects from basic “Exploratory Data Analysis(EDA)” to complex “Machine Learning(ML)” projects."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Ajay Shankar A",
    "section": "EDUCATION",
    "text": "EDUCATION\n\nUniversity of Agricultural Sciences, Dharwad | Dharwad, Karnataka | Masters in Forest Biology and Tree Improvement | Sept 2019 - Nov 2022\nCollege of Forestry, Sirsi | Uttara Kannada, Karnataka | B.Sc in Forestry | Aug 2015 - April 2019"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "Ajay Shankar A",
    "section": "Experience",
    "text": "Experience\n\nTechnical Assistant | Social Forest Department, Siruguppa | Dec 2022 - Present\nResearch Associate | EMPRI | May 2022 - Aug 2022"
  },
  {
    "objectID": "about.html#citationsprojects",
    "href": "about.html#citationsprojects",
    "title": "Ajay Shankar A",
    "section": "Citations(Projects)",
    "text": "Citations(Projects)\n\nAvailability of Wood for Handicrafts in Karnataka - Strengthening livelihoods and job creation.\nAn Assessment of Wood Availability in Karnataka"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ajay Shankar A",
    "section": "",
    "text": "This is a blog for projects completed successfully by me with R-programming language. This blog will include projects from basic “Exploratory Data Analysis(EDA)” to complex “Machine Learning(ML)” projects.\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nPalmer Penguins\n\n\n\n\n\n\n\nMachine Learning\n\n\nEDA\n\n\nCode\n\n\nAnalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 21, 2023\n\n\nAjay Shankar A\n\n\n\n\n\n\n  \n\n\n\n\nCYCLIST BIKE SHARE\n\n\n\n\n\n\n\nAnalysis\n\n\nCode\n\n\nEDA\n\n\n\n\n\n\n\n\n\n\n\nNov 14, 2023\n\n\nAjay Shankar A\n\n\n\n\n\n\n  \n\n\n\n\nPredict Price of Diamonds\n\n\n\n\n\n\n\nAnalysis\n\n\nCode\n\n\nEDA\n\n\nModeling\n\n\n\n\n\n\n\n\n\n\n\nNov 14, 2023\n\n\nAjay Shankar A\n\n\n\n\n\n\n  \n\n\n\n\nEffect of treatments on leaves\n\n\n\n\n\n\n\nAnalysis\n\n\nCode\n\n\nEDA\n\n\n\n\n\n\n\n\n\n\n\nNov 11, 2023\n\n\nAjay Shankar A\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/data_analysis_for-thesis/data_treatment_effect.html",
    "href": "posts/data_analysis_for-thesis/data_treatment_effect.html",
    "title": "Effect of treatments on leaves",
    "section": "",
    "text": "An experiment was conducted to find the rooting potential of the leaves mainly angiosperms to root when treated with different phyto-hormones.\n\n\nIn the experiment, 4 treatments were applied on 8 different species, and the observations included:\n\nNumber of roots (num_roots_n).\nLength of the longest root in centimeters (lng_long_root_cm).\nDiameter of the longest root in millimeters (dia_long_root_mm).\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(gt)\n\n\n\n\nThe data is loaded as a dataframe thesisdata and columns such as treatments(Treatment) and species(Species) are changes to factors as they are not suitable as strings.\n\n# Loading data\nthesisdata &lt;- read_csv(\"thesisdata.csv\",\n                       show_col_types = FALSE)\n# formating data\nths_data &lt;- thesisdata\nths_data$Treatment &lt;- as.factor(ths_data$Treatment)\nths_data$Treatment &lt;- \n  factor(ths_data$Treatment,levels = c(\"Control\",\n                                       \"Coconut water\", \n                                       \"IBA 1000ppm\",\n                          \"IBA 100ppm + Coconut water\"))\nths_data$Species &lt;- factor(ths_data$Species)\n\nths_data %&gt;% head()\n\n# A tibble: 6 × 5\n  Species            Treatment num_roots_n lng_long_root_cm dia_long_root_mm\n  &lt;fct&gt;              &lt;fct&gt;           &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;\n1 Conidium verigatum Control             0                0                0\n2 Conidium verigatum Control             0                0                0\n3 Conidium verigatum Control             0                0                0\n4 Conidium verigatum Control             0                0                0\n5 Conidium verigatum Control             0                0                0\n6 Conidium verigatum Control             0                0                0\n\n\n\nConfirming that only 8 species and 4 treatments are used in the experiment.\n\n\ntreatments_and_species &lt;- list(unique(ths_data$Treatment), unique(ths_data$Species))\n\nnames(treatments_and_species) &lt;- c(\"Treatment\", \"Species\")\ntreatments_and_species\n\n$Treatment\n[1] Control                    Coconut water             \n[3] IBA 1000ppm                IBA 100ppm + Coconut water\nLevels: Control Coconut water IBA 1000ppm IBA 100ppm + Coconut water\n\n$Species\n[1] Conidium verigatum    Rawolfia tetraphylla  Justicia Sps         \n[4] Zizupus rugosa        Jasminum Sps          Cyclea pelteta       \n[7] Bredelia scandens     Hemigraphis alternata\n8 Levels: Bredelia scandens Conidium verigatum ... Zizupus rugosa\n\n\n\n\n\nThe data is aggregated in Table 1 by the average of root lengths with standard deviation(SD)\n\nths_data_1 &lt;- ths_data %&gt;% group_by(Species, Treatment) %&gt;% \n  summarise(avg_n_roots = mean(num_roots_n),\n            SD_n_roots = sd(num_roots_n),\n            avg_lng_root = mean(lng_long_root_cm),\n            SD_lng_root = sd(lng_long_root_cm),\n            avg_dia_root = mean(dia_long_root_mm),\n            SD_dia_root = sd(dia_long_root_mm)) %&gt;%\n  # rounding of to 2 digits  \n  mutate(across(where(is.double), ~round(., digits = 2))) %&gt;%\n  # combining means and SD into a single column\n  unite(avg_n_roots_SD, avg_n_roots, SD_n_roots, sep = \" \\u00b1 \") %&gt;%\n  unite(avg_lng_root_SD, avg_lng_root, SD_lng_root, sep = \" \\u00b1 \") %&gt;% \n  unite(avg_dia_root_SD, avg_dia_root, SD_dia_root, sep = \" \\u00b1 \") %&gt;%\n  # using 'gt' package to get a table  \n  gt(rowname_col = \"Treatment\") %&gt;% \n  tab_header(\n    title = \"Thesis Data of the Species\",\n    subtitle = \"Influence of growth regulators on the root generation\"\n  ) %&gt;% \n  opt_align_table_header(align = \"center\") %&gt;% \n  cols_label( # renaming columns\n    avg_n_roots_SD = md(\"Mean number of roots \\u00b1 SD\"), #md is markdown language\n    avg_lng_root_SD = md(\"Mean length of longest roots \\u00b1 SD (cm)\"),\n    avg_dia_root_SD = md(\"Mean diameter of  longest roots \\u00b1 SD (mm)\")\n  ) %&gt;% # fixing columns width\n  cols_width(Treatment ~ px(150),\n             avg_n_roots_SD ~ px(150),\n             avg_lng_root_SD ~ px(150),\n             avg_dia_root_SD ~ px(150),\n            ) %&gt;% \n  cols_align(align = \"center\")\n\nths_data_1\n\n\n\n\n\nTable 1:  Aggregating data and finding average of each parameter by species and\ntreatment \n  \n    \n    \n    \n    \n  \n  \n    \n      Thesis Data of the Species\n    \n    \n      Influence of growth regulators on the root generation\n    \n    \n      \n      Mean number of roots ± SD\n      Mean length of longest roots ± SD (cm)\n      Mean diameter of  longest roots ± SD (mm)\n    \n  \n  \n    \n      Bredelia scandens\n    \n    Control\n0 ± 0\n0 ± 0\n0 ± 0\n    Coconut water\n0 ± 0\n0 ± 0\n0 ± 0\n    IBA 1000ppm\n3.22 ± 0.44\n6.06 ± 2.02\n0.89 ± 0.17\n    IBA 100ppm + Coconut water\n3 ± 0.5\n6.02 ± 1.26\n0.77 ± 0.2\n    \n      Conidium verigatum\n    \n    Control\n0 ± 0\n0 ± 0\n0 ± 0\n    Coconut water\n2.89 ± 1.05\n4.52 ± 1.1\n0.62 ± 0.15\n    IBA 1000ppm\n7.89 ± 3.62\n7.52 ± 2.55\n0.76 ± 0.15\n    IBA 100ppm + Coconut water\n2.44 ± 0.88\n4.74 ± 1.09\n0.81 ± 0.18\n    \n      Cyclea pelteta\n    \n    Control\n0 ± 0\n0 ± 0\n0 ± 0\n    Coconut water\n3.33 ± 0.71\n2.48 ± 0.37\n0.18 ± 0.04\n    IBA 1000ppm\n5.22 ± 1.39\n4.89 ± 1.17\n0.28 ± 0.07\n    IBA 100ppm + Coconut water\n8.89 ± 2.03\n12.64 ± 3.23\n0.45 ± 0.1\n    \n      Hemigraphis alternata\n    \n    Control\n0 ± 0\n0 ± 0\n0 ± 0\n    Coconut water\n1.11 ± 0.33\n5.89 ± 0.84\n0.65 ± 0.11\n    IBA 1000ppm\n2.22 ± 0.83\n8.06 ± 1.58\n0.77 ± 0.11\n    IBA 100ppm + Coconut water\n1.11 ± 0.33\n5.33 ± 1.93\n0.57 ± 0.09\n    \n      Jasminum Sps\n    \n    Control\n0 ± 0\n0 ± 0\n0 ± 0\n    Coconut water\n1.89 ± 0.6\n8.06 ± 0.88\n0.5 ± 0.05\n    IBA 1000ppm\n5.11 ± 1.17\n14.82 ± 1.59\n0.88 ± 0.19\n    IBA 100ppm + Coconut water\n2.67 ± 1.22\n12.12 ± 2.27\n1.71 ± 0.25\n    \n      Justicia Sps\n    \n    Control\n0 ± 0\n0 ± 0\n0 ± 0\n    Coconut water\n1.22 ± 0.44\n3.28 ± 1.24\n0.35 ± 0.21\n    IBA 1000ppm\n1.89 ± 0.78\n8.64 ± 2.41\n1.08 ± 0.26\n    IBA 100ppm + Coconut water\n1.22 ± 0.44\n5.47 ± 1.84\n1.35 ± 0.23\n    \n      Rawolfia tetraphylla\n    \n    Control\n0.56 ± 1.13\n0.26 ± 0.51\n0.04 ± 0.09\n    Coconut water\n1.78 ± 0.97\n5.4 ± 2.64\n0.36 ± 0.1\n    IBA 1000ppm\n4.78 ± 2.17\n11.59 ± 4.79\n0.4 ± 0.12\n    IBA 100ppm + Coconut water\n2.67 ± 1.41\n10.09 ± 2.34\n0.61 ± 0.19\n    \n      Zizupus rugosa\n    \n    Control\n0 ± 0\n0 ± 0\n0 ± 0\n    Coconut water\n1.22 ± 0.83\n2.94 ± 1.34\n0.38 ± 0.18\n    IBA 1000ppm\n4.44 ± 1.59\n5.56 ± 2.08\n0.53 ± 0.15\n    IBA 100ppm + Coconut water\n2.22 ± 0.67\n4.26 ± 1.11\n0.63 ± 0.17\n  \n  \n  \n\n\n\n\n\n\n\n\n\nPlotting a bar graph Figure 1 to see how each treatment performed on each species.\n\n\nths_data |&gt; summarise(avg_n_roots = mean(num_roots_n),\n                      avg_lng_root = mean(lng_long_root_cm),\n                      avg_dia_root = mean(dia_long_root_mm),\n                      .by = c(Species, Treatment)) %&gt;%\n  rename(\"Mean number of Roots\" = avg_n_roots,\n         \"Mean length of Longest root(cm)\" = avg_lng_root,\n         \"Mean diameter of Longest roots(mm)\" = avg_dia_root) %&gt;% \n  tidyr::pivot_longer(c(\"Mean number of Roots\", \"Mean length of Longest root(cm)\",\n                        \"Mean diameter of Longest roots(mm)\")) |&gt; \n  ggplot(aes(x = Species, y = value, fill = name)) + \n  geom_col(alpha = 0.7, position = \"dodge\") + \n  facet_wrap(~Treatment, ncol = 2) + \n  theme_bw() + labs(y = \" \", fill = \"Parameters\",\n                    title = \"Effect of treatments on rooting\") +\n theme(legend.position = \"bottom\",\n       axis.text.x = element_text(angle = 45, hjust = 1),\n       plot.title = element_text(size = 18))\n\n\n\n\nFigure 1: Influence of pytohormone treatments on leaves of different species\n\n\n\n\n\nWe can clearly see that Control treatment is not producing any roots in majority of the species.\nIBA 1000ppm is clearly showing most promising results in most of the species in the graph.\n\n\n\n\nWe are going to filter out the control treatment as it is not significant at producing roots at all and plot Figure 2 to find which treatment has better correlation at producing roots with higher diameters.\n\nths_data |&gt; filter(Treatment!= \"Control\") |&gt; \n   ggplot(aes(x = lng_long_root_cm, y = dia_long_root_mm)) + \n  geom_point(aes(color = Treatment)) + \n  geom_smooth(method = \"lm\", aes(group = Treatment, color = Treatment)) +\n  labs(title = \"Plotting Root length Vs Root Diameter\",\n       x = \"Length of the longest root\", \n       y = \"Diameter of the longest root\") +\n  theme(legend.position = \"bottom\",\n        plot.title = element_text(size = 18))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nFigure 2: Plotting a scatter plot to find correlation between root lengths and diameters\n\n\n\n\n\nThe graph clearly shows that the Coconut Water treatment has the highest slope but other treatments which produced higher root lengths did not have a correlation as that of Coconut Water treatment.\n\n\n\n\nLet’s conduct a anova test on the data after filtering Control as it has not given any roots.\nLet H0 be Null hypothesis H1 be Alternate hypothesis\n\n# ANOVA Model\nmodel_anova &lt;- ths_data %&gt;%\n  filter(Treatment != \"Control\") %&gt;%\n  aov(num_roots_n ~ Treatment, data = .)\n\n# Summary\nmodel_anova %&gt;% summary()\n\n             Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nTreatment     2  256.0  128.00   27.43 2.51e-11 ***\nResiduals   213  993.9    4.67                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# TukeyHSD\nmodel_anova %&gt;% TukeyHSD(conf.level = 0.95)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = num_roots_n ~ Treatment, data = .)\n\n$Treatment\n                                              diff        lwr        upr\nIBA 1000ppm-Coconut water                 2.666667  1.8169277  3.5164057\nIBA 100ppm + Coconut water-Coconut water  1.347222  0.4974832  2.1969612\nIBA 100ppm + Coconut water-IBA 1000ppm   -1.319444 -2.1691835 -0.4697054\n                                             p adj\nIBA 1000ppm-Coconut water                0.0000000\nIBA 100ppm + Coconut water-Coconut water 0.0006848\nIBA 100ppm + Coconut water-IBA 1000ppm   0.0009086\n\n\nWe can see from p adj that all the treatments means are different and we can reject the H0 hypothesis. H1 holds at 95% confidence interval."
  },
  {
    "objectID": "posts/data_analysis_for-thesis/data_treatment_effect.html#effect-of-treatments-on-leaves-to-produce-roots",
    "href": "posts/data_analysis_for-thesis/data_treatment_effect.html#effect-of-treatments-on-leaves-to-produce-roots",
    "title": "Effect of treatments on leaves",
    "section": "",
    "text": "An experiment was conducted to find the rooting potential of the leaves mainly angiosperms to root when treated with different phyto-hormones.\n\n\nIn the experiment, 4 treatments were applied on 8 different species, and the observations included:\n\nNumber of roots (num_roots_n).\nLength of the longest root in centimeters (lng_long_root_cm).\nDiameter of the longest root in millimeters (dia_long_root_mm).\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(gt)\n\n\n\n\nThe data is loaded as a dataframe thesisdata and columns such as treatments(Treatment) and species(Species) are changes to factors as they are not suitable as strings.\n\n# Loading data\nthesisdata &lt;- read_csv(\"thesisdata.csv\",\n                       show_col_types = FALSE)\n# formating data\nths_data &lt;- thesisdata\nths_data$Treatment &lt;- as.factor(ths_data$Treatment)\nths_data$Treatment &lt;- \n  factor(ths_data$Treatment,levels = c(\"Control\",\n                                       \"Coconut water\", \n                                       \"IBA 1000ppm\",\n                          \"IBA 100ppm + Coconut water\"))\nths_data$Species &lt;- factor(ths_data$Species)\n\nths_data %&gt;% head()\n\n# A tibble: 6 × 5\n  Species            Treatment num_roots_n lng_long_root_cm dia_long_root_mm\n  &lt;fct&gt;              &lt;fct&gt;           &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;\n1 Conidium verigatum Control             0                0                0\n2 Conidium verigatum Control             0                0                0\n3 Conidium verigatum Control             0                0                0\n4 Conidium verigatum Control             0                0                0\n5 Conidium verigatum Control             0                0                0\n6 Conidium verigatum Control             0                0                0\n\n\n\nConfirming that only 8 species and 4 treatments are used in the experiment.\n\n\ntreatments_and_species &lt;- list(unique(ths_data$Treatment), unique(ths_data$Species))\n\nnames(treatments_and_species) &lt;- c(\"Treatment\", \"Species\")\ntreatments_and_species\n\n$Treatment\n[1] Control                    Coconut water             \n[3] IBA 1000ppm                IBA 100ppm + Coconut water\nLevels: Control Coconut water IBA 1000ppm IBA 100ppm + Coconut water\n\n$Species\n[1] Conidium verigatum    Rawolfia tetraphylla  Justicia Sps         \n[4] Zizupus rugosa        Jasminum Sps          Cyclea pelteta       \n[7] Bredelia scandens     Hemigraphis alternata\n8 Levels: Bredelia scandens Conidium verigatum ... Zizupus rugosa\n\n\n\n\n\nThe data is aggregated in Table 1 by the average of root lengths with standard deviation(SD)\n\nths_data_1 &lt;- ths_data %&gt;% group_by(Species, Treatment) %&gt;% \n  summarise(avg_n_roots = mean(num_roots_n),\n            SD_n_roots = sd(num_roots_n),\n            avg_lng_root = mean(lng_long_root_cm),\n            SD_lng_root = sd(lng_long_root_cm),\n            avg_dia_root = mean(dia_long_root_mm),\n            SD_dia_root = sd(dia_long_root_mm)) %&gt;%\n  # rounding of to 2 digits  \n  mutate(across(where(is.double), ~round(., digits = 2))) %&gt;%\n  # combining means and SD into a single column\n  unite(avg_n_roots_SD, avg_n_roots, SD_n_roots, sep = \" \\u00b1 \") %&gt;%\n  unite(avg_lng_root_SD, avg_lng_root, SD_lng_root, sep = \" \\u00b1 \") %&gt;% \n  unite(avg_dia_root_SD, avg_dia_root, SD_dia_root, sep = \" \\u00b1 \") %&gt;%\n  # using 'gt' package to get a table  \n  gt(rowname_col = \"Treatment\") %&gt;% \n  tab_header(\n    title = \"Thesis Data of the Species\",\n    subtitle = \"Influence of growth regulators on the root generation\"\n  ) %&gt;% \n  opt_align_table_header(align = \"center\") %&gt;% \n  cols_label( # renaming columns\n    avg_n_roots_SD = md(\"Mean number of roots \\u00b1 SD\"), #md is markdown language\n    avg_lng_root_SD = md(\"Mean length of longest roots \\u00b1 SD (cm)\"),\n    avg_dia_root_SD = md(\"Mean diameter of  longest roots \\u00b1 SD (mm)\")\n  ) %&gt;% # fixing columns width\n  cols_width(Treatment ~ px(150),\n             avg_n_roots_SD ~ px(150),\n             avg_lng_root_SD ~ px(150),\n             avg_dia_root_SD ~ px(150),\n            ) %&gt;% \n  cols_align(align = \"center\")\n\nths_data_1\n\n\n\n\n\nTable 1:  Aggregating data and finding average of each parameter by species and\ntreatment \n  \n    \n    \n    \n    \n  \n  \n    \n      Thesis Data of the Species\n    \n    \n      Influence of growth regulators on the root generation\n    \n    \n      \n      Mean number of roots ± SD\n      Mean length of longest roots ± SD (cm)\n      Mean diameter of  longest roots ± SD (mm)\n    \n  \n  \n    \n      Bredelia scandens\n    \n    Control\n0 ± 0\n0 ± 0\n0 ± 0\n    Coconut water\n0 ± 0\n0 ± 0\n0 ± 0\n    IBA 1000ppm\n3.22 ± 0.44\n6.06 ± 2.02\n0.89 ± 0.17\n    IBA 100ppm + Coconut water\n3 ± 0.5\n6.02 ± 1.26\n0.77 ± 0.2\n    \n      Conidium verigatum\n    \n    Control\n0 ± 0\n0 ± 0\n0 ± 0\n    Coconut water\n2.89 ± 1.05\n4.52 ± 1.1\n0.62 ± 0.15\n    IBA 1000ppm\n7.89 ± 3.62\n7.52 ± 2.55\n0.76 ± 0.15\n    IBA 100ppm + Coconut water\n2.44 ± 0.88\n4.74 ± 1.09\n0.81 ± 0.18\n    \n      Cyclea pelteta\n    \n    Control\n0 ± 0\n0 ± 0\n0 ± 0\n    Coconut water\n3.33 ± 0.71\n2.48 ± 0.37\n0.18 ± 0.04\n    IBA 1000ppm\n5.22 ± 1.39\n4.89 ± 1.17\n0.28 ± 0.07\n    IBA 100ppm + Coconut water\n8.89 ± 2.03\n12.64 ± 3.23\n0.45 ± 0.1\n    \n      Hemigraphis alternata\n    \n    Control\n0 ± 0\n0 ± 0\n0 ± 0\n    Coconut water\n1.11 ± 0.33\n5.89 ± 0.84\n0.65 ± 0.11\n    IBA 1000ppm\n2.22 ± 0.83\n8.06 ± 1.58\n0.77 ± 0.11\n    IBA 100ppm + Coconut water\n1.11 ± 0.33\n5.33 ± 1.93\n0.57 ± 0.09\n    \n      Jasminum Sps\n    \n    Control\n0 ± 0\n0 ± 0\n0 ± 0\n    Coconut water\n1.89 ± 0.6\n8.06 ± 0.88\n0.5 ± 0.05\n    IBA 1000ppm\n5.11 ± 1.17\n14.82 ± 1.59\n0.88 ± 0.19\n    IBA 100ppm + Coconut water\n2.67 ± 1.22\n12.12 ± 2.27\n1.71 ± 0.25\n    \n      Justicia Sps\n    \n    Control\n0 ± 0\n0 ± 0\n0 ± 0\n    Coconut water\n1.22 ± 0.44\n3.28 ± 1.24\n0.35 ± 0.21\n    IBA 1000ppm\n1.89 ± 0.78\n8.64 ± 2.41\n1.08 ± 0.26\n    IBA 100ppm + Coconut water\n1.22 ± 0.44\n5.47 ± 1.84\n1.35 ± 0.23\n    \n      Rawolfia tetraphylla\n    \n    Control\n0.56 ± 1.13\n0.26 ± 0.51\n0.04 ± 0.09\n    Coconut water\n1.78 ± 0.97\n5.4 ± 2.64\n0.36 ± 0.1\n    IBA 1000ppm\n4.78 ± 2.17\n11.59 ± 4.79\n0.4 ± 0.12\n    IBA 100ppm + Coconut water\n2.67 ± 1.41\n10.09 ± 2.34\n0.61 ± 0.19\n    \n      Zizupus rugosa\n    \n    Control\n0 ± 0\n0 ± 0\n0 ± 0\n    Coconut water\n1.22 ± 0.83\n2.94 ± 1.34\n0.38 ± 0.18\n    IBA 1000ppm\n4.44 ± 1.59\n5.56 ± 2.08\n0.53 ± 0.15\n    IBA 100ppm + Coconut water\n2.22 ± 0.67\n4.26 ± 1.11\n0.63 ± 0.17\n  \n  \n  \n\n\n\n\n\n\n\n\n\nPlotting a bar graph Figure 1 to see how each treatment performed on each species.\n\n\nths_data |&gt; summarise(avg_n_roots = mean(num_roots_n),\n                      avg_lng_root = mean(lng_long_root_cm),\n                      avg_dia_root = mean(dia_long_root_mm),\n                      .by = c(Species, Treatment)) %&gt;%\n  rename(\"Mean number of Roots\" = avg_n_roots,\n         \"Mean length of Longest root(cm)\" = avg_lng_root,\n         \"Mean diameter of Longest roots(mm)\" = avg_dia_root) %&gt;% \n  tidyr::pivot_longer(c(\"Mean number of Roots\", \"Mean length of Longest root(cm)\",\n                        \"Mean diameter of Longest roots(mm)\")) |&gt; \n  ggplot(aes(x = Species, y = value, fill = name)) + \n  geom_col(alpha = 0.7, position = \"dodge\") + \n  facet_wrap(~Treatment, ncol = 2) + \n  theme_bw() + labs(y = \" \", fill = \"Parameters\",\n                    title = \"Effect of treatments on rooting\") +\n theme(legend.position = \"bottom\",\n       axis.text.x = element_text(angle = 45, hjust = 1),\n       plot.title = element_text(size = 18))\n\n\n\n\nFigure 1: Influence of pytohormone treatments on leaves of different species\n\n\n\n\n\nWe can clearly see that Control treatment is not producing any roots in majority of the species.\nIBA 1000ppm is clearly showing most promising results in most of the species in the graph.\n\n\n\n\nWe are going to filter out the control treatment as it is not significant at producing roots at all and plot Figure 2 to find which treatment has better correlation at producing roots with higher diameters.\n\nths_data |&gt; filter(Treatment!= \"Control\") |&gt; \n   ggplot(aes(x = lng_long_root_cm, y = dia_long_root_mm)) + \n  geom_point(aes(color = Treatment)) + \n  geom_smooth(method = \"lm\", aes(group = Treatment, color = Treatment)) +\n  labs(title = \"Plotting Root length Vs Root Diameter\",\n       x = \"Length of the longest root\", \n       y = \"Diameter of the longest root\") +\n  theme(legend.position = \"bottom\",\n        plot.title = element_text(size = 18))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nFigure 2: Plotting a scatter plot to find correlation between root lengths and diameters\n\n\n\n\n\nThe graph clearly shows that the Coconut Water treatment has the highest slope but other treatments which produced higher root lengths did not have a correlation as that of Coconut Water treatment.\n\n\n\n\nLet’s conduct a anova test on the data after filtering Control as it has not given any roots.\nLet H0 be Null hypothesis H1 be Alternate hypothesis\n\n# ANOVA Model\nmodel_anova &lt;- ths_data %&gt;%\n  filter(Treatment != \"Control\") %&gt;%\n  aov(num_roots_n ~ Treatment, data = .)\n\n# Summary\nmodel_anova %&gt;% summary()\n\n             Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nTreatment     2  256.0  128.00   27.43 2.51e-11 ***\nResiduals   213  993.9    4.67                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# TukeyHSD\nmodel_anova %&gt;% TukeyHSD(conf.level = 0.95)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = num_roots_n ~ Treatment, data = .)\n\n$Treatment\n                                              diff        lwr        upr\nIBA 1000ppm-Coconut water                 2.666667  1.8169277  3.5164057\nIBA 100ppm + Coconut water-Coconut water  1.347222  0.4974832  2.1969612\nIBA 100ppm + Coconut water-IBA 1000ppm   -1.319444 -2.1691835 -0.4697054\n                                             p adj\nIBA 1000ppm-Coconut water                0.0000000\nIBA 100ppm + Coconut water-Coconut water 0.0006848\nIBA 100ppm + Coconut water-IBA 1000ppm   0.0009086\n\n\nWe can see from p adj that all the treatments means are different and we can reject the H0 hypothesis. H1 holds at 95% confidence interval."
  },
  {
    "objectID": "posts/diamonds_ml/ml_diamonds.html",
    "href": "posts/diamonds_ml/ml_diamonds.html",
    "title": "Predict Price of Diamonds",
    "section": "",
    "text": "Building a model to predict the price of the diamonds using tidymodels.\nDiamonds data set is readily available to use through the ggplot2 library in the tidyverse and we will be using this data set predict the prices of the other diamonds.\nIn the data set various parameters of diamonds are given and each of these parameters may or may not effect the price of the diamonds.\n\nlibrary(tidyverse)\n\ndata(\"diamonds\")\ndiamonds\n\n# A tibble: 53,940 × 10\n   carat cut       color clarity depth table price     x     y     z\n   &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n 2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n 3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n 4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n 5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n 6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n 7  0.24 Very Good I     VVS1     62.3    57   336  3.95  3.98  2.47\n 8  0.26 Very Good H     SI1      61.9    55   337  4.07  4.11  2.53\n 9  0.22 Fair      E     VS2      65.1    61   337  3.87  3.78  2.49\n10  0.23 Very Good H     VS1      59.4    61   338  4     4.05  2.39\n# ℹ 53,930 more rows\n\n\nData has over 50,000 observations which is good for modeling.\n\n\n\nThe diamonds data set is available to explore in ggplot2 library as mentioned above.\nLet’s check for NA’s before exploring the data\n\ndiamonds %&gt;% map( ~sum(is.na(.))) %&gt;% unlist()\n\n  carat     cut   color clarity   depth   table   price       x       y       z \n      0       0       0       0       0       0       0       0       0       0 \n\n\n\ndiamonds %&gt;% ggplot(aes(carat)) + geom_freqpoly(binwidth = 0.05)\n\n\n\n\nFigure 1: ?(caption)\n\n\n\n\nFrom the Figure 1 we can observe that - Most of the diamonds are between 0.2 to 1.5 carats. - There are peaks which means higher number of diamonds at whole and common fractions.\nMy general knowledge is that the weight i.e, carat of the diamond influences the price most. Let’s visualize that.\n\ndiamonds %&gt;% ggplot(aes(carat, price)) + geom_hex(bins = 50)\n\n\n\n\nThe price tends to follow exponential curve the log2() curve, we can confirm this by another graph.\n\ndiamonds %&gt;% filter(carat &lt; 2.5) %&gt;% \n  mutate(log_price = log2(price),\n                    log_carat = log2(carat)) %&gt;% \n  ggplot(aes(log_carat, log_price)) + geom_hex(bins = 50)\n\n\n\n\nFigure 2: ?(caption)\n\n\n\n\nThe above Figure 2 shows that once we apply log2() to both price and carat the relationship mostly looks to be linear.\n\ndiamonds %&gt;% filter(carat &lt;= 2.5) %&gt;% ggplot(aes(carat, price)) +\n  geom_col()\n\n\n\n\nWe can see that price jumps when the weight is exactly or greater than to the whole and common fractions such as 0.5, 1.0, 1.5 and 2.\n\nlibrary(patchwork)\n\nplot_parameter &lt;- function(param){\n  ggplot(diamonds, aes({{param}}, price)) + geom_boxplot()\n}\n\n(plot_parameter(cut) + plot_parameter(color)) /\n  (plot_parameter(clarity))"
  },
  {
    "objectID": "posts/diamonds_ml/ml_diamonds.html#predicting-diamonds-price",
    "href": "posts/diamonds_ml/ml_diamonds.html#predicting-diamonds-price",
    "title": "Predict Price of Diamonds",
    "section": "",
    "text": "Building a model to predict the price of the diamonds using tidymodels.\nDiamonds data set is readily available to use through the ggplot2 library in the tidyverse and we will be using this data set predict the prices of the other diamonds.\nIn the data set various parameters of diamonds are given and each of these parameters may or may not effect the price of the diamonds.\n\nlibrary(tidyverse)\n\ndata(\"diamonds\")\ndiamonds\n\n# A tibble: 53,940 × 10\n   carat cut       color clarity depth table price     x     y     z\n   &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n 2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n 3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n 4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n 5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n 6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n 7  0.24 Very Good I     VVS1     62.3    57   336  3.95  3.98  2.47\n 8  0.26 Very Good H     SI1      61.9    55   337  4.07  4.11  2.53\n 9  0.22 Fair      E     VS2      65.1    61   337  3.87  3.78  2.49\n10  0.23 Very Good H     VS1      59.4    61   338  4     4.05  2.39\n# ℹ 53,930 more rows\n\n\nData has over 50,000 observations which is good for modeling.\n\n\n\nThe diamonds data set is available to explore in ggplot2 library as mentioned above.\nLet’s check for NA’s before exploring the data\n\ndiamonds %&gt;% map( ~sum(is.na(.))) %&gt;% unlist()\n\n  carat     cut   color clarity   depth   table   price       x       y       z \n      0       0       0       0       0       0       0       0       0       0 \n\n\n\ndiamonds %&gt;% ggplot(aes(carat)) + geom_freqpoly(binwidth = 0.05)\n\n\n\n\nFigure 1: ?(caption)\n\n\n\n\nFrom the Figure 1 we can observe that - Most of the diamonds are between 0.2 to 1.5 carats. - There are peaks which means higher number of diamonds at whole and common fractions.\nMy general knowledge is that the weight i.e, carat of the diamond influences the price most. Let’s visualize that.\n\ndiamonds %&gt;% ggplot(aes(carat, price)) + geom_hex(bins = 50)\n\n\n\n\nThe price tends to follow exponential curve the log2() curve, we can confirm this by another graph.\n\ndiamonds %&gt;% filter(carat &lt; 2.5) %&gt;% \n  mutate(log_price = log2(price),\n                    log_carat = log2(carat)) %&gt;% \n  ggplot(aes(log_carat, log_price)) + geom_hex(bins = 50)\n\n\n\n\nFigure 2: ?(caption)\n\n\n\n\nThe above Figure 2 shows that once we apply log2() to both price and carat the relationship mostly looks to be linear.\n\ndiamonds %&gt;% filter(carat &lt;= 2.5) %&gt;% ggplot(aes(carat, price)) +\n  geom_col()\n\n\n\n\nWe can see that price jumps when the weight is exactly or greater than to the whole and common fractions such as 0.5, 1.0, 1.5 and 2.\n\nlibrary(patchwork)\n\nplot_parameter &lt;- function(param){\n  ggplot(diamonds, aes({{param}}, price)) + geom_boxplot()\n}\n\n(plot_parameter(cut) + plot_parameter(color)) /\n  (plot_parameter(clarity))"
  }
]